{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6438130d",
   "metadata": {},
   "source": [
    "# Chapter 4: Introduction to Feedforward Neural Networks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this chapter we introduce feedforward neural networks and backpropagation.\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "Artificial neural networks are inspired by brains. \n",
    "\n",
    "Brains are made up of neurons, artificial neural networks are made up of perceptrons.\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "A perceptron $p: \\mathbb{R^n} \\rightarrow \\mathbb{R}$ is a function which takes the following form:\n",
    "$$\n",
    "  p(x; w, b) = f\\left(\\sum_{i=1}^n w_ix_i+b\\right),\n",
    "$$\n",
    "where $w \\in \\mathbb{R^n}, b \\in \\mathbb{R}$ are parameters called weights and bias respectively and $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is any non-linear function called the activation function.\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "Here is a visual representation of a perceptron:\n",
    "\n",
    "![](../images/perceptron.png){fig-align=\"center\"}\n",
    "\n",
    "For example, taking\n",
    "$$\n",
    "  f(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "we get a logistic regression model.\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "There are many different choices for an activation function, see [here](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions).\n",
    "\n",
    "By far the most popular nowadays is called Rectified Linear Unit or ReLU for short. It is defined as\n",
    "$$\n",
    "f(x) = \\begin{cases}\n",
    "  x, \\text{ if } x > 0;\\\\\n",
    "  0, \\text{ if } x \\le 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Note that it is not differentiable at 0, however in practice this won't cause any issues.\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "Here is the graph of ReLU:\n",
    "\n",
    "![](../images/relu.svg){fig-align=\"center\"}\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "By connecting several perceptrons together we get a (artificial) **Neural Network** (NN).\n",
    "\n",
    "The simplest way to connect perceptrons is in a way that the resulting NN would not have any loops. Such a NN is called a **Feedforward Neural Network**.\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "In a feedforward NN perceptrons naturally form layers. Here is an illustration of a feedforward NN with 3 layers:\n",
    "\n",
    "![](../images/fnn.png){fig-align=\"center\"}\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "The leftmost layer is called the input layer, the rightmost - the output layer. Middle layers are called hidden layers.\n",
    "\n",
    "If a NN has at least one hidden layer it is called **deep**.\n",
    "\n",
    "Note that the input layer does not actually contain any perceptrons. The user of the NN uses the input layer to provide their inputs.\n",
    "\n",
    "We are going to count layers from 0, so the input layer is layer 0.\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "We are going to require that all perceptrons in a given layer have the same activation function.\n",
    "\n",
    "We will denote the activation function of the $i$-th layer by $f^{[i]}.$\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "Suppose layer $i-1$ contains $n$ perceptrons and layer $i$ contains $m$ perceptrons. Then the perceptrons in layer $i$ are going to take $n$ dimensional vectors as input.\n",
    "\n",
    "If we collect all the weights of the $i$-th layer into the rows of a $m$ by $n$ matrix $W$ and all the biases into an $m$ dimensional column vector $b$ then we can compute the output of the $i$-th layer compactly as follows:\n",
    "$$\n",
    "f^{[i]}(Wx+b),\n",
    "$$\n",
    "where it is understood that we apply $f^{[i]}$ componentwise.\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "We are going to denote the weight matrix and bias vector of the $i$-th layer by $W^{[i]}$ and $b^{[i]}$ respectively.\n",
    "\n",
    "We are also going to set\n",
    "$$\n",
    "  A^{[i]}(x) = W^{[i]}x+b^{[i]},\n",
    "$$\n",
    "letter $A$ stands for affine.\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "So in our notation the output of the $i$-th layer will be\n",
    "$$\n",
    "f^{[i]}(W^{[i]}x+b^{[i]})=f^{[i]}(A^{[i]}(x))=f^{[i]} \\circ A^{[i]}(x),\n",
    "$$\n",
    "where $\\circ$ denotes function composition.\n",
    "\n",
    "If we denote a feedforward NN with $l$ layers by $N$, then the output of it can be computed as\n",
    "$$\n",
    "N(x) = f^{[l]} \\circ A^{[l]} \\circ f^{[l-1]} \\circ A^{[l-1]} \\circ \\dots \\circ f^{[1]} \\circ A^{[1]}(x).\n",
    "$$\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "Suppose we have a classification problem with $n$ classes. Then usually the output of a feedforward NN is an $n$ dimensional vector.\n",
    "\n",
    "We would like the output vector to represent confidences of the classes, that is the components should be between 0 and 1 and sum to 1. This can be achieved by applying softmax to the output.\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "Softmax is a function from $\\mathbb{R}^n$ to $\\mathbb{R}^n$ whose $i$-th component of the output is defined to be\n",
    "$$\n",
    "\\text{softmax}_i(x) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}.\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "We can apply feedforward NNs to classification and regression problems the same way we did logistic regression.\n",
    "\n",
    "That is, we use a training dataset to define a loss function $L$ that depends on NN's weights. We then train the model by minimizing $L$, using SGD for example.\n",
    "\n",
    "In order to use SGD we need an efficient way to compute the partial derivatives of $L$ with respect to weights and biases. This is done using an algorithm called backpropagation.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "The hardest part of backpropagation is not getting lost in notation.\n",
    "\n",
    "To this end for a function $f:\\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}$ define\n",
    "$$\n",
    "  \\frac{\\partial f}{\\partial W}(W) = \\begin{pmatrix}\n",
    "    \\frac{\\partial f}{\\partial w_{11}} & \\dots & \\frac{\\partial f}{\\partial w_{1n}} \\\\\n",
    "    \\dots & \\dots & \\dots \\\\\n",
    "    \\frac{\\partial f}{\\partial w_{m1}} & \\dots & \\frac{\\partial f}{\\partial w_{mn}}\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Suppose $y$ is an $m$ dimensional row vector and $x$ is an $n$ dimensional column vector. It is easy to show that\n",
    "$$\n",
    "y \\frac{\\partial}{\\partial W}(Wx)=\\frac{\\partial}{\\partial W}(yWx)=y^Tx^T.\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Other useful derivative for us will be\n",
    "$$\n",
    "\\frac{\\partial A(x)}{\\partial x} = \\frac{\\partial}{\\partial x}(Wx) + \\frac{\\partial b}{\\partial x} = W + 0 = W,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\frac{\\partial A(x)}{\\partial x}\n",
    "$$\n",
    "is the Jacobian of $A$.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "If $f$ is an activation function and $x$ is an dimensional vector, then\n",
    "$$\n",
    "\\frac{f(x)}{\\partial x} = \\text{diag}(f'(x)),\n",
    "$$\n",
    "where again \n",
    "$$\n",
    "\\frac{\\partial f(x)}{\\partial x}\n",
    "$$\n",
    "is the Jacobian of $f$ and $\\text{diag}(f'(x))$ is a diagonal $n$ by $n$ matrix with $f'(x)$ on the diagonal.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "For concreteness, we will illustrate backpropagation by example. The general case is the same.\n",
    "\n",
    "Let's figure out the formulas for backpropagation to train a model with one hidden layer for a classification problem. Our model looks like this:\n",
    "\n",
    "![](../images/fnn.png){fig-align=\"center\"}\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "So we have two affine functions $A^{[1]}$ and $A^{[2]}$ and two activation functions $f^{[1]}$ and $f^{[2]}.$ The first pair is for the hidden layer, the second pair is for the output layer.\n",
    "\n",
    "We can compute the output of the model using\n",
    "$$\n",
    "N(x) = f^{[2]} \\circ A^{[2]} \\circ f^{[1]} \\circ A^{[1]}(x).\n",
    "$$\n",
    "\n",
    "To compute the confidences of the classes we can apply softmax to $N(x),$ i.e.\n",
    "$$\n",
    "\\text{confidences}(x) = \\text{softmax} \\circ N(x).\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "We are going to use cross-entropy for loss. Also, in implementations, cross-entropy and softmax are usually combined because the derivatives are much nicer than computing the derivatives of cross-entropy and softmax separately.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Suppose vectors $\\{y, x\\} = \\{y^i, x^i\\}_{i=1}^{n}$ represent the training data. So the vectors $y^i$ have 0 in all components except one where it has the value 1. Suppose the dimension of $y^i$ is $m$. Then our loss function will be\n",
    "$$\n",
    "L(W; N(x)) = \\frac{1}{n} \\sum_{i=1}^{n} - \\log \\frac{\\langle y^i, e^{N(x^i; W)} \\rangle}{\\sum_{j=1}^n e^{N(x^i; W)_j}}\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Then the Jacobian of $L(W; y_{pred})$ with respect to $y_{pred}$ can be computed as\n",
    "$$\n",
    "  \\frac{\\partial L(W; y_{pred})}{\\partial y_{pred}} = \\frac{1}{n} \\sum_{i=1}^n \\text{softmax}(y_{pred}^i) - y^i,\n",
    "$$\n",
    "where the Jacobian is a vector of same dimension as $y^i$ and the sum is componentwise. We interpret this vector as a row vector (we need to do this for matrix multiplication to be defined properly in later formulas).\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "In order to train our NN we need to compute\n",
    "$$\n",
    "\\frac{\\partial L(W)}{\\partial W^{[1]}}, \\ \\frac{\\partial L(W)}{\\partial b^{[1]}}, \\ \\frac{\\partial L(W)}{\\partial W^{[2]}}, \\ \\frac{\\partial L(W)}{\\partial b^{[2]}}.\n",
    "$$\n",
    "\n",
    "We can do this using the chain rule and formula\n",
    "$$\n",
    "y \\frac{\\partial}{\\partial W}(Wx)=y^Tx^T.\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "We compute\n",
    "\n",
    "![](../images/backprop1.png){fig-align=\"center\"}\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Then \n",
    "\n",
    "![](../images/backprop2.png){fig-align=\"center\"}\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Similarly\n",
    "\n",
    "![](../images/backprop3.png){fig-align=\"center\"}\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Also\n",
    "\n",
    "![](../images/backprop4.png){fig-align=\"center\"}\n",
    "\n",
    "and\n",
    "\n",
    "![](../images/backprop5.png){fig-align=\"center\"}\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "So the full backpropagation algorithm consists of two steps:\n",
    "\n",
    "1. Forward pass: Compute the values of\n",
    "\\begin{equation*}\n",
    "  \\begin{split}\n",
    "  &A^{[1]}(x), \\ f^{[1]} \\circ A^{[1]}(x), \\\\\n",
    "  &A^{[2]} \\circ f^{[1]} \\circ A^{[1]}(x), \\ f^{[2]} \\circ A^{[2]} \\circ f^{[1]} \\circ A^{[1]}(x).\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "2. Backward pass: Use the values from step one and derived formulas to compute the partial derivatives in terms of weights and biases.\n",
    "\n",
    "Once you have the partial derivatives you can, for example, plug them into SGD to minimize the loss and train the model.\n",
    "\n",
    "## Practice task\n",
    "\n",
    "Try to implement your own feedforward NN with one hidden layer for a classification task. Use the formulas we derived to perform backpropagation and SGD to minimize the loss function.\n",
    "\n",
    "Make the hidden layer twice as big as the input layer. Use ReLU for the activation function of the first layer. Let the activation function of the output layer be the identity function, i.e. $f^{[2]}(x)=x.$\n",
    "\n",
    "## Practice task\n",
    "\n",
    "Some notes:\n",
    "\n",
    "1. In theoretical considerations it is convenient to consider $x$ to be a column vector. However, in practical implementations it is more convenient to consider $x$ to be a row vector.\n",
    "\n",
    "2. Numpy arrays have matrix multiplication operator `@`.\n",
    "\n",
    "## Practice task\n",
    "\n",
    "3. You can generate some mock data for testing your implementation using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2015bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10000, 20), y shape: (10000, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=10, n_classes=3, n_redundant=10, random_state=34)\n",
    "y = OneHotEncoder(sparse_output=False).fit_transform(y.reshape(-1, 1))\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
