{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcc232e",
   "metadata": {},
   "source": [
    "# Bonus: Other classical algorithms\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this bonus chapter we cover some classical algorithms that we did not cover before we dove into neural networks.\n",
    "\n",
    "There are no code examples, but all the algorithms that we will talk about are implemented in `sklearn`.\n",
    "\n",
    "## Naive Bayes classifier\n",
    "\n",
    "Naive Bayes classifier is a classification algorithm, as the name suggest. Its probably the simplest possible classification algorithm.\n",
    "\n",
    "As a reminder, in a classification task we are given an input vector $x \\in \\mathbb{R}^n$ and we are asked to predict to which of $m$ distinct classes the input belongs.\n",
    "We are also given training data (for which we know what the output should be for a given input) to fit our model on.\n",
    "\n",
    "## Naive Bayes classifier\n",
    "\n",
    "We can set this problem up as follows.\n",
    "We model our input as a random vector $X$ and our output as a finite random variable $Y$ that takes values in the set $\\{1, 2, \\dots, m\\}$.\n",
    "\n",
    "One way to do classification is to come up with an algorithm that estimates the probabilities\n",
    "$$\n",
    "  \\mathbb{P}(Y=y | X=x), \\text{ where } y \\in \\{1, 2, \\dots, m\\},\n",
    "$$\n",
    "i.e. given input $x$ what is the probability that $Y$ is $y$.\n",
    "You then pick the $y$ with the highest probability.\n",
    "\n",
    "## Naive Bayes classifier\n",
    "\n",
    "Denote $p(y | x) = \\mathbb{P}(Y=y | X=x)$ (the conditional point mass function of $Y$).\n",
    "\n",
    "As can be guessed from the name, the Naive Bayes classifier uses the Bayes rule:\n",
    "$$\n",
    "  p(y|x) = \\frac{p(x|y)p(y)}{p(x)},\n",
    "$$\n",
    "it should be obvious what the terms on the RHS mean.\n",
    "\n",
    "## Naive Bayes classifier\n",
    "\n",
    "The idea of the Naive Bayes classifier is that it is possible to estimate $p(x|y)$ and $p(y)$ from the training data.\n",
    "Also we can ignore the $p(x)$ term because it is the same for all classes of $Y$.\n",
    "That is, to figure out for which $y \\in \\{1, 2, \\dots, m\\}$ the $p(y|x)$ is the largest (for fixed $x$) it is enough to compute $p(x|y)$ and $p(y)$.\n",
    "\n",
    "## Naive Bayes classifier\n",
    "\n",
    "The idea of Naive Bayes is that we assume that the components of $x$ are independant random variables when conditioned on $Y$, that is\n",
    "$$\n",
    "  p(x|y) = p(x_1|y)p(x_2|y)\\dots p(x_n|y).\n",
    "$$\n",
    "\n",
    "This is a pretty heavy assumption as usually it does not hold. This is why the algorithm is called \"naive\".\n",
    "This assumption is made because it makes the model increadibly simple and easy to fit, as we will see shortly.\n",
    "\n",
    "## Naive Bayes classifier\n",
    "\n",
    "What we need to do now is to estimate $p(y)$ and $p(x_i|y)$ for $i=1,\\dots,n.$\n",
    "\n",
    "We can estimate $p(y)$ using empirical probabilities, that is\n",
    "$$\n",
    "  p(y) = \\frac{\\text{No. of samples of class y in training data}}{\\text{No. of samples in training data}}\n",
    "$$\n",
    "\n",
    "## Naive Bayes classifier\n",
    "\n",
    "If $x_i$ is categorical (i.e. takes a finite amount of values) we can also estimate $p(x_i|y)$ using empirical probabilities:\n",
    "$$\n",
    "  \\mathbb{P}(X_i=c | Y=y) = \\frac{\\text{No. of samples where $X_i=c$ and $Y=y$}}{\\text{No. of samples of class y in training data}}.\n",
    "$$\n",
    "\n",
    "If $x_i$ is not categorical we can discretize it by bucketing and then estimate as above.\n",
    "\n",
    "So our model ends up being just a table of values of the different probabilities.\n",
    "\n",
    "## Naive Bayes classifier\n",
    "\n",
    "Another approach for estimating $p(x_i|y)$ is to assume some distribution, such as Gaussian or Poisson and then to fit it using the training data.\n",
    "Then use the fitted distributions to estimate the probability.\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "Support vector machines (SVMs) are binary classifiers.\n",
    "Of course you can use SVMs for multiclass classification as well by fitting a SVM to predict each class separately.\n",
    "\n",
    "SVMs use hyperplanes to separare the two classes. However, it is very simple to apply different kernels (basically coordinate transforms) and make them look very non-linear.\n",
    "\n",
    "SVMs were really popular before neural networks took off.\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "Suppose the input $x$ is $n$ dimensional and output $y$ is binary.\n",
    "The formulas will be simpler if we assume that $y$ takes the values $+1$ and $-1$.\n",
    "\n",
    "The idea of SVMs is that we partition the input space into two using a hyperplane (that is an affine subspace of dimension $n-1$) and then the model will output $1$ if $x$ is on one side of this hyperplane and $-1$ if its on the other.\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "First, let's simplify a bit and assume that there does exists a hyperplane $H$ that partitions the training data neatly based on class.\n",
    "That is, the training data falls into two clusters and there is a hyperplane $H$ separating these clusters.\n",
    "\n",
    "A hyperplane can be defined as the set of solutions to a single linear equation:\n",
    "$$\n",
    "  \\theta_1 X_1 + \\dots + \\theta_n X_n + \\theta_0 = 0.\n",
    "$$\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "Suppose $\\theta = (\\theta_1, \\dots, \\theta_n), \\theta_0$ defines $H$ that separates our classes.\n",
    "\n",
    "The model outputs $+1$ if $\\langle \\theta, x \\rangle + \\theta_0 > 0$ and $-1$ otherwise (here $\\langle - , - \\rangle$ is the standard scalar product).\n",
    "\n",
    "Note that $\\theta, \\theta_0$ and $-\\theta, -\\theta_0$ define the same hyperplane, so we pick the one that is compatible with how we labelled our classes.\n",
    "\n",
    "So $\\theta, \\theta_0$ are parameters or weights of our model.\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "Now, there might be several $H$ that separate our classes. How do we pick the best one?\n",
    "\n",
    "The idea is that you pick the separating hyperplane that is furthest away from your training samples.\n",
    "This gives the largest amount of \"slack\" when running the model on data that it has not seen yet, thus hopefully providing the best results.\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "Now let $H$ denote the hyperplane that we are trying to find. How can we compute it?\n",
    "\n",
    "There are hyperplanes $H_+$ and $H_-$ defined by equations\n",
    "$$\n",
    "  H_+: \\theta_1 X_1 + \\dots + \\theta_n X_n + \\theta_0 = 1,\n",
    "$$\n",
    "$$\n",
    "  H_-: \\theta_1 X_1 + \\dots + \\theta_n X_n + \\theta_0 = -1,\n",
    "$$\n",
    "with the property that $y(\\langle \\theta, x \\rangle + \\theta_0) \\ge 1$ for all training samples. \n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "These are hyperplanes that separate the clusters of samples in the training data and each hyperplane touches one of the clusters.\n",
    "\n",
    "In high dimensions there might be multiple possible choices for $\\theta, \\theta_0$ with this property.\n",
    "\n",
    "If we find the choice that maximizes the distance between $H_+$ and $H_-$, then the hyperplane that we are looking for $H$ is the hyperplane that is in the middle of $H_+$ and $H_-$.\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "The distance between $H_+$ and $H_-$ is given by\n",
    "$$\n",
    "  \\frac{2}{\\sqrt{\\theta_1^2+\\dots+\\theta_n^2}}.\n",
    "$$\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "So what we need to do is to minimize\n",
    "$$\n",
    "  \\frac{\\theta_1^2+\\dots+\\theta_n^2}{2}\n",
    "$$\n",
    "given the constraints\n",
    "$$\n",
    "  y_i(\\langle \\theta, x_i \\rangle + \\theta_0) \\ge 1,\n",
    "$$\n",
    "where $(x_i, y_i)$ is our training data.\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "There is a standard way to rewrite this quadratic optimization problem with contraints to a quadratic optimization problem without contraints called the method of Lagrange multipliers (you might have covered it in a mathematical analysis course).\n",
    "\n",
    "Then you run your favourite optimization algortihm on the rewritten problem to get model weights $\\theta_0, \\theta.$\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "Now suppose your training data is noisy and there is no separating hyperplane $H$.\n",
    "We still want to separate our data using a hyperplane, and we want to optimize this hyperplane somehow that it would be as good as possible.\n",
    "\n",
    "What is done is that for each training sample $x_i, y_i$ the \"slack\" variable $\\xi_i \\ge 0$ is defined. This variable represents by what amount the sample is on the wrong side of $H$ (so its 0 if the sample is on the correct side).\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "You then minimize\n",
    "$$\n",
    "  \\frac{\\theta_1^2+\\dots+\\theta_n^2}{2} + C\\sum_{i=1}^M \\xi_i\n",
    "$$\n",
    "given the constraints\n",
    "$$\n",
    "  y_i(\\theta x_i^T + \\theta_0) \\ge 1 - \\xi_i \\text{ and } \\xi_i \\ge 0.\n",
    "$$\n",
    "Here $C > 0$ is a hyperparameter and $M$ is the amount of training samples you have.\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "SVMs are very flexible because you can use change of variables to make your separating boundaries not look like hyperplanes at all (they are still hyperplanes but in the changed variables).\n",
    "\n",
    "Basically what you do is you define some change of variable $\\varphi: \\mathbb{R}^n \\rightarrow \\mathbb{R}^q$ and then you run the same algorithm not in $\\mathbb{R}^n,$ but in $\\mathbb{R}^q$.\n",
    "Notice that you can even change the amount of variables!\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "Usually you want to increase the number of variables.\n",
    "This makes the computations that you have to do when fitting more expensive.\n",
    "This might not be that big of a deal nowadays, but since this is a classical algorithm, people have spent some time thinking how to get around this problem.\n",
    "\n",
    "It turns out that if you pick your change of variables in a clever way then the computations won't be much more expensive.\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "The computation that you will have to do in the higher dimensional space is the scalar product of transformed vectors, that is you will have to compute\n",
    "$$\n",
    "  \\langle \\varphi(u), \\varphi(v) \\rangle.\n",
    "$$\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "The idea is to pick your change of variables in such a way that there is a function $K: \\mathbb{R} \\rightarrow \\mathbb{R}$ such that\n",
    "$$\n",
    "  K(\\langle u, v \\rangle) = \\langle \\varphi(u), \\varphi(v) \\rangle \\text{ for all } u,v \\in \\mathbb{R}^n,\n",
    "$$\n",
    "that is with such a choice of $\\varphi$ you will be able to compute the scalar product in the original lower dimensional space.\n",
    "The function $K$ is called the kernel function.\n",
    "\n",
    "## Support vector machines\n",
    "\n",
    "Here are some popular choices:\n",
    "$$\n",
    "  \\text{polynomial kernel } K(\\langle u, v \\rangle) = (\\langle u, v \\rangle + c)^m,\n",
    "$$\n",
    "$$\n",
    "  \\text{sigmoid kernel } K(\\langle u, v \\rangle) = \\tanh (a\\langle u, v \\rangle + b), a, b \\ge 0.\n",
    "$$\n",
    "\n",
    "## Clustering\n",
    "\n",
    "Clustering is an unsupervised learning technique where you try to partition unlablled data into $n$ clusters (classes).\n",
    "\n",
    "This is sort of like classification except you do not know what the classes are and have no labels to train on.\n",
    "\n",
    "## k-means clustering\n",
    "\n",
    "There are several different algorithms for performing clustering.\n",
    "The most popular one is probably k-means clustering.\n",
    "\n",
    "## k-means clustering\n",
    "\n",
    "To perform k-means clustering you first need to specify $k$ - how many clusters you are going to partition your data into.\n",
    "This is a major drawback of k-means. There are other algorithms that pick $k$ automatically.\n",
    "\n",
    "Now the algorithm for finding the clusters is as follows:\n",
    "\n",
    "1. Initialize by picking $k$ random points which will be the centroids of your clusters $C_j$, that is the points that will define the center of your clusters.\n",
    "\n",
    "## k-means clustering\n",
    "\n",
    "2. Assign each data point $x_i$ to that cluster $C_j$ the centroid of which is closest to $x_i$.\n",
    "3. Recompute the centroids. The formula for $j$-th centroid is\n",
    "$$\n",
    "  \\sum_{x_i \\in C_j} \\frac{x_j}{|C_j|},\n",
    "$$\n",
    "where $|C_j|$ the amount of data points in the $C_j$ cluster.\n",
    "4. Iterate 2 and 3 until the centroids stop changing.\n",
    "\n",
    "## k-means clustering\n",
    "\n",
    "Note that you can use other distance functions, instead of just the Euclidean distance, when doing k-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f325129",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
