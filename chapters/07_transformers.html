<!DOCTYPE html>
<html lang="en"><head>
<script src="07_transformers_files/libs/clipboard/clipboard.min.js"></script>
<script src="07_transformers_files/libs/quarto-html/tabby.min.js"></script>
<script src="07_transformers_files/libs/quarto-html/popper.min.js"></script>
<script src="07_transformers_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="07_transformers_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="07_transformers_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="07_transformers_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <title>Chapter 7: Transformers</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="07_transformers_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="07_transformers_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="07_transformers_files/libs/revealjs/dist/theme/quarto-37f64f33d17d36c7c7406230333751da.css">
  <link href="07_transformers_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="07_transformers_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="07_transformers_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="07_transformers_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Chapter 7: Transformers</h1>

<div class="quarto-title-authors">
</div>

</section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
<p>In this chapter we cover transformers.</p>
<p>Transformers are currently the state of the art for natural language processing (NLP) tasks. For example, large language models are basically just huge transformers (with some extra tricks).</p>
</section>
<section id="self-attention" class="slide level2">
<h2>Self-attention</h2>
<p>The main idea behind transformers is the self-attention operation.</p>
<p>This operation takes in a sequence of <span class="math inline">\(n\)</span> dimensional vectors <span class="math inline">\(x_1, \dots, x_k\)</span> and outputs another sequence of <span class="math inline">\(n\)</span> dimensional vectors of same length <span class="math inline">\(y_1, \dots, y_k.\)</span> In NLP tasks these vectors represent the tokens of the text.</p>
</section>
<section id="self-attention-1" class="slide level2">
<h2>Self-attention</h2>
<p>The output vector <span class="math inline">\(y_i\)</span> is obtained as a weighted sum of input vectors: <span class="math display">\[
  y_i = \sum_{j} w_{ij}x_j.
\]</span></p>
<p>These <span class="math inline">\(w_{ij}\)</span> are not weights of the model but are instead computed as <span class="math display">\[
  w'_{ij} = x_ix_j^T, \ w_{ij} = \frac{e^{w'_{ij}}}{\sum_j e^{w'_{ij}}}
\]</span></p>
</section>
<section id="self-attention-2" class="slide level2">
<h2>Self-attention</h2>
<p>That is to compute <span class="math inline">\(w_{ij}\)</span> we first compute the Euclidean inner product of <span class="math inline">\(x_i\)</span> with the rest of the input vectors <span class="math inline">\(x_j\)</span> and then we apply softmax to them. Softmax is applied to normalize the inner products.</p>
<p>For now we have no weights in self-attention, we will discuss how to add them later.</p>
</section>
<section id="self-attention-3" class="slide level2">
<h2>Self-attention</h2>
<p>To understand what problem self-attention tries to solve consider the following paragraph:</p>
<p>“The cat lived in a barn. It liked to chase mice.”</p>
<p>To parse this paragraph you need to know what “It” in the second sentence refers to. To do this you need to look at all the nouns in the preceding sentence and pick one based on context.</p>
<p>The idea is that when trying to understand what specific words in a sentence mean we need to pay special attention to other words.</p>
</section>
<section id="self-attention-4" class="slide level2">
<h2>Self-attention</h2>
<p>This is exactly what self-attention allows the model to do. When parsing input vector <span class="math inline">\(x_i\)</span> it allows the model to focus on all other input vectors that are relevant when parsing <span class="math inline">\(x_i\)</span>.</p>
<p>The inner product measures how “related” the two vectors are. The meaning of related depends on the modeling task.</p>
<p>To produce the output <span class="math inline">\(y_i\)</span> we first measure how related <span class="math inline">\(x_i\)</span> is to all other input vectors <span class="math inline">\(x_j,\)</span> we then compute the weighted sum of the <span class="math inline">\(x_i\)</span> based on this relevance to obtain the <span class="math inline">\(y_i.\)</span></p>
</section>
<section id="self-attention-5" class="slide level2">
<h2>Self-attention</h2>
<p>To get the self-attention layer used in actual transformers we need three extra additions.</p>
</section>
<section id="self-attention-6" class="slide level2">
<h2>Self-attention</h2>
<ol type="1">
<li>Queries, keys and values:</li>
</ol>
<p>In self-attention each input vector <span class="math inline">\(x_i\)</span> is used in three distinct ways:</p>
<ul>
<li>It is compared to every other input vector to get the <span class="math inline">\(w_{ij}\)</span> used to compute its own output <span class="math inline">\(y_i.\)</span></li>
<li>It is compared to every other input vector to get the <span class="math inline">\(w_{ji}\)</span> used to compute outputs for all other input vectors <span class="math inline">\(x_j.\)</span></li>
<li>It is used in the weighted sum to get each output vector <span class="math inline">\(y_j.\)</span></li>
</ul>
</section>
<section id="self-attention-7" class="slide level2">
<h2>Self-attention</h2>
<p>These three distinct roles are called query, key and value. In actual self-attention three different <span class="math inline">\(k\)</span> by <span class="math inline">\(k\)</span> matrices <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span> are used to preprocess the <span class="math inline">\(x_i\)</span> before computing <span class="math inline">\(y_i.\)</span> This gives extra flexibility and also weights for the model to learn.</p>
<p>So the formulas now are: <span class="math display">\[
  q_i = Qx_i, \ k_i = Kx_i, \ v_i = Vx_i,
\]</span> <span class="math display">\[
  w'_{ij} = q_ik_i^T, w_{ij} = \text{softmax}(w'_{ij}),
\]</span> <span class="math display">\[
  y_i = \sum_{j}w_{ij}v_j.
\]</span></p>
</section>
<section id="self-attention-8" class="slide level2">
<h2>Self-attention</h2>
<ol start="2" type="1">
<li>Normalizing the inner product:</li>
</ol>
<p>The inner product used to compute <span class="math inline">\(w'_{ij}\)</span> is normalized as <span class="math display">\[
w'_{ij} = \frac{q_ik_i^T}{\sqrt{n}},
\]</span> where <span class="math inline">\(n\)</span> is the dimension of <span class="math inline">\(x_i.\)</span></p>
</section>
<section id="self-attention-9" class="slide level2">
<h2>Self-attention</h2>
<ol start="3" type="1">
<li>Multi head attention:</li>
</ol>
<p>A single self-attention operation can learn only one relationship between the inputs. However, it is reasonable to assume that there probably is more than one relationship. We would have to apply several self-attention operations to the input to be able to learn them. This increases the model size considerably.</p>
<p>Turns out we can apply several self-attention operations without increasing the size of the model. This is called multi head attention.</p>
</section>
<section id="self-attention-10" class="slide level2">
<h2>Self-attention</h2>
<p>For concreteness, suppose the dimension of our input vectors <span class="math inline">\(x_i\)</span> is <span class="math inline">\(40.\)</span> To apply multi head attention with <span class="math inline">\(4\)</span> “heads” we proceed as follows. First, we are going to have four separate query, key and value matrices each: <span class="math display">\[
  Q_{r}, K_r, V_r, \ r=1,2,3,4.
\]</span> These matrices are not going to be square but instead <span class="math inline">\(40\)</span> by <span class="math inline">\(10.\)</span></p>
</section>
<section id="self-attention-11" class="slide level2">
<h2>Self-attention</h2>
<p>We apply these matrices to our input vectors to project them into four sets of <span class="math inline">\(10\)</span> dimensional query, key and value vectors. We then apply self-attention to each of these sets.</p>
<p>We then concatenate the outputs to obtain <span class="math inline">\(40\)</span> dimensional vectors <span class="math inline">\(y_i.\)</span> Lastly, for the model to be able to learn a proper embedding (instead of just a concatenation) we multiply each vector by a <span class="math inline">\(40\)</span> by <span class="math inline">\(40\)</span> matrix <span class="math inline">\(W.\)</span></p>
</section>
<section id="self-attention-12" class="slide level2">
<h2>Self-attention</h2>
<p>Here is the diagram of the whole process (it was taken from this <a href="https://peterbloem.nl/blog/transformers">blogpost</a>):</p>

<img data-src="../images/multi-head.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="transformers" class="slide level2">
<h2>Transformers</h2>
<p>The output of an attention layer is then usually fed to a feedforward layer to obtain a transformer block. Models are called transformers if they are built up from transformer blocks.</p>
<p>Let’s build our own transformer block using <code>PyTorch</code>.</p>
<p>This is the transformer block used in the <a href="https://arxiv.org/abs/1706.03762">paper</a> that introduced the transformer architecture.</p>
</section>
<section id="transformers-1" class="slide level2">
<h2>Transformers</h2>
<div id="f699773d" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="cc145697-b698-4afd-98a9-2c141f4670b6" data-execution_count="103">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href=""></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href=""></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb1-3"><a href=""></a></span>
<span id="cb1-4"><a href=""></a><span class="kw">class</span> TransformerBlock(nn.Module):</span>
<span id="cb1-5"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim, no_heads, attn_mask, fc_dim<span class="op">=</span><span class="dv">2048</span>):</span>
<span id="cb1-6"><a href=""></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-7"><a href=""></a>    <span class="va">self</span>.mask <span class="op">=</span> attn_mask</span>
<span id="cb1-8"><a href=""></a>    <span class="va">self</span>.attention <span class="op">=</span> nn.MultiheadAttention(embed_dim<span class="op">=</span>embed_dim, num_heads<span class="op">=</span>no_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-9"><a href=""></a>    <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb1-10"><a href=""></a>    <span class="va">self</span>.fc <span class="op">=</span> nn.Sequential(</span>
<span id="cb1-11"><a href=""></a>      nn.Linear(embed_dim, fc_dim),</span>
<span id="cb1-12"><a href=""></a>      nn.ReLU(),</span>
<span id="cb1-13"><a href=""></a>      nn.Linear(fc_dim, embed_dim)</span>
<span id="cb1-14"><a href=""></a>    )</span>
<span id="cb1-15"><a href=""></a>    <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb1-16"><a href=""></a></span>
<span id="cb1-17"><a href=""></a>  <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-18"><a href=""></a>    att, _ <span class="op">=</span> <span class="va">self</span>.attention(x, x, x, is_causal<span class="op">=</span><span class="va">True</span>, attn_mask<span class="op">=</span><span class="va">self</span>.mask.to(x.device))</span>
<span id="cb1-19"><a href=""></a>    x <span class="op">=</span> <span class="va">self</span>.norm1(x <span class="op">+</span> att)</span>
<span id="cb1-20"><a href=""></a>    x <span class="op">=</span> <span class="va">self</span>.norm2(x <span class="op">+</span> <span class="va">self</span>.fc(x))</span>
<span id="cb1-21"><a href=""></a>    <span class="cf">return</span> x</span>
<span id="cb1-22"><a href=""></a></span>
<span id="cb1-23"><a href=""></a><span class="bu">print</span>(TransformerBlock(<span class="dv">256</span>, <span class="dv">4</span>, <span class="dv">4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

</section>
<section id="transformers-1-output" class="slide level2 output-location-slide"><h2>Transformers</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="cc145697-b698-4afd-98a9-2c141f4670b6" data-execution_count="103">
<div class="cell-output cell-output-stdout">
<pre><code>TransformerBlock(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
  )
  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=2048, bias=True)
    (1): ReLU()
    (2): Linear(in_features=2048, out_features=256, bias=True)
  )
  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
)</code></pre>
</div>
</div></section><section id="transformers-2" class="slide level2">
<h2>Transformers</h2>
<p>Here is a diagram for our transformer block:</p>

<img data-src="../images/transformer_block.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="transformers-3" class="slide level2">
<h2>Transformers</h2>
<p>Self-attention layers are permutation invariant, i.e.&nbsp;if we shuffle up the order of the input <span class="math inline">\(x_i\)</span> the output <span class="math inline">\(y_i\)</span> remain the same except also shuffled. For NLP, we would like our model to be sensitive to word order. To achieve this to each token embedding we are also going to add a positional embedding.</p>
<p>Let’s build a transformer model.</p>
</section>
<section id="transformers-4" class="slide level2">
<h2>Transformers</h2>
<div id="f0ca1f39" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="4c7efb04-1596-4089-cac6-679ce2b15f29" data-execution_count="104">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href=""></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb3-2"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, no_tokens, seq_length, pad_idx, embed_dim, no_heads, depth):</span>
<span id="cb3-3"><a href=""></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-4"><a href=""></a>    mask <span class="op">=</span> nn.Transformer.generate_square_subsequent_mask(seq_length)</span>
<span id="cb3-5"><a href=""></a></span>
<span id="cb3-6"><a href=""></a>    <span class="va">self</span>.token_embedding <span class="op">=</span> nn.Embedding(embedding_dim<span class="op">=</span>embed_dim, num_embeddings<span class="op">=</span>no_tokens)</span>
<span id="cb3-7"><a href=""></a>    <span class="va">self</span>.pos_embedding <span class="op">=</span> nn.Embedding(embedding_dim<span class="op">=</span>embed_dim, num_embeddings<span class="op">=</span>seq_length)</span>
<span id="cb3-8"><a href=""></a></span>
<span id="cb3-9"><a href=""></a>    tblocks <span class="op">=</span> []</span>
<span id="cb3-10"><a href=""></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(depth):</span>
<span id="cb3-11"><a href=""></a>      tblocks.append(TransformerBlock(embed_dim<span class="op">=</span>embed_dim, no_heads<span class="op">=</span>no_heads, attn_mask<span class="op">=</span>mask))</span>
<span id="cb3-12"><a href=""></a>    <span class="va">self</span>.tblocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>tblocks)</span>
<span id="cb3-13"><a href=""></a></span>
<span id="cb3-14"><a href=""></a>    <span class="va">self</span>.toprobs <span class="op">=</span> nn.Linear(embed_dim, no_tokens)</span>
<span id="cb3-15"><a href=""></a></span>
<span id="cb3-16"><a href=""></a>  <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-17"><a href=""></a>    tokens <span class="op">=</span> <span class="va">self</span>.token_embedding(x)</span>
<span id="cb3-18"><a href=""></a>    b, n, e <span class="op">=</span> tokens.size()</span>
<span id="cb3-19"><a href=""></a>    positions <span class="op">=</span> <span class="va">self</span>.pos_embedding(torch.arange(n, device<span class="op">=</span>tokens.device))[<span class="va">None</span>, :, :].expand(b, n, e)</span>
<span id="cb3-20"><a href=""></a>    x <span class="op">=</span> tokens <span class="op">+</span> positions</span>
<span id="cb3-21"><a href=""></a>    x <span class="op">=</span> <span class="va">self</span>.tblocks(x)</span>
<span id="cb3-22"><a href=""></a>    <span class="cf">return</span> <span class="va">self</span>.toprobs(x)</span>
<span id="cb3-23"><a href=""></a></span>
<span id="cb3-24"><a href=""></a><span class="bu">print</span>(Transformer(<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">5</span>, <span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

</section>
<section id="transformers-4-output" class="slide level2 output-location-slide"><h2>Transformers</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="4c7efb04-1596-4089-cac6-679ce2b15f29" data-execution_count="104">
<div class="cell-output cell-output-stdout">
<pre><code>Transformer(
  (token_embedding): Embedding(10, 10)
  (pos_embedding): Embedding(10, 10)
  (tblocks): Sequential(
    (0): TransformerBlock(
      (attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)
      )
      (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
      (fc): Sequential(
        (0): Linear(in_features=10, out_features=2048, bias=True)
        (1): ReLU()
        (2): Linear(in_features=2048, out_features=10, bias=True)
      )
      (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerBlock(
      (attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)
      )
      (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
      (fc): Sequential(
        (0): Linear(in_features=10, out_features=2048, bias=True)
        (1): ReLU()
        (2): Linear(in_features=2048, out_features=10, bias=True)
      )
      (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerBlock(
      (attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)
      )
      (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
      (fc): Sequential(
        (0): Linear(in_features=10, out_features=2048, bias=True)
        (1): ReLU()
        (2): Linear(in_features=2048, out_features=10, bias=True)
      )
      (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
    )
  )
  (toprobs): Linear(in_features=10, out_features=10, bias=True)
)</code></pre>
</div>
</div></section><section id="transformers-5" class="slide level2">
<h2>Transformers</h2>
<p>Let’s apply our transformer to text generation. Let’s generate text the way large language models do it.</p>
<p>Given a sequence of tokens our model is going to predict the next token. We can use this to generate text token by token by feeding the model its output as input.</p>
</section>
<section id="transformers-6" class="slide level2">
<h2>Transformers</h2>
<p>To speed up training we are actually going to ask the model to predict the input sequence, but shifted to the right. I.e. the model is still learning to predict the next token, but now we can use the whole sequence for training instead of just the last token.</p>
</section>
<section id="transformers-7" class="slide level2">
<h2>Transformers</h2>
<p>Since we are giving the whole sequence to the model we need to make sure that it is not able to peek ahead when predicting the next token. This is done by adding a mask to the attention layers’ weights. Then, when computing the output of the attention layer the model is only able to use the current and previous tokens.</p>
</section>
<section id="transformers-8" class="slide level2">
<h2>Transformers</h2>
<p>This idea is summarized in the following diagram, which was taken from this <a href="https://peterbloem.nl/blog/transformers">blogpost</a>.</p>

<img data-src="../images/masked-attention.svg" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="transformers-9" class="slide level2">
<h2>Transformers</h2>
<p>We have already added this mask when building our transformer model.</p>
<p>For the actual task, we are going to feed a list of lithuanian names to the model and ask it to generate more names letter by letter.</p>
<p>Let’s build a dataset class that is going to return tokenized names and also the same names shifted one letter to the right for training.</p>
</section>
<section id="transformers-10" class="slide level2">
<h2>Transformers</h2>
<div id="c8b85dcc" class="cell" data-execution_count="305">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href=""></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href=""></a><span class="im">import</span> torch</span>
<span id="cb5-3"><a href=""></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-4"><a href=""></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset</span>
<span id="cb5-5"><a href=""></a></span>
<span id="cb5-6"><a href=""></a><span class="kw">class</span> CharLevelTokenizerLt:</span>
<span id="cb5-7"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb5-8"><a href=""></a>    <span class="va">self</span>.i2t <span class="op">=</span> [</span>
<span id="cb5-9"><a href=""></a>      <span class="st">'&lt;pad&gt;'</span>, <span class="st">'&lt;eos&gt;'</span>,</span>
<span id="cb5-10"><a href=""></a>      <span class="st">'a'</span>, <span class="st">'b'</span>, <span class="st">'c'</span>, <span class="st">'d'</span>, <span class="st">'e'</span>, <span class="st">'f'</span>, <span class="st">'g'</span>, <span class="st">'h'</span>,</span>
<span id="cb5-11"><a href=""></a>      <span class="st">'i'</span>, <span class="st">'j'</span>, <span class="st">'k'</span>, <span class="st">'l'</span>, <span class="st">'m'</span>, <span class="st">'n'</span>, <span class="st">'o'</span>, <span class="st">'p'</span>,</span>
<span id="cb5-12"><a href=""></a>      <span class="st">'r'</span>, <span class="st">'s'</span>, <span class="st">'t'</span>, <span class="st">'u'</span>, <span class="st">'v'</span>, <span class="st">'y'</span>, <span class="st">'z'</span>, <span class="st">'č'</span>,</span>
<span id="cb5-13"><a href=""></a>      <span class="st">'ė'</span>, <span class="st">'š'</span>, <span class="st">'ū'</span>, <span class="st">'ž'</span></span>
<span id="cb5-14"><a href=""></a>    ]</span>
<span id="cb5-15"><a href=""></a>    <span class="va">self</span>.t2i <span class="op">=</span> {token:index <span class="cf">for</span> index, token <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.i2t)}</span>
<span id="cb5-16"><a href=""></a>    <span class="va">self</span>.no_tokens <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.i2t)</span>
<span id="cb5-17"><a href=""></a>    <span class="va">self</span>.pad_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-18"><a href=""></a>    <span class="va">self</span>.eos_idx <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-19"><a href=""></a></span>
<span id="cb5-20"><a href=""></a>  <span class="kw">def</span> string_to_idx(<span class="va">self</span>, string, seq_length<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb5-21"><a href=""></a>    tokens <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> string]</span>
<span id="cb5-22"><a href=""></a>    <span class="cf">return</span> <span class="va">self</span>.tokens_to_idx(tokens, seq_length)</span>
<span id="cb5-23"><a href=""></a></span>
<span id="cb5-24"><a href=""></a>  <span class="kw">def</span> tokens_to_idx(<span class="va">self</span>, tokens, seq_length<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb5-25"><a href=""></a>    idxs <span class="op">=</span> [<span class="va">self</span>.t2i[token] <span class="cf">if</span> token <span class="kw">in</span> <span class="va">self</span>.t2i <span class="cf">else</span> <span class="va">self</span>.unk_idx <span class="cf">for</span> token <span class="kw">in</span> tokens]</span>
<span id="cb5-26"><a href=""></a>    idxs <span class="op">=</span> [<span class="op">*</span>idxs, <span class="va">self</span>.eos_idx]</span>
<span id="cb5-27"><a href=""></a>    <span class="cf">if</span> seq_length <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-28"><a href=""></a>      idxs <span class="op">=</span> idxs <span class="op">+</span> [<span class="va">self</span>.pad_idx] <span class="op">*</span> (seq_length <span class="op">-</span> <span class="bu">len</span>(idxs))</span>
<span id="cb5-29"><a href=""></a>      idxs <span class="op">=</span> idxs[:seq_length]</span>
<span id="cb5-30"><a href=""></a>    <span class="cf">return</span> idxs</span>
<span id="cb5-31"><a href=""></a></span>
<span id="cb5-32"><a href=""></a>  <span class="kw">def</span> idx_to_string(<span class="va">self</span>, indices):</span>
<span id="cb5-33"><a href=""></a>    tokens <span class="op">=</span> <span class="va">self</span>.idx_to_tokens(indices)</span>
<span id="cb5-34"><a href=""></a>    <span class="cf">return</span> <span class="st">' '</span>.join(tokens)</span>
<span id="cb5-35"><a href=""></a></span>
<span id="cb5-36"><a href=""></a>  <span class="kw">def</span> idx_to_tokens(<span class="va">self</span>, indices):</span>
<span id="cb5-37"><a href=""></a>    <span class="cf">return</span> [<span class="va">self</span>.i2t[idx] <span class="cf">for</span> idx <span class="kw">in</span> indices <span class="cf">if</span> idx <span class="op">!=</span> <span class="va">self</span>.pad_idx <span class="kw">or</span> idx <span class="op">!=</span> <span class="va">self</span>.eos_idx]</span>
<span id="cb5-38"><a href=""></a></span>
<span id="cb5-39"><a href=""></a><span class="kw">class</span> NameData(Dataset):</span>
<span id="cb5-40"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, seq_length):</span>
<span id="cb5-41"><a href=""></a>    <span class="va">self</span>.tokenizer <span class="op">=</span> CharLevelTokenizerLt()</span>
<span id="cb5-42"><a href=""></a>    <span class="va">self</span>.data <span class="op">=</span> pd.read_csv(<span class="st">"../data/names/vardai.csv"</span>)</span>
<span id="cb5-43"><a href=""></a></span>
<span id="cb5-44"><a href=""></a>    <span class="va">self</span>.no_tokens <span class="op">=</span> <span class="va">self</span>.tokenizer.no_tokens</span>
<span id="cb5-45"><a href=""></a>    <span class="va">self</span>.pad_idx <span class="op">=</span> <span class="va">self</span>.tokenizer.pad_idx</span>
<span id="cb5-46"><a href=""></a>    <span class="va">self</span>.eos_idx <span class="op">=</span> <span class="va">self</span>.tokenizer.eos_idx</span>
<span id="cb5-47"><a href=""></a>    <span class="va">self</span>.seq_length <span class="op">=</span> seq_length</span>
<span id="cb5-48"><a href=""></a></span>
<span id="cb5-49"><a href=""></a>  <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb5-50"><a href=""></a>    <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.data)</span>
<span id="cb5-51"><a href=""></a></span>
<span id="cb5-52"><a href=""></a>  <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb5-53"><a href=""></a>    name <span class="op">=</span> <span class="va">self</span>.data.loc[idx][<span class="st">"name"</span>]</span>
<span id="cb5-54"><a href=""></a></span>
<span id="cb5-55"><a href=""></a>    inp  <span class="op">=</span> <span class="va">self</span>.tokenizer.tokens_to_idx(name[:], <span class="va">self</span>.seq_length)</span>
<span id="cb5-56"><a href=""></a>    target <span class="op">=</span> <span class="va">self</span>.tokenizer.tokens_to_idx(name[<span class="dv">1</span>:], <span class="va">self</span>.seq_length)</span>
<span id="cb5-57"><a href=""></a></span>
<span id="cb5-58"><a href=""></a>    inp <span class="op">=</span> torch.IntTensor(inp)</span>
<span id="cb5-59"><a href=""></a>    target <span class="op">=</span> torch.LongTensor(target)</span>
<span id="cb5-60"><a href=""></a>    target <span class="op">=</span> torch.zeros(<span class="va">self</span>.no_tokens<span class="op">*</span><span class="va">self</span>.seq_length, dtype<span class="op">=</span>torch.<span class="bu">float</span>).view(<span class="va">self</span>.seq_length, <span class="va">self</span>.no_tokens).scatter(<span class="dv">1</span>, target.view(<span class="va">self</span>.seq_length, <span class="dv">1</span>), value<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-61"><a href=""></a></span>
<span id="cb5-62"><a href=""></a>    <span class="cf">return</span> inp, target</span>
<span id="cb5-63"><a href=""></a></span>
<span id="cb5-64"><a href=""></a>seq_length <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb5-65"><a href=""></a>data <span class="op">=</span> NameData(seq_length<span class="op">=</span>seq_length)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="transformers-11" class="slide level2">
<h2>Transformers</h2>
<p>We can now train our model.</p>
</section>
<section id="transformers-12" class="slide level2">
<h2>Transformers</h2>
<div id="7fd62212" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="a5085495-289c-40c2-abc5-5ce8b0a80756">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href=""></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb6-2"><a href=""></a></span>
<span id="cb6-3"><a href=""></a><span class="co"># Hyperparameters</span></span>
<span id="cb6-4"><a href=""></a>learning_rate <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb6-5"><a href=""></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb6-6"><a href=""></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb6-7"><a href=""></a></span>
<span id="cb6-8"><a href=""></a>device <span class="op">=</span> torch.accelerator.current_accelerator().<span class="bu">type</span> <span class="cf">if</span> torch.accelerator.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb6-9"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss"> device"</span>)</span>
<span id="cb6-10"><a href=""></a></span>
<span id="cb6-11"><a href=""></a>model <span class="op">=</span> Transformer(</span>
<span id="cb6-12"><a href=""></a>  no_tokens<span class="op">=</span>data.no_tokens,</span>
<span id="cb6-13"><a href=""></a>  seq_length<span class="op">=</span>data.seq_length,</span>
<span id="cb6-14"><a href=""></a>  pad_idx<span class="op">=</span>data.pad_idx,</span>
<span id="cb6-15"><a href=""></a>  embed_dim<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb6-16"><a href=""></a>  no_heads<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb6-17"><a href=""></a>  depth<span class="op">=</span><span class="dv">2</span></span>
<span id="cb6-18"><a href=""></a>).to(device)</span>
<span id="cb6-19"><a href=""></a></span>
<span id="cb6-20"><a href=""></a>dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb6-21"><a href=""></a>  data,</span>
<span id="cb6-22"><a href=""></a>  batch_size<span class="op">=</span>batch_size,</span>
<span id="cb6-23"><a href=""></a>  shuffle<span class="op">=</span><span class="va">True</span></span>
<span id="cb6-24"><a href=""></a>)</span>
<span id="cb6-25"><a href=""></a></span>
<span id="cb6-26"><a href=""></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss().to(device)</span>
<span id="cb6-27"><a href=""></a>optimizer <span class="op">=</span> torch.optim.Adam(params<span class="op">=</span>model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb6-28"><a href=""></a></span>
<span id="cb6-29"><a href=""></a><span class="kw">def</span> train_epoch(dataloader, model, loss_fn, optimizer):</span>
<span id="cb6-30"><a href=""></a>  size <span class="op">=</span> <span class="bu">len</span>(dataloader.dataset)</span>
<span id="cb6-31"><a href=""></a></span>
<span id="cb6-32"><a href=""></a>  model.train() <span class="co"># Set model to training mode</span></span>
<span id="cb6-33"><a href=""></a></span>
<span id="cb6-34"><a href=""></a>  <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb6-35"><a href=""></a>    X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb6-36"><a href=""></a>    <span class="co"># Forward pass</span></span>
<span id="cb6-37"><a href=""></a>    pred <span class="op">=</span> model(X)</span>
<span id="cb6-38"><a href=""></a>    loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb6-39"><a href=""></a></span>
<span id="cb6-40"><a href=""></a>    <span class="co"># Backward pass</span></span>
<span id="cb6-41"><a href=""></a>    loss.backward()</span>
<span id="cb6-42"><a href=""></a>    optimizer.step()</span>
<span id="cb6-43"><a href=""></a></span>
<span id="cb6-44"><a href=""></a>    <span class="co"># Reset the computed gradients back to zero</span></span>
<span id="cb6-45"><a href=""></a>    optimizer.zero_grad()</span>
<span id="cb6-46"><a href=""></a></span>
<span id="cb6-47"><a href=""></a>    <span class="cf">if</span> batch <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-48"><a href=""></a>      loss, current <span class="op">=</span> loss.item(), batch <span class="op">*</span> batch_size <span class="op">+</span> <span class="bu">len</span>(X)</span>
<span id="cb6-49"><a href=""></a>      <span class="bu">print</span>(<span class="ss">f"loss: </span><span class="sc">{</span>loss<span class="sc">:&gt;7f}</span><span class="ss">  [</span><span class="sc">{</span>current<span class="sc">:&gt;5d}</span><span class="ss">/</span><span class="sc">{</span>size<span class="sc">:&gt;5d}</span><span class="ss">]"</span>)</span>
<span id="cb6-50"><a href=""></a></span>
<span id="cb6-51"><a href=""></a><span class="co"># Organize the training loop</span></span>
<span id="cb6-52"><a href=""></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb6-53"><a href=""></a>  <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>t<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">-------------------------------"</span>)</span>
<span id="cb6-54"><a href=""></a>  train_epoch(dataloader, model, loss_fn, optimizer)</span>
<span id="cb6-55"><a href=""></a></span>
<span id="cb6-56"><a href=""></a><span class="bu">print</span>(<span class="st">"Done!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

</section>
<section id="transformers-12-output" class="slide level2 output-location-slide"><h2>Transformers</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="a5085495-289c-40c2-abc5-5ce8b0a80756">
<div class="cell-output cell-output-stdout">
<pre><code>Using cpu device
Epoch 1
-------------------------------
loss: 1.488388  [   32/ 6189]
loss: 0.880608  [ 1632/ 6189]
loss: 0.790527  [ 3232/ 6189]
loss: 0.797287  [ 4832/ 6189]
Epoch 2
-------------------------------
loss: 0.755919  [   32/ 6189]
loss: 0.740433  [ 1632/ 6189]
loss: 0.711342  [ 3232/ 6189]
loss: 0.697305  [ 4832/ 6189]
Epoch 3
-------------------------------
loss: 0.706349  [   32/ 6189]
loss: 0.684327  [ 1632/ 6189]
loss: 0.698026  [ 3232/ 6189]
loss: 0.664101  [ 4832/ 6189]
Epoch 4
-------------------------------
loss: 0.684363  [   32/ 6189]
loss: 0.655325  [ 1632/ 6189]
loss: 0.693487  [ 3232/ 6189]
loss: 0.675284  [ 4832/ 6189]
Epoch 5
-------------------------------
loss: 0.680090  [   32/ 6189]
loss: 0.673038  [ 1632/ 6189]
loss: 0.632579  [ 3232/ 6189]
loss: 0.619733  [ 4832/ 6189]
Epoch 6
-------------------------------
loss: 0.626240  [   32/ 6189]
loss: 0.685372  [ 1632/ 6189]
loss: 0.665441  [ 3232/ 6189]
loss: 0.611739  [ 4832/ 6189]
Epoch 7
-------------------------------
loss: 0.624446  [   32/ 6189]
loss: 0.690106  [ 1632/ 6189]
loss: 0.664815  [ 3232/ 6189]
loss: 0.679398  [ 4832/ 6189]
Epoch 8
-------------------------------
loss: 0.626988  [   32/ 6189]
loss: 0.618654  [ 1632/ 6189]
loss: 0.642076  [ 3232/ 6189]
loss: 0.587421  [ 4832/ 6189]
Epoch 9
-------------------------------
loss: 0.669804  [   32/ 6189]
loss: 0.682446  [ 1632/ 6189]
loss: 0.656556  [ 3232/ 6189]
loss: 0.601503  [ 4832/ 6189]
Epoch 10
-------------------------------
loss: 0.638494  [   32/ 6189]
loss: 0.656802  [ 1632/ 6189]
loss: 0.608127  [ 3232/ 6189]
loss: 0.666279  [ 4832/ 6189]
Done!</code></pre>
</div>
</div></section><section id="transformers-13" class="slide level2">
<h2>Transformers</h2>
<p>Now let’s sample from the model.</p>
<p>For comparison, you can try generating random sequences of characters from the lithuanian alphabet to see that these results are quite good.</p>
</section>
<section id="transformers-14" class="slide level2">
<h2>Transformers</h2>
<div id="3dc1f120" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="18283844-6bdf-4808-8aaf-44b4327d841d">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href=""></a><span class="kw">def</span> sample(model, seed, temperature<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb8-2"><a href=""></a>  <span class="cf">assert</span> <span class="bu">len</span>(seed) <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb8-3"><a href=""></a>  model.<span class="bu">eval</span>()</span>
<span id="cb8-4"><a href=""></a>  <span class="cf">with</span> torch.no_grad():</span>
<span id="cb8-5"><a href=""></a>    idx_to_sample <span class="op">=</span> <span class="bu">len</span>(seed) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb8-6"><a href=""></a></span>
<span id="cb8-7"><a href=""></a>    tokenizer <span class="op">=</span> CharLevelTokenizerLt()</span>
<span id="cb8-8"><a href=""></a></span>
<span id="cb8-9"><a href=""></a>    result <span class="op">=</span> <span class="st">''</span> <span class="op">+</span> seed</span>
<span id="cb8-10"><a href=""></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">15</span> <span class="op">-</span> <span class="bu">len</span>(seed)):</span>
<span id="cb8-11"><a href=""></a>      inp <span class="op">=</span> torch.LongTensor(tokenizer.string_to_idx(result, <span class="dv">15</span>)).view(<span class="dv">1</span>, <span class="dv">15</span>).to(device)</span>
<span id="cb8-12"><a href=""></a>      inp <span class="op">=</span> model(inp)<span class="op">/</span>temperature</span>
<span id="cb8-13"><a href=""></a>      inp <span class="op">=</span> inp[<span class="dv">0</span>, idx_to_sample, :]</span>
<span id="cb8-14"><a href=""></a>      p <span class="op">=</span> nn.functional.softmax(inp, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-15"><a href=""></a>      cd <span class="op">=</span> torch.distributions.Categorical(p)</span>
<span id="cb8-16"><a href=""></a>      next_char <span class="op">=</span> cd.sample()</span>
<span id="cb8-17"><a href=""></a>      <span class="cf">if</span> next_char <span class="op">==</span> tokenizer.eos_idx <span class="kw">or</span> next_char <span class="op">==</span> tokenizer.pad_idx:</span>
<span id="cb8-18"><a href=""></a>        <span class="cf">break</span></span>
<span id="cb8-19"><a href=""></a>      result <span class="op">+=</span> tokenizer.idx_to_string([next_char])</span>
<span id="cb8-20"><a href=""></a>      idx_to_sample <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb8-21"><a href=""></a></span>
<span id="cb8-22"><a href=""></a>    <span class="cf">return</span> result</span>
<span id="cb8-23"><a href=""></a></span>
<span id="cb8-24"><a href=""></a><span class="bu">print</span>(</span>
<span id="cb8-25"><a href=""></a>  <span class="st">' '</span>.join([sample(model<span class="op">=</span>model, seed<span class="op">=</span><span class="st">'jo'</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>)]) <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span></span>
<span id="cb8-26"><a href=""></a>  <span class="op">+</span> <span class="st">' '</span>.join([sample(model<span class="op">=</span>model, seed<span class="op">=</span><span class="st">'jū'</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>)]) <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span></span>
<span id="cb8-27"><a href=""></a>  <span class="op">+</span> <span class="st">' '</span>.join([sample(model<span class="op">=</span>model, seed<span class="op">=</span><span class="st">'ja'</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>)]) <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span></span>
<span id="cb8-28"><a href=""></a>  <span class="op">+</span> <span class="st">' '</span>.join([sample(model<span class="op">=</span>model, seed<span class="op">=</span><span class="st">'je'</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>)]) <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span></span>
<span id="cb8-29"><a href=""></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

</section>
<section id="transformers-14-output" class="slide level2 output-location-slide"><h2>Transformers</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="18283844-6bdf-4808-8aaf-44b4327d841d">
<div class="cell-output cell-output-stdout">
<pre><code>jogberė jofygerė joberas jokrėlis jognaris
jūvatdė jūtvenas jūva jūbomiras jūpva
jažgeinė jazfielijus jažgindė jaržvildė jažuydijorė
ježbimanda jepirojas jefronis ježontija ježborina
</code></pre>
</div>
</div></section><section id="extras" class="slide level2">
<h2>Extras</h2>
<p>There are many pre-trained open source transformers. Here are some classic examples:</p>
<ul>
<li><a href="https://huggingface.co/google-bert/bert-base-uncased">BERT</a></li>
<li><a href="https://huggingface.co/openai-community/gpt2">GPT-2</a></li>
</ul>
</section>
<section id="practice-task" class="slide level2">
<h2>Practice task</h2>
<p>Try building a transformer model for sentiment analysis on this <a href="https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment">dataset</a>.</p>

</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="07_transformers_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="07_transformers_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="07_transformers_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="07_transformers_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="07_transformers_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="07_transformers_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="07_transformers_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="07_transformers_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="07_transformers_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="07_transformers_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>