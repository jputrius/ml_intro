{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1bd1334a",
      "metadata": {
        "id": "1bd1334a"
      },
      "source": [
        "# Chapter 7: Transformers\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this chapter we cover transformers.\n",
        "\n",
        "Transformers are currently the state of the art for natural language processing (NLP) tasks. For example, large language models are basically just huge transformers (with some extra tricks).\n",
        "\n",
        "## Self-attention\n",
        "\n",
        "The main idea behind transformers is the self-attention operation.\n",
        "\n",
        "This operation takes in a sequence of $n$ dimensional vectors $x_1, \\dots, x_k$ and outputs another sequence of $n$ dimensional vectors of same length $y_1, \\dots, y_k.$ In NLP tasks these vectors represent the tokens of the text.\n",
        "\n",
        "## Self-attention\n",
        "\n",
        "The output vector $y_i$ is obtained as a weighted sum of input vectors:\n",
        "$$\n",
        "  y_i = \\sum_{j} w_{ij}x_j.\n",
        "$$\n",
        "\n",
        "These $w_{ij}$ are not weights of the model but are instead computed as\n",
        "$$\n",
        "  w'_{ij} = x_ix_j^T, \\ w_{ij} = \\frac{e^{w'_{ij}}}{\\sum_j e^{w'_{ij}}}\n",
        "$$\n",
        "\n",
        "## Self-attention\n",
        "\n",
        "That is to compute $w_{ij}$ we first compute the Euclidean inner product of $x_i$ with the rest of the input vectors $x_j$ and then we apply softmax to them. Softmax is applied to normalize the inner products.\n",
        "\n",
        "For now we have no weights in self-attention, we will discuss how to add them later.\n",
        "\n",
        "## Self-attention\n",
        "\n",
        "To understand what problem self-attention tries to solve consider the following paragraph:\n",
        "\n",
        "\"The cat lived in a barn. It liked to chase mice.\"\n",
        "\n",
        "To parse this paragraph you need to know what \"It\" in the second sentence refers to. To do this you need to look at all the nouns in the preceding sentence and pick one based on context.\n",
        "\n",
        "The idea is that when trying to understand what specific words in a sentence mean we need to pay special attention to other words.\n",
        "\n",
        "## Self-attention\n",
        "\n",
        "This is exactly what self-attention allows the model to do. When parsing input vector $x_i$ it allows the model to focus on all other input vectors that are relevant when parsing $x_i$.\n",
        "\n",
        "The inner product measures how \"related\" the two vectors are. The meaning of related depends on the modeling task.\n",
        "\n",
        "To produce the output $y_i$ we first measure how related $x_i$ is to all other input vectors $x_j,$ we then compute the weighted sum of the $x_i$ based on this relevance to obtain the $y_i.$\n",
        "\n",
        "## Self-attention\n",
        "\n",
        "To get the self-attention layer used in actual transformers we need three extra additions.\n",
        "\n",
        "## Self-attention\n",
        "\n",
        "1. Queries, keys and values:\n",
        "\n",
        "In self-attention each input vector $x_i$ is used in three distinct ways:\n",
        "\n",
        "- It is compared to every other input vector to get the $w_{ij}$ used to compute its own output $y_i.$\n",
        "- It is compared to every other input vector to get the $w_{ji}$ used to compute outputs for all other input vectors $x_j.$\n",
        "- It is used in the weighted sum to get each output vector $y_j.$\n",
        "\n",
        "## Self-attention\n",
        "\n",
        "These three distinct roles are called query, key and value. In actual self-attention three different $k$ by $k$ matrices $Q$, $K$ and $V$ are used to preprocess the $x_i$ before computing $y_i.$ This gives extra flexibility and also weights for the model to learn.\n",
        "\n",
        "So the formulas now are:\n",
        "$$\n",
        "  q_i = Qx_i, \\ k_i = Kx_i, \\ v_i = Vx_i,\n",
        "$$\n",
        "$$\n",
        "  w'_{ij} = q_ik_i^T, w_{ij} = \\text{softmax}(w'_{ij}),\n",
        "$$\n",
        "$$\n",
        "  y_i = \\sum_{j}w_{ij}v_j.\n",
        "$$\n",
        "\n",
        "## Self-attention\n",
        "\n",
        "2. Normalizing the inner product:\n",
        "\n",
        "The inner product used to compute $w'_{ij}$ is normalized as\n",
        "$$\n",
        "w'_{ij} = \\frac{q_ik_i^T}{\\sqrt{n}},\n",
        "$$\n",
        "where $n$ is the dimension of $x_i.$\n",
        "\n",
        "## Self-attention\n",
        "\n",
        "3. Multi head attention:\n",
        "\n",
        "A single self-attention operation can learn only one relationship between the inputs. However, it is reasonable to assume that there probably is more than one relationship. We would have to apply several self-attention operations to the input to be able to learn them. This increases the model size considerably.\n",
        "\n",
        "Turns out we can apply several self-attention operations without increasing the size of the model. This is called multi head attention.\n",
        "\n",
        "## Self-attention\n",
        "\n",
        "For concreteness, suppose the dimension of our input vectors $x_i$ is $40.$ To apply multi head attention with $4$ \"heads\" we proceed as follows.\n",
        "First, we are going to have four separate query, key and value matrices each:\n",
        "$$\n",
        "  Q_{r}, K_r, V_r, \\ r=1,2,3,4.\n",
        "$$\n",
        "These matrices are not going to be square but instead $40$ by $10.$\n",
        "\n",
        "## Self-attention\n",
        "\n",
        "We apply these matrices to our input vectors to project them into four sets of $10$ dimensional query, key and value vectors. We then apply self-attention to each of these sets.\n",
        "\n",
        "We then concatenate the outputs to obtain $40$ dimensional vectors $y_i.$ Lastly, for the model to be able to learn a proper embedding (instead of just a concatenation) we multiply each vector by a $40$ by $40$ matrix $W.$\n",
        "\n",
        "## Self-attention\n",
        "\n",
        "Here is the diagram of the whole process (it was taken from this [blogpost](https://peterbloem.nl/blog/transformers)):\n",
        "\n",
        "![](../images/multi-head.png){fig-align=\"center\"}\n",
        "\n",
        "## Transformers\n",
        "\n",
        "The output of an attention layer is then usually fed to a feedforward layer to obtain a transformer block. Models are called transformers if they are built up from transformer blocks.\n",
        "\n",
        "Let's build our own transformer block using `PyTorch`.\n",
        "\n",
        "This is the transformer block used in the [paper](https://arxiv.org/abs/1706.03762) that introduced the transformer architecture.\n",
        "\n",
        "## Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f699773d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f699773d",
        "outputId": "cc145697-b698-4afd-98a9-2c141f4670b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TransformerBlock(\n",
            "  (attention): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "  )\n",
            "  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=2048, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "  )\n",
            "  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#| output-location: slide\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embed_dim, no_heads, attn_mask, fc_dim=2048):\n",
        "    super().__init__()\n",
        "    self.mask = attn_mask\n",
        "    self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=no_heads, batch_first=True)\n",
        "    self.norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.fc = nn.Sequential(\n",
        "      nn.Linear(embed_dim, fc_dim),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(fc_dim, embed_dim)\n",
        "    )\n",
        "    self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    att, _ = self.attention(x, x, x, is_causal=True, attn_mask=self.mask.to(x.device))\n",
        "    x = self.norm1(x + att)\n",
        "    x = self.norm2(x + self.fc(x))\n",
        "    return x\n",
        "\n",
        "print(TransformerBlock(256, 4, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "710f6bcc",
      "metadata": {
        "id": "710f6bcc"
      },
      "source": [
        "## Transformers\n",
        "\n",
        "Here is a diagram for our transformer block:\n",
        "\n",
        "![](../images/transformer_block.png){fig-align=\"center\"}\n",
        "\n",
        "## Transformers\n",
        "\n",
        "Self-attention layers are permutation invariant, i.e. if we shuffle up the order of the input $x_i$ the output $y_i$ remain the same except also shuffled. For NLP, we would like our model to be sensitive to word order. To achieve this to each token embedding we are also going to add a positional embedding.\n",
        "\n",
        "Let's build a transformer model.\n",
        "\n",
        "## Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f0ca1f39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0ca1f39",
        "outputId": "4c7efb04-1596-4089-cac6-679ce2b15f29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformer(\n",
            "  (token_embedding): Embedding(10, 10)\n",
            "  (pos_embedding): Embedding(10, 10)\n",
            "  (tblocks): Sequential(\n",
            "    (0): TransformerBlock(\n",
            "      (attention): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=10, out_features=2048, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=2048, out_features=10, bias=True)\n",
            "      )\n",
            "      (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): TransformerBlock(\n",
            "      (attention): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=10, out_features=2048, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=2048, out_features=10, bias=True)\n",
            "      )\n",
            "      (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): TransformerBlock(\n",
            "      (attention): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=10, out_features=2048, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=2048, out_features=10, bias=True)\n",
            "      )\n",
            "      (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (toprobs): Linear(in_features=10, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#| output-location: slide\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, no_tokens, seq_length, pad_idx, embed_dim, no_heads, depth):\n",
        "    super().__init__()\n",
        "    mask = nn.Transformer.generate_square_subsequent_mask(seq_length)\n",
        "\n",
        "    self.token_embedding = nn.Embedding(embedding_dim=embed_dim, num_embeddings=no_tokens)\n",
        "    self.pos_embedding = nn.Embedding(embedding_dim=embed_dim, num_embeddings=seq_length)\n",
        "\n",
        "    tblocks = []\n",
        "    for i in range(depth):\n",
        "      tblocks.append(TransformerBlock(embed_dim=embed_dim, no_heads=no_heads, attn_mask=mask))\n",
        "    self.tblocks = nn.Sequential(*tblocks)\n",
        "\n",
        "    self.toprobs = nn.Linear(embed_dim, no_tokens)\n",
        "\n",
        "  def forward(self, x):\n",
        "    tokens = self.token_embedding(x)\n",
        "    b, n, e = tokens.size()\n",
        "    positions = self.pos_embedding(torch.arange(n, device=tokens.device))[None, :, :].expand(b, n, e)\n",
        "    x = tokens + positions\n",
        "    x = self.tblocks(x)\n",
        "    return self.toprobs(x)\n",
        "\n",
        "print(Transformer(10, 10, 0, 10, 5, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8678a6a",
      "metadata": {
        "id": "c8678a6a"
      },
      "source": [
        "## Transformers\n",
        "\n",
        "Let's apply our transformer to text generation. Let's generate text the way large language models do it.\n",
        "\n",
        "Given a sequence of tokens our model is going to predict the next token. We can use this to generate text token by token by feeding the model its output as input.\n",
        "\n",
        "## Transformers\n",
        "\n",
        "To speed up training we are actually going to ask the model to predict the input sequence, but shifted to the right. I.e. the model is still learning to predict the next token, but now we can use the whole sequence for training instead of just the last token.\n",
        "\n",
        "## Transformers\n",
        "\n",
        "Since we are giving the whole sequence to the model we need to make sure that it is not able to peek ahead when predicting the next token. This is done by adding a mask to the attention layers' weights. Then, when computing the output of the attention layer the model is only able to use the current and previous tokens.\n",
        "\n",
        "## Transformers\n",
        "\n",
        "This idea is summarized in the following diagram, which was taken from this [blogpost](https://peterbloem.nl/blog/transformers).\n",
        "\n",
        "![](../images/masked-attention.svg){fig-align=\"center\"}\n",
        "\n",
        "## Transformers\n",
        "\n",
        "We have already added this mask when building our transformer model.\n",
        "\n",
        "For the actual task, we are going to feed a list of lithuanian names to the model and ask it to generate more names letter by letter.\n",
        "\n",
        "Let's build a dataset class that is going to return tokenized names and also the same names shifted one letter to the right for training.\n",
        "\n",
        "## Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c8b85dcc",
      "metadata": {
        "id": "c8b85dcc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CharLevelTokenizerLt:\n",
        "  def __init__(self):\n",
        "    self.i2t = [\n",
        "      '<pad>', '<eos>',\n",
        "      'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h',\n",
        "      'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
        "      'r', 's', 't', 'u', 'v', 'y', 'z', 'č',\n",
        "      'ė', 'š', 'ū', 'ž'\n",
        "    ]\n",
        "    self.t2i = {token:index for index, token in enumerate(self.i2t)}\n",
        "    self.no_tokens = len(self.i2t)\n",
        "    self.pad_idx = 0\n",
        "    self.eos_idx = 1\n",
        "\n",
        "  def string_to_idx(self, string, seq_length=None):\n",
        "    tokens = [token for token in string]\n",
        "    return self.tokens_to_idx(tokens, seq_length)\n",
        "\n",
        "  def tokens_to_idx(self, tokens, seq_length=None):\n",
        "    idxs = [self.t2i[token] if token in self.t2i else self.unk_idx for token in tokens]\n",
        "    idxs = [*idxs, self.eos_idx]\n",
        "    if seq_length is not None:\n",
        "      idxs = idxs + [self.pad_idx] * (seq_length - len(idxs))\n",
        "      idxs = idxs[:seq_length]\n",
        "    return idxs\n",
        "\n",
        "  def idx_to_string(self, indices):\n",
        "    tokens = self.idx_to_tokens(indices)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "  def idx_to_tokens(self, indices):\n",
        "    return [self.i2t[idx] for idx in indices if idx != self.pad_idx or idx != self.eos_idx]\n",
        "\n",
        "class NameData(Dataset):\n",
        "  def __init__(self, seq_length):\n",
        "    self.tokenizer = CharLevelTokenizerLt()\n",
        "    self.data = pd.read_csv(\"../data/names/vardai.csv\")\n",
        "\n",
        "    self.no_tokens = self.tokenizer.no_tokens\n",
        "    self.pad_idx = self.tokenizer.pad_idx\n",
        "    self.eos_idx = self.tokenizer.eos_idx\n",
        "    self.seq_length = seq_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    name = self.data.loc[idx][\"name\"]\n",
        "\n",
        "    inp  = self.tokenizer.tokens_to_idx(name[:], self.seq_length)\n",
        "    target = self.tokenizer.tokens_to_idx(name[1:], self.seq_length)\n",
        "\n",
        "    inp = torch.IntTensor(inp)\n",
        "    target = torch.LongTensor(target)\n",
        "    target = torch.zeros(self.no_tokens*self.seq_length, dtype=torch.float).view(self.seq_length, self.no_tokens).scatter(1, target.view(self.seq_length, 1), value=1)\n",
        "\n",
        "    return inp, target\n",
        "\n",
        "seq_length = 15\n",
        "batch_size = 32\n",
        "\n",
        "data = NameData(seq_length=seq_length)\n",
        "\n",
        "dataloader = DataLoader(\n",
        "  data,\n",
        "  batch_size=batch_size,\n",
        "  shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e4ec0df",
      "metadata": {},
      "source": [
        "## Transformers\n",
        "\n",
        "Let's create a function for training one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e578bf1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm # This is a library that implements loading bars\n",
        "import sys\n",
        "\n",
        "def train_epoch(dataloader, model, loss_fn, optimizer):\n",
        "  model.train() # Set model to training mode\n",
        "\n",
        "  total_loss = 0\n",
        "  total_batches = 0\n",
        "\n",
        "  with tqdm(dataloader, unit=\"batch\", file=sys.stdout) as ep_tqdm:\n",
        "    ep_tqdm.set_description(\"Train\")\n",
        "    for X, y in ep_tqdm:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "        \n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Reset the computed gradients back to zero\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Output stats\n",
        "      total_loss += loss\n",
        "      total_batches += 1\n",
        "      ep_tqdm.set_postfix(average_batch_loss=(total_loss/total_batches).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9208023e",
      "metadata": {},
      "source": [
        "## Transformers\n",
        "\n",
        "Now we can train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7fd62212",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fd62212",
        "outputId": "a5085495-289c-40c2-abc5-5ce8b0a80756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 214.43batch/s, average_batch_loss=0.816]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 211.03batch/s, average_batch_loss=0.703]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 227.83batch/s, average_batch_loss=0.673]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 233.20batch/s, average_batch_loss=0.658]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 214.30batch/s, average_batch_loss=0.648]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 227.10batch/s, average_batch_loss=0.64] \n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 225.38batch/s, average_batch_loss=0.635]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 207.40batch/s, average_batch_loss=0.63] \n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 196.02batch/s, average_batch_loss=0.626]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 223.32batch/s, average_batch_loss=0.624]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 231.81batch/s, average_batch_loss=0.621]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 232.91batch/s, average_batch_loss=0.62] \n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 228.96batch/s, average_batch_loss=0.618]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 230.13batch/s, average_batch_loss=0.616]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 224.80batch/s, average_batch_loss=0.615]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 214.27batch/s, average_batch_loss=0.614]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 213.94batch/s, average_batch_loss=0.613]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 231.70batch/s, average_batch_loss=0.612]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 222.33batch/s, average_batch_loss=0.612]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Train: 100%|██████████| 194/194 [00:00<00:00, 220.21batch/s, average_batch_loss=0.612]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#| output-location: slide\n",
        "# Hyperparameters\n",
        "learning_rate = 0.0001\n",
        "epochs = 20\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "model = Transformer(\n",
        "  no_tokens=data.no_tokens,\n",
        "  seq_length=data.seq_length,\n",
        "  pad_idx=data.pad_idx,\n",
        "  embed_dim=128,\n",
        "  no_heads=8,\n",
        "  depth=4\n",
        ").to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Organize the training loop\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_epoch(dataloader, model, loss_fn, optimizer)\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84327f9",
      "metadata": {},
      "source": [
        "## Transformers\n",
        "\n",
        "Now let's sample from the model.\n",
        "\n",
        "For comparison, you can try generating random sequences of characters from the lithuanian alphabet to see that these results are quite good.\n",
        "\n",
        "## Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3dc1f120",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dc1f120",
        "outputId": "18283844-6bdf-4808-8aaf-44b4327d841d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "joelmantė jopvydas johelas jozavinė joherdas\n",
            "jūrija jūkija jūfraja jūšrija jūratijus\n",
            "jačius jaolgvinė jaūgvidė jaomintė jaūdronė\n",
            "jevdrohijas jeongirdas jevfropijas ječimonė jeompinė\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#| output-location: slide\n",
        "def sample(model, seed, temperature=1.0):\n",
        "  assert len(seed) > 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    idx_to_sample = len(seed) - 1\n",
        "\n",
        "    tokenizer = CharLevelTokenizerLt()\n",
        "\n",
        "    result = '' + seed\n",
        "    for _ in range(15 - len(seed)):\n",
        "      inp = torch.LongTensor(tokenizer.string_to_idx(result, 15)).view(1, 15).to(device)\n",
        "      inp = model(inp)/temperature\n",
        "      inp = inp[0, idx_to_sample, :]\n",
        "      p = nn.functional.softmax(inp, dim=0)\n",
        "      cd = torch.distributions.Categorical(p)\n",
        "      next_char = cd.sample()\n",
        "      if next_char == tokenizer.eos_idx or next_char == tokenizer.pad_idx:\n",
        "        break\n",
        "      result += tokenizer.idx_to_string([next_char])\n",
        "      idx_to_sample += 1\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\n",
        "  ' '.join([sample(model=model, seed='jo') for _ in range(5)]) + '\\n'\n",
        "  + ' '.join([sample(model=model, seed='jū') for _ in range(5)]) + '\\n'\n",
        "  + ' '.join([sample(model=model, seed='ja') for _ in range(5)]) + '\\n'\n",
        "  + ' '.join([sample(model=model, seed='je') for _ in range(5)]) + '\\n'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2034cb8",
      "metadata": {},
      "source": [
        "## Extras\n",
        "\n",
        "There are many pre-trained open source transformers. Here are some classic examples:\n",
        "\n",
        "- [BERT](https://huggingface.co/google-bert/bert-base-uncased)\n",
        "- [GPT-2](https://huggingface.co/openai-community/gpt2)\n",
        "\n",
        "## Practice task\n",
        "\n",
        "Try building a transformer model for sentiment analysis on this [dataset](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
