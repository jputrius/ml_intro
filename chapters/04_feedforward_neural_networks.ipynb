{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6438130d",
   "metadata": {},
   "source": [
    "# Introduction to Feedforward Neural Networks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this chapter we introduce feedforward neural networks and backpropagation.\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "Artificial neural networks are inspired by brains. \n",
    "\n",
    "Brains are made up of neurons, artificial neural networks are made up of perceptrons.\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "A perceptron $p: \\mathbb{R^n} \\rightarrow \\mathbb{R}$ is a function which takes the following form:\n",
    "$$\n",
    "  p(x; w, b) = f\\left(\\sum_{i=1}^n w_ix_i+b\\right),\n",
    "$$\n",
    "where $w \\in \\mathbb{R^n}, b \\in \\mathbb{R}$ are parameters called weights and bias respectively and $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is any non-linear function called the activation function.\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "Here is a visual representation of a perceptron:\n",
    "\n",
    "![](../images/perceptron.png){fig-align=\"center\"}\n",
    "\n",
    "For example, taking\n",
    "$$\n",
    "  f(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "we get a logistic regression model.\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "There are many different choices for an activation function, see [here](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions).\n",
    "\n",
    "By far the most popular nowadays is called Rectified Linear Unit or ReLU for short. It is defined as\n",
    "$$\n",
    "f(x) = \\begin{cases}\n",
    "  x, \\text{ if } x > 0;\\\\\n",
    "  0, \\text{ if } x \\le 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Note that it is not differentiable at 0, however in practice this won't cause any issues.\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "Here is the graph of ReLU:\n",
    "\n",
    "![](../images/relu.svg){fig-align=\"center\"}\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "By connecting several perceptrons together we get a (artificial) **Neural Network** (NN).\n",
    "\n",
    "The simplest way to connect perceptrons is in a way that the resulting NN would not have any loops. Such a NN is called a **Feedforward Neural Network**.\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "Today we will cover an even simpler case - the neurons will form discrete layers in our feedforward NN. Here is an illustration of a feedforward NN with 3 layers:\n",
    "\n",
    "![](../images/fnn.png){fig-align=\"center\"}\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "The leftmost layer is called the input layer, the rightmost - the output layer. Middle layers are called hidden layers.\n",
    "\n",
    "If a NN has at least one hidden layer it is called **deep**.\n",
    "\n",
    "Note that the input layer does not actually contain any perceptrons. The user of the NN uses the input layer to provide their inputs.\n",
    "\n",
    "We are going to count layers from 0, so the input layer is layer 0.\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "We are going to require that all perceptrons in a given layer have the same activation function.\n",
    "\n",
    "We will denote the activation function of the $i$-th layer by $f^{[i]}.$\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "Suppose layer $i-1$ contains $n$ perceptrons and layer $i$ contains $m$ perceptrons. Then the perceptrons in layer $i$ are going to take $n$ dimensional vectors as input.\n",
    "\n",
    "If we collect all the weights of the $i$-th layer into the rows of a $m$ by $n$ matrix $W$ and all the biases into an $m$ dimensional column vector $b$ then we can compute the output of the $i$-th layer compactly as follows:\n",
    "$$\n",
    "f^{[i]}(Wx+b),\n",
    "$$\n",
    "where it is understood that we apply $f^{[i]}$ componentwise.\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "We are going to denote the weight matrix and bias vector of the $i$-th layer by $W^{[i]}$ and $b^{[i]}$ respectively.\n",
    "\n",
    "We are also going to set\n",
    "$$\n",
    "  A^{[i]}(x) = W^{[i]}x+b^{[i]},\n",
    "$$\n",
    "letter $A$ stands for affine.\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "So in our notation the output of the $i$-th layer will be\n",
    "$$\n",
    "f^{[i]}(W^{[i]}x+b^{[i]})=f^{[i]}(A^{[i]}(x))=f^{[i]} \\circ A^{[i]}(x),\n",
    "$$\n",
    "where $\\circ$ denotes function composition.\n",
    "\n",
    "If we denote a feedforward NN with $l$ layers by $N$, then the output of it can be computed as\n",
    "$$\n",
    "N(x) = f^{[l]} \\circ A^{[l]} \\circ f^{[l-1]} \\circ A^{[l-1]} \\circ \\dots \\circ f^{[1]} \\circ A^{[1]}(x).\n",
    "$$\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "What should the activation function of the output layer be?\n",
    "\n",
    "If we have a regression problem then we use an activation function that is suitable for the problem.\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "Now suppose we have a classification problem with $n > 2$ classes. Then usually the output of a feedforward NN is an $n$ dimensional vector.\n",
    "In this case the output layer is usually made to have no activation function (i.e. the activation function is the identity function).\n",
    "\n",
    "To transform this output to class confidences (a vector that has components that are between 0 and 1 and sum to 1) usually a softmax operation is applied.\n",
    "\n",
    "## Feedforward Neural Networks\n",
    "\n",
    "Softmax is a function from $\\mathbb{R}^n$ to $\\mathbb{R}^n$ whose $i$-th component of the output is defined to be\n",
    "$$\n",
    "\\text{softmax}_i(x) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}.\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "We can apply feedforward NNs to classification and regression problems the same way we did logistic regression.\n",
    "\n",
    "That is, we use a training dataset to define a loss function $L$ that depends on NN's weights. We then train the model by minimizing $L$, using SGD or Adam for example.\n",
    "\n",
    "In order to use SGD or Adam we need an efficient way to compute the partial derivatives of $L$ with respect to weights and biases. This is done using an algorithm called backpropagation.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "The hardest part of backpropagation is not getting lost in notation.\n",
    "\n",
    "To this end for a function $f:\\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}$ define\n",
    "$$\n",
    "  \\frac{\\partial f}{\\partial W}(W) = \\begin{pmatrix}\n",
    "    \\frac{\\partial f}{\\partial w_{11}} & \\dots & \\frac{\\partial f}{\\partial w_{1n}} \\\\\n",
    "    \\dots & \\dots & \\dots \\\\\n",
    "    \\frac{\\partial f}{\\partial w_{m1}} & \\dots & \\frac{\\partial f}{\\partial w_{mn}}\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Suppose $y$ is an $m$ dimensional row vector and $x$ is an $n$ dimensional column vector. It is easy to show that\n",
    "$$\n",
    "y \\frac{\\partial}{\\partial W}(Wx)=\\frac{\\partial}{\\partial W}(yWx)=y^Tx^T.\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Other useful derivative for us will be\n",
    "$$\n",
    "\\frac{\\partial A(x)}{\\partial x} = \\frac{\\partial}{\\partial x}(Wx) + \\frac{\\partial b}{\\partial x} = W + 0 = W,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\frac{\\partial A(x)}{\\partial x}\n",
    "$$\n",
    "is the Jacobian of $A$.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "If $f$ is an activation function and $x$ is an dimensional vector, then\n",
    "$$\n",
    "\\frac{f(x)}{\\partial x} = \\text{diag}(f'(x)),\n",
    "$$\n",
    "where again \n",
    "$$\n",
    "\\frac{\\partial f(x)}{\\partial x}\n",
    "$$\n",
    "is the Jacobian of $f$ and $\\text{diag}(f'(x))$ is a diagonal $n$ by $n$ matrix with $f'(x)$ on the diagonal.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "For concreteness, we will illustrate backpropagation by example. The general case is the same.\n",
    "\n",
    "Let's figure out the formulas for backpropagation to train a model with one hidden layer for a classification problem. Our model looks like this:\n",
    "\n",
    "![](../images/fnn.png){fig-align=\"center\"}\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "So we have two affine functions $A^{[1]}$ and $A^{[2]}$ and two activation functions $f^{[1]}$ and $f^{[2]}$ (in practice $f^{[2]}$ is usually the identity function if number of classes is gereater than 2). The first pair is for the hidden layer, the second pair is for the output layer.\n",
    "\n",
    "We can compute the output of the model using\n",
    "$$\n",
    "N(x) = f^{[2]} \\circ A^{[2]} \\circ f^{[1]} \\circ A^{[1]}(x).\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "To compute the confidences of the classes we can apply softmax to $N(x),$ i.e.\n",
    "$$\n",
    "\\text{confidences}(x) = \\text{softmax} \\circ N(x).\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "We are going to use cross-entropy for loss. Also, in implementations, cross-entropy and softmax are usually combined because the derivatives are much nicer than computing the derivatives of cross-entropy and softmax separately.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Suppose vectors $\\{y, x\\} = \\{y^i, x^i\\}_{i=1}^{n}$ represent the training data. So the vectors $y^i$ have 0 in all components except one where it has the value 1. Suppose the dimension of $y^i$ is $m$. Then our loss function will be\n",
    "$$\n",
    "L(W; N(x)) = \\frac{1}{n} \\sum_{i=1}^{n} - \\log \\frac{\\langle y^i, e^{N(x^i; W)} \\rangle}{\\sum_{j=1}^n e^{N(x^i; W)_j}}\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Then the Jacobian of $L(W; y_{pred})$ with respect to $y_{pred}$ can be computed as\n",
    "$$\n",
    "  \\frac{\\partial L(W; y_{pred})}{\\partial y_{pred}} = \\frac{1}{n} \\sum_{i=1}^n \\text{softmax}(y_{pred}^i) - y^i,\n",
    "$$\n",
    "where the Jacobian is a vector of same dimension as $y^i$ and the sum is componentwise. We interpret this vector as a row vector (we need to do this for matrix multiplication to be defined properly in later formulas).\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "In order to train our NN we need to compute\n",
    "$$\n",
    "\\frac{\\partial L(W)}{\\partial W^{[1]}}, \\ \\frac{\\partial L(W)}{\\partial b^{[1]}}, \\ \\frac{\\partial L(W)}{\\partial W^{[2]}}, \\ \\frac{\\partial L(W)}{\\partial b^{[2]}}.\n",
    "$$\n",
    "\n",
    "We can do this using the chain rule and formula\n",
    "$$\n",
    "y \\frac{\\partial}{\\partial W}(Wx)=y^Tx^T.\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "We compute\n",
    "\n",
    "![](../images/backprop1.png){fig-align=\"center\"}\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Then \n",
    "\n",
    "![](../images/backprop2.png){fig-align=\"center\"}\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Similarly\n",
    "\n",
    "![](../images/backprop3.png){fig-align=\"center\"}\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Also\n",
    "\n",
    "![](../images/backprop4.png){fig-align=\"center\"}\n",
    "\n",
    "and\n",
    "\n",
    "![](../images/backprop5.png){fig-align=\"center\"}\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "So the full backpropagation algorithm consists of two steps:\n",
    "\n",
    "1. Forward pass: Compute the values of\n",
    "\\begin{equation*}\n",
    "  \\begin{split}\n",
    "  &A^{[1]}(x), \\ f^{[1]} \\circ A^{[1]}(x), \\\\\n",
    "  &A^{[2]} \\circ f^{[1]} \\circ A^{[1]}(x), \\ f^{[2]} \\circ A^{[2]} \\circ f^{[1]} \\circ A^{[1]}(x).\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "2. Backward pass: Use the values from step one and derived formulas to compute the partial derivatives in terms of weights and biases.\n",
    "\n",
    "Once you have the partial derivatives you can, for example, plug them into SGD to minimize the loss and train the model.\n",
    "\n",
    "## Vanishing gradient problem\n",
    "\n",
    "In practice, you gain more performance gains by increasing the number of layers instead of increasing the number of weights in existing layers.\n",
    "\n",
    "However, training NNs with a lot of layers requires you to use extra tricks because of the vanishing gradient problem.\n",
    "\n",
    "## Vanishing gradient problem\n",
    "\n",
    "Suppose you have a super deep neural network and you used a logistic function as an activation function for the hidden layers.\n",
    "The logistic function has the derivative\n",
    "$$\n",
    "  \\left(\\frac{1}{1+e^{-x}}\\right)'=\\frac{e^{-x}}{(1+e^{-x})^2}.\n",
    "$$\n",
    "This function's range is $(0, 0.25)$.\n",
    "\n",
    "So, when performing backpropagation you will have multiplied your values by a number $< 0.25$ a bunch of times by the time you get to computing the partial derivatives of weights in the early layers of your model.\n",
    "\n",
    "## Vanishing gradient problem\n",
    "\n",
    "In consequence, the gradients of the early layers will be exponentially smaller than the gradients of the later levels and the model will have problems utilizing those early layers when learning.\n",
    "\n",
    "This is an example of the vanishing gradient problem. You might also encounter the opposite problem - the exploding gradient problem.\n",
    "\n",
    "## Vanishing gradient problem\n",
    "\n",
    "This is why ReLU is popular - the output of its derivative is either 0 or 1, so it doesn't cause vanishing gradients.\n",
    "It also sparses out the matrices when computing the gradients.\n",
    "\n",
    "However, ReLU has its own problems. For example, since the output of the derivative is 0 when input is negative, it might happen that some perceptrons become inactive for all inputs in the training set and then the gradient for those percetrons' weights are always 0 - the perceptrons \"die\".\n",
    "\n",
    "## Vanishing gradient problem\n",
    "\n",
    "There are other tricks for addressing the vanishing / exploding gradient problem. We will cover them in later chapters.\n",
    "\n",
    "## Universal Approximation Theorem\n",
    "\n",
    "In practice more layers leads to better models, but in theory only one hidden layer is enough, this is because of the Universal Approximation Theorem.\n",
    "\n",
    "This theorem states that for every continuous function defined on a compact (bounded and closed) subset of $\\mathbb{R}^n$ taking values in $\\mathbb{R}^m$ you can find a neural network with one hidden layer that approximates your target function as good as you want as long as the activation function you are using is continuous and not a polynomial.\n",
    "\n",
    "I.e. one hidden layer is enough to learn any continuous function.\n",
    "\n",
    "## Universal Approximation Theorem\n",
    "\n",
    "Note that the name of the theorem is clickbait, because you can do the same thing using a polynomial (Stone–Weierstrass theorem) or a piece-wise affine function (reason why graphing libraries work).\n",
    "\n",
    "The feature of neural networks that is interesting and useful is that they tend to generalize well to data that was not seen during training, while polynomials or piece-wise affine functions don't.\n",
    "\n",
    "## Stuff to watch\n",
    "\n",
    "- [Explanation of backpropagation](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    "- [Explanation of the proof of Universal Approximation Theorem](https://www.youtube.com/watch?v=NNBftbvBRTw)\n",
    "\n",
    "## Practice task\n",
    "\n",
    "Try to implement your own feedforward NN with one hidden layer for a classification task. Use the formulas we derived to perform backpropagation and SGD to minimize the loss function.\n",
    "\n",
    "Make the hidden layer twice as big as the input layer. Use ReLU for the activation function of the first layer. Let the activation function of the output layer be the identity function, i.e. $f^{[2]}(x)=x.$\n",
    "\n",
    "## Practice task\n",
    "\n",
    "Some notes:\n",
    "\n",
    "1. In theoretical considerations it is convenient to consider $x$ to be a column vector. However, in practical implementations it is more convenient to consider $x$ to be a row vector.\n",
    "\n",
    "2. Numpy arrays have matrix multiplication operator `@`.\n",
    "\n",
    "## Practice task\n",
    "\n",
    "3. You can generate some mock data for testing your implementation using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2015bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10000, 20), y shape: (10000, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=10, n_classes=3, n_redundant=10, random_state=34)\n",
    "y = OneHotEncoder(sparse_output=False).fit_transform(y.reshape(-1, 1))\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
