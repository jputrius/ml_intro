<!DOCTYPE html>
<html lang="en"><head>
<script src="06_natural_language_processing_files/libs/clipboard/clipboard.min.js"></script>
<script src="06_natural_language_processing_files/libs/quarto-html/tabby.min.js"></script>
<script src="06_natural_language_processing_files/libs/quarto-html/popper.min.js"></script>
<script src="06_natural_language_processing_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="06_natural_language_processing_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="06_natural_language_processing_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="06_natural_language_processing_files/libs/quarto-html/quarto-syntax-highlighting-c8ad9e5dbd60b7b70b38521ab19b7da4.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.34">

  <title>Chapter 6: Natural Language Processing</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="06_natural_language_processing_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="06_natural_language_processing_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="06_natural_language_processing_files/libs/revealjs/dist/theme/quarto-37f64f33d17d36c7c7406230333751da.css">
  <link href="06_natural_language_processing_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="06_natural_language_processing_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="06_natural_language_processing_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="06_natural_language_processing_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Chapter 6: Natural Language Processing</h1>

<div class="quarto-title-authors">
</div>

</section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
<p>In this chapter we begin discussing natural language processing.</p>
<p>We first cover TF-IDF. Next we discuss text embeddings and how to use them with feedforward neural networks.</p>
</section>
<section id="introduction-1" class="slide level2">
<h2>Introduction</h2>
<p>Natural language processing (NLP) refers to a set of tasks where the input is unstructured text. There are many different possible goals, for example we might wish to classify the text or translate it to some other language.</p>
<p>The first challenge we encounter is how to present text in a format on which we can apply machine learning models.</p>
<p>The classical approach here is TF-IDF so we begin by discussing it.</p>
</section>
<section id="tf-idf" class="slide level2">
<h2>TF-IDF</h2>
<p>TF-IDF is a product of two statistics - <strong>term frequency</strong> (TF) and <strong>inverse document frequency</strong> (IDF).</p>
<p>First we need some terminology:</p>
<ul>
<li><strong>Term</strong> - this is a single unit of text. Depending on the task this can vary. One obvious choice is splitting text into terms by words. However, there usually are smarter ways of defining what a term is depending on language and task. Nowadays the word <strong>token</strong> is used more commonly instead of term.</li>
</ul>
</section>
<section id="tf-idf-1" class="slide level2">
<h2>TF-IDF</h2>
<ul>
<li><strong>Document</strong> - a collection of terms, for us this is usually going to be a single input row.</li>
<li><strong>Corpus</strong> - the set of all documents.</li>
</ul>
</section>
<section id="tf-idf-2" class="slide level2">
<h2>TF-IDF</h2>
<p>Denote by <span class="math inline">\(f_{t, d}\)</span> the number of times the token <span class="math inline">\(t\)</span> appears in document <span class="math inline">\(d\)</span>.</p>
<p>Term frequency of token <span class="math inline">\(t\)</span> in document <span class="math inline">\(d\)</span> is defined to be <span class="math display">\[
\text{TF(t, d)} = \frac{f_{t, d}}{\sum_{t' \in d} f_{t', d}}.
\]</span> I.e. the more times the token <span class="math inline">\(t\)</span> appears in the document the higher its <span class="math inline">\(TF\)</span> will be.</p>
</section>
<section id="tf-idf-3" class="slide level2">
<h2>TF-IDF</h2>
<p>The inverse document frequency of token <span class="math inline">\(t\)</span> in corpus <span class="math inline">\(c\)</span> is defined to be <span class="math display">\[
\text{IDF(t, c)} = \log \left(\frac{\text{total number of documents in c}}{\text{number of documents in c that contain t}}\right)
\]</span> Inverse document frequency measures how rare the token is in the corpus. The less documents the token <span class="math inline">\(t\)</span> appears in the higher its <span class="math inline">\(IDF\)</span> will be.</p>
</section>
<section id="tf-idf-4" class="slide level2">
<h2>TF-IDF</h2>
<p>TF-IDF of a token <span class="math inline">\(t\)</span> in document <span class="math inline">\(d\)</span> is then <span class="math display">\[
\text{TF-IDF}(t, d, c) = \text{TF}(t, d)\text{IDF}(t, c).
\]</span></p>
<p>We apply TF-IDF in ML by first computing the IDF of all terms from the training set. We can then encode each input row as a vector of dimension equal to the number of tokens in our training set. Each component of this vector represents one token and contain the TF-IDF of that token.</p>
<p>In this way we represent our input row as a vector which we can then pass to a ML model.</p>
</section>
<section id="tf-idf-5" class="slide level2">
<h2>TF-IDF</h2>
<p>Let’s use TF-IDF for <strong>sentiment analysis</strong>. Our input will be tweet on some specific stock or financial markets in general. The goal is to classify wether the sentiment is bearish (meaning pessimistic), bullish (meaning optimistic) or neutral.</p>
<p>The dataset can be found <a href="https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment">here</a>.</p>
<div id="6adf3a25" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:351,&quot;referenced_widgets&quot;:[&quot;2990b5e56e204b04ab0297ef129f749b&quot;,&quot;b4fd1e98e0a1429ab4051de6e1230097&quot;,&quot;4989a9fd14a5416ca4dc0ac8f8ada4b1&quot;,&quot;cb55db8df1a44b19abfd3ec68bef3a1a&quot;,&quot;ce64bab8993b41dfadc1375b5b66f994&quot;,&quot;efc2f37f602b4cf6b681f9416a732e20&quot;,&quot;2b055c91fc854231a13197cb9ada40aa&quot;,&quot;57d255f2380d440b995e97ee8f1d4141&quot;,&quot;15ad7250e5b24da2bd81a2c89bd45b72&quot;,&quot;5be218ab7fd34430870e4130cebbfbb6&quot;,&quot;db471a0d5db54cf5b9d6cb6bacc76848&quot;,&quot;de40276b64314d4e8e4c40eae4eec18e&quot;,&quot;d413a5bb765d4dd78999bb7bc984c664&quot;,&quot;39b29e2aa4a24f7fbc49a5c16a00fbdc&quot;,&quot;4202d1662b6349028c32929b5031e086&quot;,&quot;f7855563e2ec4461b094be3f53b73015&quot;,&quot;8829eaf7b77443a794d893f88abb26bc&quot;,&quot;a849adecf5894e2ca694d71159146182&quot;,&quot;a889d7d9f151458eaae4ad5c5fbb0050&quot;,&quot;8f97eddcc40f458f82de666060c70650&quot;,&quot;ab9be7204e75494bb7d447edfc451d8a&quot;,&quot;d1454cfdba9d4948a44fb3c4c579ef00&quot;,&quot;4763af8410484a3fbd74977ab9e95cc3&quot;,&quot;945692102fe54239878780e1a72ccd7a&quot;,&quot;cc5853cf65794713bf718f31308e93c3&quot;,&quot;19173c0ee32245a6916c72b700b19c6b&quot;,&quot;6fa14e56224341dc87bf7b91e857d996&quot;,&quot;9f8cd581d95f472eafb5d312634b6360&quot;,&quot;b0ee68077f5442a98a6263da8d28db6f&quot;,&quot;302e984304174c369694a8ea90b06980&quot;,&quot;9fc7a77f0136403f9a870436a734774e&quot;,&quot;db7c995a369a44d8b285638c8f4c08d5&quot;,&quot;f0c175431fb54db9b2634241902af48f&quot;,&quot;4acf432320ad4d55a93b696878047a6c&quot;,&quot;3d90566c941142aab9c79c2d41571e60&quot;,&quot;efb15f862a794bc69ab6eb7aa72d2b97&quot;,&quot;7c3a34c77e22414e87ae488a01ac9193&quot;,&quot;1bf8eea8fa0c4611a555392d3e60d36b&quot;,&quot;c9b84b56989d48c58da3ecb90372b30b&quot;,&quot;3d92e8ec7e834a87ae8f48720e95b0e4&quot;,&quot;e0f52f6574cc47588bd11142df0a9a1f&quot;,&quot;64fa021d06504c1897166bd770de8c65&quot;,&quot;fc3a1d86947d4de58a3431595a2e3f6b&quot;,&quot;08e2e62fb73849cd8652fbd516906f8c&quot;,&quot;d10b3ff73c264775959ee61f46d79350&quot;,&quot;f0f230b4ba3d4b30aa1684613374730c&quot;,&quot;77a1c43466bd48e893b976dbe70ab476&quot;,&quot;1090ac965875411e9ee659ef7536cb34&quot;,&quot;af6da04c0d2f44e796bcc76f2f18dd5d&quot;,&quot;0b7350eecc95449c9ef126350f2945b4&quot;,&quot;c6cb01024a374394932076cbc7a08077&quot;,&quot;7f5f511d9159491dab3d2820991e2faa&quot;,&quot;593132bf0c3448648ba5cdf9f996f44c&quot;,&quot;cb854a7924184f6294ba02c5f75fd3e3&quot;,&quot;7db3a4ba8b4144d1895a890bba97c432&quot;,&quot;585e2c58d9ad4acd8479f9e40cb9eb81&quot;,&quot;ae9032e7aea04c67a0d25fca543c8819&quot;,&quot;68946fc2be48404e898b2b29b5de4d3a&quot;,&quot;4eee89a08ae44688969d02004f3ef266&quot;,&quot;ae584a56cdb841ffb321c6445b658ab7&quot;,&quot;0536714f783545ff85dbf5352a0b283e&quot;,&quot;68b5b64e79024ed8b6b95a110378d287&quot;,&quot;29178bee0ff64bda8790927481fd8db4&quot;,&quot;f796bc7de727493884fd5b7ac143466b&quot;,&quot;817ec2f0b9534594b4132549016a055b&quot;,&quot;21d1e9de3f7b48159f79fa24fe5664b4&quot;,&quot;77d52acf1dc24900a31f301d9dc45ca1&quot;,&quot;fab924503e3c43489f3283f8ed7abb2e&quot;,&quot;d075285778b24a9ba6e827a259d1eab1&quot;,&quot;8cff3a8fe3bb4b4780718d9b1ada0a40&quot;,&quot;f82a7c7d723d490d9ee56a1e90c85509&quot;,&quot;7f1c28331a454b96a6b3e07762b0ba70&quot;,&quot;d3a878b2f1884bee933a81ca455f7793&quot;,&quot;8678bfde74ce44cf9b410d959abe9db6&quot;,&quot;77d8054404c44692b35275a50851e7c9&quot;,&quot;1709652f8e864cfebb3e7f01ab475bd1&quot;,&quot;176d972fdeef4adfac584fc846543f4b&quot;]}}" data-outputid="90b3f2c5-3763-4c7a-e02c-f0ffb69e4419" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href=""></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href=""></a></span>
<span id="cb1-3"><a href=""></a>train <span class="op">=</span> pd.read_csv(<span class="st">"hf://datasets/zeroshot/twitter-financial-news-sentiment/sent_train.csv"</span>)</span>
<span id="cb1-4"><a href=""></a>test <span class="op">=</span> pd.read_csv(<span class="st">"hf://datasets/zeroshot/twitter-financial-news-sentiment/sent_valid.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="tf-idf-6" class="slide level2">
<h2>TF-IDF</h2>
<p>There is an implementation of TF-IDF in <code>sklearn</code>.</p>
<p>Let’s use logistic regression for classification after we transform our input using TF-IDF.</p>
</section>
<section id="tf-idf-7" class="slide level2">
<h2>TF-IDF</h2>
<div id="7338e8e5" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="51d515e5-d569-4d5c-852e-2538a0e5a8f1" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href=""></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb2-2"><a href=""></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb2-3"><a href=""></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb2-4"><a href=""></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb2-5"><a href=""></a></span>
<span id="cb2-6"><a href=""></a>X_train, y_train <span class="op">=</span> train[<span class="st">"text"</span>], train[<span class="st">"label"</span>]</span>
<span id="cb2-7"><a href=""></a>X_test, y_test <span class="op">=</span> test[<span class="st">"text"</span>], test[<span class="st">"label"</span>]</span>
<span id="cb2-8"><a href=""></a></span>
<span id="cb2-9"><a href=""></a><span class="kw">def</span> make_pipeline():</span>
<span id="cb2-10"><a href=""></a>  model <span class="op">=</span> LogisticRegression(random_state<span class="op">=</span><span class="dv">34</span>, class_weight<span class="op">=</span><span class="st">"balanced"</span>)</span>
<span id="cb2-11"><a href=""></a></span>
<span id="cb2-12"><a href=""></a>  pipeline <span class="op">=</span> Pipeline(</span>
<span id="cb2-13"><a href=""></a>    steps<span class="op">=</span>[</span>
<span id="cb2-14"><a href=""></a>      (<span class="st">"transform"</span>, TfidfVectorizer(lowercase<span class="op">=</span><span class="va">True</span>)),</span>
<span id="cb2-15"><a href=""></a>      (<span class="st">"model"</span>, model),</span>
<span id="cb2-16"><a href=""></a>    ],</span>
<span id="cb2-17"><a href=""></a>  )</span>
<span id="cb2-18"><a href=""></a></span>
<span id="cb2-19"><a href=""></a>  <span class="cf">return</span> pipeline</span>
<span id="cb2-20"><a href=""></a></span>
<span id="cb2-21"><a href=""></a>pipeline <span class="op">=</span> make_pipeline()</span>
<span id="cb2-22"><a href=""></a>pipeline.fit(X_train, y_train)</span>
<span id="cb2-23"><a href=""></a>y_pred <span class="op">=</span> pipeline.predict(X_test)</span>
<span id="cb2-24"><a href=""></a><span class="bu">print</span>(classification_report(y_test, y_pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

</section>
<section id="tf-idf-7-output" class="slide level2 output-location-slide"><h2>TF-IDF</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="51d515e5-d569-4d5c-852e-2538a0e5a8f1" data-execution_count="2">
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

           0       0.56      0.69      0.62       347
           1       0.66      0.73      0.70       475
           2       0.91      0.83      0.87      1566

    accuracy                           0.79      2388
   macro avg       0.71      0.75      0.73      2388
weighted avg       0.81      0.79      0.80      2388
</code></pre>
</div>
</div></section><section id="embeddings" class="slide level2">
<h2>Embeddings</h2>
<p>TF-IDF is a good technique if you need to process a lot of data quickly and cheaply and are satisfied with mediocre performance.</p>
<p>One obvious drawback of TF-IDF is that we loose all information on the order of words in a sentence.</p>
</section>
<section id="embeddings-1" class="slide level2">
<h2>Embeddings</h2>
<p>In certain languages (such as English) word order is important to understand the meanings of sentences.</p>
<p>For example, the following two sentences would have the same TF-IDF but their meaning is different:</p>
<ol type="1">
<li>Dog chases cat.</li>
<li>Cat chases dog.</li>
</ol>
</section>
<section id="embeddings-2" class="slide level2">
<h2>Embeddings</h2>
<p>To keep the order of words we need to pass tokens to the model sequentially. We then represent tokens using <strong>embeddings</strong>. That is, to every token we assign a vector in <span class="math inline">\(\mathbb{R}^n.\)</span></p>
<p>There are many different ways for generating token embeddings (outdated historic example <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a>).</p>
<p>In our case we will let the model learn its own embeddings.</p>
</section>
<section id="embeddings-3" class="slide level2">
<h2>Embeddings</h2>
<p>To pass tokens to the model we will one hot encode them (where the dimension of the one hot encoded vector will be equal to the number of unique tokens in our training set) and then project this vector down to a space of lower dimension using a linear transformation. This will give us an embedding.</p>
<p>The model will then be able to learn this embedding on its own by learning the weights of the matrix used to project the one hot encoded tokens.</p>
</section>
<section id="embeddings-4" class="slide level2">
<h2>Embeddings</h2>
<p>This is implemented in <code>PyTorch</code> in the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html">Embedding layer</a>.</p>
<p>In order to use it we need to convert our input string into a tensor of token indices.</p>
<p>Let’s write a <code>Dictionary</code> class that is going to assign an index to each token we encounter in the training set and convert a string into a list of token indices.</p>
<p>We will use the default English tokenizer supplied by <code>spaCy</code> package. However, note that there are much better tokenizers available.</p>
</section>
<section id="embeddings-5" class="slide level2">
<h2>Embeddings</h2>
<div id="6fb146b4" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href=""></a><span class="im">from</span> spacy.lang.en <span class="im">import</span> English</span>
<span id="cb4-2"><a href=""></a></span>
<span id="cb4-3"><a href=""></a><span class="kw">class</span> Dictionary:</span>
<span id="cb4-4"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, min_count<span class="op">=</span><span class="dv">10</span>, init_tokens<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-5"><a href=""></a>    <span class="va">self</span>.nlp <span class="op">=</span> English()</span>
<span id="cb4-6"><a href=""></a>    <span class="va">self</span>.min_count <span class="op">=</span> min_count</span>
<span id="cb4-7"><a href=""></a>    <span class="va">self</span>.init_tokens <span class="op">=</span> init_tokens</span>
<span id="cb4-8"><a href=""></a>    <span class="va">self</span>.i2t, <span class="va">self</span>.t2i, <span class="va">self</span>.no_tokens <span class="op">=</span> <span class="va">self</span>._default_maps()</span>
<span id="cb4-9"><a href=""></a>    <span class="va">self</span>.pad_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-10"><a href=""></a>    <span class="va">self</span>.unk_idx <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-11"><a href=""></a></span>
<span id="cb4-12"><a href=""></a>  <span class="kw">def</span> _default_maps(<span class="va">self</span>):</span>
<span id="cb4-13"><a href=""></a>    <span class="co"># &lt;pad&gt; - token used for padding</span></span>
<span id="cb4-14"><a href=""></a>    <span class="co"># &lt;unk&gt; - unknown, used for tokens not encountered in dictionary building</span></span>
<span id="cb4-15"><a href=""></a>    i2t <span class="op">=</span> [<span class="st">'&lt;pad&gt;'</span>, <span class="st">'&lt;unk&gt;'</span>]</span>
<span id="cb4-16"><a href=""></a>    <span class="cf">if</span> <span class="va">self</span>.init_tokens <span class="op">!=</span> <span class="va">None</span>:</span>
<span id="cb4-17"><a href=""></a>      i2t <span class="op">=</span> [<span class="op">*</span>i2t, <span class="op">*</span><span class="va">self</span>.init_tokens]</span>
<span id="cb4-18"><a href=""></a>    t2i <span class="op">=</span> {token:index <span class="cf">for</span> index, token <span class="kw">in</span> <span class="bu">enumerate</span>(i2t)}</span>
<span id="cb4-19"><a href=""></a>    <span class="cf">return</span> i2t, t2i, <span class="bu">len</span>(i2t)</span>
<span id="cb4-20"><a href=""></a></span>
<span id="cb4-21"><a href=""></a>  <span class="kw">def</span> build(<span class="va">self</span>, corpus):</span>
<span id="cb4-22"><a href=""></a>    tokens <span class="op">=</span> {}</span>
<span id="cb4-23"><a href=""></a>    <span class="cf">for</span> idx, row <span class="kw">in</span> <span class="bu">enumerate</span>(corpus):</span>
<span id="cb4-24"><a href=""></a>      <span class="cf">for</span> token <span class="kw">in</span> <span class="va">self</span>.nlp(row):</span>
<span id="cb4-25"><a href=""></a>        <span class="cf">if</span> token.text <span class="kw">not</span> <span class="kw">in</span> tokens:</span>
<span id="cb4-26"><a href=""></a>          tokens[token.text] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-27"><a href=""></a>        <span class="cf">else</span>:</span>
<span id="cb4-28"><a href=""></a>          tokens[token.text] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-29"><a href=""></a>    i2t, _, _ <span class="op">=</span> <span class="va">self</span>._default_maps()</span>
<span id="cb4-30"><a href=""></a>    <span class="va">self</span>.i2t <span class="op">=</span> [</span>
<span id="cb4-31"><a href=""></a>      <span class="op">*</span>i2t,</span>
<span id="cb4-32"><a href=""></a>      <span class="op">*</span>[token <span class="cf">for</span> token, count <span class="kw">in</span> tokens.items() <span class="cf">if</span> count <span class="op">&gt;=</span> <span class="va">self</span>.min_count]</span>
<span id="cb4-33"><a href=""></a>    ]</span>
<span id="cb4-34"><a href=""></a>    <span class="va">self</span>.t2i <span class="op">=</span> {token:index <span class="cf">for</span> index, token <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.i2t)}</span>
<span id="cb4-35"><a href=""></a>    <span class="va">self</span>.no_tokens <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.i2t)</span>
<span id="cb4-36"><a href=""></a></span>
<span id="cb4-37"><a href=""></a>  <span class="kw">def</span> string_to_idx(<span class="va">self</span>, string, seq_length<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-38"><a href=""></a>    tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> <span class="va">self</span>.nlp(string) <span class="cf">if</span> <span class="kw">not</span> token.is_punct]</span>
<span id="cb4-39"><a href=""></a>    <span class="cf">return</span> <span class="va">self</span>.tokens_to_idx(tokens, seq_length)</span>
<span id="cb4-40"><a href=""></a></span>
<span id="cb4-41"><a href=""></a>  <span class="kw">def</span> tokens_to_idx(<span class="va">self</span>, tokens, seq_length<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-42"><a href=""></a>    idxs <span class="op">=</span> [<span class="va">self</span>.t2i[token] <span class="cf">if</span> token <span class="kw">in</span> <span class="va">self</span>.t2i <span class="cf">else</span> <span class="va">self</span>.unk_idx <span class="cf">for</span> token <span class="kw">in</span> tokens]</span>
<span id="cb4-43"><a href=""></a>    <span class="cf">if</span> seq_length <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-44"><a href=""></a>      idxs <span class="op">=</span> idxs <span class="op">+</span> [<span class="va">self</span>.pad_idx] <span class="op">*</span> (seq_length <span class="op">-</span> <span class="bu">len</span>(idxs))</span>
<span id="cb4-45"><a href=""></a>      idxs <span class="op">=</span> idxs[:seq_length]</span>
<span id="cb4-46"><a href=""></a>    <span class="cf">return</span> idxs</span>
<span id="cb4-47"><a href=""></a></span>
<span id="cb4-48"><a href=""></a>  <span class="kw">def</span> idx_to_string(<span class="va">self</span>, indices, ignore_pad<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb4-49"><a href=""></a>    tokens <span class="op">=</span> <span class="va">self</span>.idx_to_tokens(indices, ignore_pad)</span>
<span id="cb4-50"><a href=""></a>    <span class="cf">return</span> tokens.join(<span class="st">' '</span>)</span>
<span id="cb4-51"><a href=""></a></span>
<span id="cb4-52"><a href=""></a>  <span class="kw">def</span> idx_to_tokens(<span class="va">self</span>, indices, ignore_pad<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb4-53"><a href=""></a>    <span class="cf">if</span> ignore_pad:</span>
<span id="cb4-54"><a href=""></a>      <span class="cf">return</span> [<span class="va">self</span>.i2t[idx] <span class="cf">for</span> idx <span class="kw">in</span> indices <span class="cf">if</span> idx <span class="op">!=</span> <span class="va">self</span>.pad_idx]</span>
<span id="cb4-55"><a href=""></a>    <span class="cf">return</span> [<span class="va">self</span>.i2t[idx] <span class="cf">for</span> idx <span class="kw">in</span> indices]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="embeddings-6" class="slide level2">
<h2>Embeddings</h2>
<p>One more problem is that now our inputs have variable lengths. If we want to use a feedforward NN to classify our data, then we need the input dimensions to be fixed.</p>
<p>To get around this we will fix the input size. The inputs that are shorter than this size will be padded with a special padding token which the model will (hopefully) learn to ignore. And longer inputs will be truncated.</p>
</section>
<section id="embeddings-7" class="slide level2">
<h2>Embeddings</h2>
<p>Another way to handle inputs of arbitrary length would be to use a recurrent neural network (RNN) of some sort. We will cover RNNs in a later chapter.</p>
<p>We also need to write a custom Dataset class to represent our data. This is easy. All we need to do is to inherit from the <code>Dataset</code> class and implement the <code>__getitem__</code> and <code>__len__</code> methods.</p>
</section>
<section id="embeddings-8" class="slide level2">
<h2>Embeddings</h2>
<div id="20ccaa7c" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:81,&quot;referenced_widgets&quot;:[&quot;088a5add4c2b4c8bba79f2cb20c8687a&quot;,&quot;e78d63fe5da7490cae2e986f235bc446&quot;,&quot;d2859ec1aeb14cc594c5f65ab5b30f2f&quot;,&quot;0acbce8a74d848b18648de1eccaba0ad&quot;,&quot;81017da71b1b4f58b3a1d285b6f4aba7&quot;,&quot;a639d4cd177743cc9e4adde406cfc331&quot;,&quot;9d4b4a463ed242b9adec35e52553ecf8&quot;,&quot;8d22972b1593440bb66ab5581e731dc2&quot;,&quot;504c38a814f344a8aca46e8846ab38ab&quot;,&quot;75f3c3bbed464591aa6184167afaaa26&quot;,&quot;2fe6f90e23ce4baeaa1862e4d0c364e5&quot;,&quot;bde31e4861b24b9cbcaddacd0b981353&quot;,&quot;d863a2ec094547e3bf96038945b03ed1&quot;,&quot;f85c4ad4a78547118837e2fab19e584b&quot;,&quot;79256142853c45619f3ad924f957bb45&quot;,&quot;41c89cd86a4f45b397ddde557cf7828d&quot;,&quot;edadf1ef836b40798324beaeb0878539&quot;,&quot;ef76adb13a6a4ff1ae2608f69c081cc9&quot;,&quot;c2b3839430e443deb4d2e34b5f4d8821&quot;,&quot;9fa10d556cc74b288bbff7969bff6a85&quot;,&quot;399b259bf34343e9861f780a5c876942&quot;,&quot;0c55898ae30f4647a8f888f4b7536f04&quot;]}}" data-outputid="25607928-1937-4289-9019-d9f60a74d931" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href=""></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href=""></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb5-3"><a href=""></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb5-4"><a href=""></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-5"><a href=""></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-6"><a href=""></a></span>
<span id="cb5-7"><a href=""></a><span class="kw">class</span> Tweets(Dataset):</span>
<span id="cb5-8"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, seq_length, train<span class="op">=</span><span class="va">False</span>, dictionary<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb5-9"><a href=""></a>    <span class="va">self</span>.seq_length <span class="op">=</span> seq_length</span>
<span id="cb5-10"><a href=""></a></span>
<span id="cb5-11"><a href=""></a>    <span class="cf">if</span> train:</span>
<span id="cb5-12"><a href=""></a>      <span class="va">self</span>.dataset <span class="op">=</span> pd.read_csv(<span class="st">"hf://datasets/zeroshot/twitter-financial-news-sentiment/sent_train.csv"</span>)</span>
<span id="cb5-13"><a href=""></a>    <span class="cf">else</span>:</span>
<span id="cb5-14"><a href=""></a>      <span class="va">self</span>.dataset <span class="op">=</span> pd.read_csv(<span class="st">"hf://datasets/zeroshot/twitter-financial-news-sentiment/sent_valid.csv"</span>)</span>
<span id="cb5-15"><a href=""></a></span>
<span id="cb5-16"><a href=""></a>    <span class="cf">if</span> dictionary <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb5-17"><a href=""></a>      <span class="va">self</span>.dictionary <span class="op">=</span> Dictionary()</span>
<span id="cb5-18"><a href=""></a>      <span class="va">self</span>.dictionary.build(<span class="va">self</span>.dataset[<span class="st">"text"</span>])</span>
<span id="cb5-19"><a href=""></a>    <span class="cf">else</span>:</span>
<span id="cb5-20"><a href=""></a>      <span class="va">self</span>.dictionary <span class="op">=</span> dictionary</span>
<span id="cb5-21"><a href=""></a></span>
<span id="cb5-22"><a href=""></a>    <span class="va">self</span>.no_tokens <span class="op">=</span> <span class="va">self</span>.dictionary.no_tokens</span>
<span id="cb5-23"><a href=""></a></span>
<span id="cb5-24"><a href=""></a>  <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb5-25"><a href=""></a>    <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.dataset)</span>
<span id="cb5-26"><a href=""></a></span>
<span id="cb5-27"><a href=""></a>  <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb5-28"><a href=""></a>    tokens <span class="op">=</span> <span class="va">self</span>.dictionary.string_to_idx(<span class="va">self</span>.dataset.iloc[idx][<span class="st">"text"</span>], seq_length<span class="op">=</span><span class="va">self</span>.seq_length)</span>
<span id="cb5-29"><a href=""></a>    tokens <span class="op">=</span> torch.LongTensor(tokens)</span>
<span id="cb5-30"><a href=""></a></span>
<span id="cb5-31"><a href=""></a>    label <span class="op">=</span> <span class="va">self</span>.dataset.iloc[idx][<span class="st">"label"</span>]</span>
<span id="cb5-32"><a href=""></a>    label <span class="op">=</span> torch.zeros(<span class="dv">3</span>, dtype<span class="op">=</span>torch.<span class="bu">float</span>).scatter_(<span class="dv">0</span>, torch.tensor(label), value<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-33"><a href=""></a></span>
<span id="cb5-34"><a href=""></a>    <span class="cf">return</span> tokens, label</span>
<span id="cb5-35"><a href=""></a></span>
<span id="cb5-36"><a href=""></a>seq_length <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb5-37"><a href=""></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb5-38"><a href=""></a></span>
<span id="cb5-39"><a href=""></a>train_data <span class="op">=</span> Tweets(</span>
<span id="cb5-40"><a href=""></a>  seq_length<span class="op">=</span>seq_length,</span>
<span id="cb5-41"><a href=""></a>  train<span class="op">=</span><span class="va">True</span></span>
<span id="cb5-42"><a href=""></a>)</span>
<span id="cb5-43"><a href=""></a></span>
<span id="cb5-44"><a href=""></a>test_data <span class="op">=</span> Tweets(</span>
<span id="cb5-45"><a href=""></a>  seq_length<span class="op">=</span>seq_length,</span>
<span id="cb5-46"><a href=""></a>  train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb5-47"><a href=""></a>  dictionary<span class="op">=</span>train_data.dictionary</span>
<span id="cb5-48"><a href=""></a>)</span>
<span id="cb5-49"><a href=""></a></span>
<span id="cb5-50"><a href=""></a>train_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb5-51"><a href=""></a>  train_data,</span>
<span id="cb5-52"><a href=""></a>  batch_size<span class="op">=</span>batch_size,</span>
<span id="cb5-53"><a href=""></a>  shuffle<span class="op">=</span><span class="va">True</span></span>
<span id="cb5-54"><a href=""></a>)</span>
<span id="cb5-55"><a href=""></a></span>
<span id="cb5-56"><a href=""></a>test_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb5-57"><a href=""></a>  test_data,</span>
<span id="cb5-58"><a href=""></a>  batch_size<span class="op">=</span>batch_size,</span>
<span id="cb5-59"><a href=""></a>  shuffle<span class="op">=</span><span class="va">False</span></span>
<span id="cb5-60"><a href=""></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="cnns-for-nlp" class="slide level2">
<h2>CNNs For NLP</h2>
<p>So now our input is a matrix of fixed dimension. This is very similar to an image. Maybe a convolutional neural network (CNN) might work for our task?</p>
<p>Convolutional layers indeed have properties that we want:</p>
<ol type="1">
<li>They take the order of inputs into account, hence the model can take word order into account.</li>
<li>They can easily learn patterns irrespective where they occur in the input. In our case the model should be able to learn that seeing something like “this movie is bad” anywhere in the text means that the review is negative.</li>
</ol>
</section>
<section id="cnns-for-nlp-1" class="slide level2">
<h2>CNNs For NLP</h2>
<p>Nowadays, the go to mechanism for handling text is called attention. We will cover attention in the next chapter when we talk about transformers. For now let’s stick with CNNs.</p>
<p>Let’s build our model.</p>
</section>
<section id="cnns-for-nlp-2" class="slide level2">
<h2>CNNs For NLP</h2>
<div id="a1be6be1" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9740c2be-301a-4144-dbf6-b2861d76e70d" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href=""></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb6-2"><a href=""></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb6-3"><a href=""></a></span>
<span id="cb6-4"><a href=""></a><span class="kw">class</span> TextClassifier(nn.Module):</span>
<span id="cb6-5"><a href=""></a>  <span class="kw">def</span> _apply_conv_layer(<span class="va">self</span>, x, conv_layer):</span>
<span id="cb6-6"><a href=""></a>    x <span class="op">=</span> conv_layer(x)</span>
<span id="cb6-7"><a href=""></a>    x <span class="op">=</span> F.relu(x).squeeze(<span class="dv">3</span>)</span>
<span id="cb6-8"><a href=""></a>    <span class="cf">return</span> F.max_pool1d(x, kernel_size <span class="op">=</span> x.shape[<span class="dv">2</span>]).squeeze(<span class="dv">2</span>)</span>
<span id="cb6-9"><a href=""></a></span>
<span id="cb6-10"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, no_tokens, seq_length):</span>
<span id="cb6-11"><a href=""></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-12"><a href=""></a>    <span class="va">self</span>.seq_length <span class="op">=</span> seq_length</span>
<span id="cb6-13"><a href=""></a>    <span class="va">self</span>.no_tokens <span class="op">=</span> no_tokens</span>
<span id="cb6-14"><a href=""></a>    <span class="va">self</span>.embed_dim <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb6-15"><a href=""></a>    <span class="va">self</span>.num_out_conv_channels <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb6-16"><a href=""></a>    <span class="va">self</span>.conv_kernel_sizes <span class="op">=</span> [<span class="dv">4</span>, <span class="dv">5</span>]</span>
<span id="cb6-17"><a href=""></a></span>
<span id="cb6-18"><a href=""></a>    <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(<span class="va">self</span>.no_tokens, <span class="va">self</span>.embed_dim)</span>
<span id="cb6-19"><a href=""></a>    <span class="va">self</span>.conv_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb6-20"><a href=""></a>      nn.Conv2d(</span>
<span id="cb6-21"><a href=""></a>        in_channels <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb6-22"><a href=""></a>        out_channels <span class="op">=</span> <span class="va">self</span>.num_out_conv_channels,</span>
<span id="cb6-23"><a href=""></a>        kernel_size <span class="op">=</span> (k, <span class="va">self</span>.embed_dim)</span>
<span id="cb6-24"><a href=""></a>      ) <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.conv_kernel_sizes</span>
<span id="cb6-25"><a href=""></a>    ]) <span class="co"># Use ModuleList if you need a list of some layers</span></span>
<span id="cb6-26"><a href=""></a>    <span class="va">self</span>.linear_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb6-27"><a href=""></a>      nn.Linear(<span class="bu">len</span>(<span class="va">self</span>.conv_kernel_sizes)<span class="op">*</span><span class="va">self</span>.num_out_conv_channels, <span class="dv">3</span>),</span>
<span id="cb6-28"><a href=""></a>      nn.ReLU(),</span>
<span id="cb6-29"><a href=""></a>    )</span>
<span id="cb6-30"><a href=""></a></span>
<span id="cb6-31"><a href=""></a>  <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-32"><a href=""></a>    x <span class="op">=</span> <span class="va">self</span>.embedding(x)</span>
<span id="cb6-33"><a href=""></a>    x <span class="op">=</span> x.unsqueeze(<span class="dv">1</span>) <span class="co"># Adds channel dimension</span></span>
<span id="cb6-34"><a href=""></a></span>
<span id="cb6-35"><a href=""></a>    x <span class="op">=</span> [<span class="va">self</span>._apply_conv_layer(x, conv_layer) <span class="cf">for</span> conv_layer <span class="kw">in</span> <span class="va">self</span>.conv_layers]</span>
<span id="cb6-36"><a href=""></a>    x <span class="op">=</span> torch.cat(x, <span class="dv">1</span>)</span>
<span id="cb6-37"><a href=""></a></span>
<span id="cb6-38"><a href=""></a>    <span class="cf">return</span> <span class="va">self</span>.linear_stack(x)</span>
<span id="cb6-39"><a href=""></a></span>
<span id="cb6-40"><a href=""></a><span class="bu">print</span>(TextClassifier(train_data.no_tokens, seq_length))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

</section>
<section id="cnns-for-nlp-2-output" class="slide level2 output-location-slide"><h2>CNNs For NLP</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9740c2be-301a-4144-dbf6-b2861d76e70d" data-execution_count="5">
<div class="cell-output cell-output-stdout">
<pre><code>TextClassifier(
  (embedding): Embedding(1774, 512)
  (conv_layers): ModuleList(
    (0): Conv2d(1, 128, kernel_size=(4, 512), stride=(1, 1))
    (1): Conv2d(1, 128, kernel_size=(5, 512), stride=(1, 1))
  )
  (linear_stack): Sequential(
    (0): Linear(in_features=256, out_features=3, bias=True)
    (1): ReLU()
  )
)</code></pre>
</div>
</div></section><section id="cnns-for-nlp-3" class="slide level2">
<h2>CNNs for NLP</h2>
<p>Let’s also copy over the code for training for one epoch from the last chapter.</p>
<div id="d308ba3c" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href=""></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm <span class="co"># This is a library that implements loading bars</span></span>
<span id="cb8-2"><a href=""></a><span class="im">import</span> sys</span>
<span id="cb8-3"><a href=""></a></span>
<span id="cb8-4"><a href=""></a><span class="kw">def</span> train_epoch(dataloader, model, loss_fn, optimizer):</span>
<span id="cb8-5"><a href=""></a>  model.train() <span class="co"># Set model to training mode</span></span>
<span id="cb8-6"><a href=""></a></span>
<span id="cb8-7"><a href=""></a>  total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-8"><a href=""></a>  total_batches <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-9"><a href=""></a></span>
<span id="cb8-10"><a href=""></a>  <span class="cf">with</span> tqdm(dataloader, unit<span class="op">=</span><span class="st">"batch"</span>, <span class="bu">file</span><span class="op">=</span>sys.stdout) <span class="im">as</span> ep_tqdm:</span>
<span id="cb8-11"><a href=""></a>    ep_tqdm.set_description(<span class="st">"Train"</span>)</span>
<span id="cb8-12"><a href=""></a>    <span class="cf">for</span> X, y <span class="kw">in</span> ep_tqdm:</span>
<span id="cb8-13"><a href=""></a>      X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb8-14"><a href=""></a></span>
<span id="cb8-15"><a href=""></a>      <span class="co"># Forward pass</span></span>
<span id="cb8-16"><a href=""></a>      pred <span class="op">=</span> model(X)</span>
<span id="cb8-17"><a href=""></a>      loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb8-18"><a href=""></a>        </span>
<span id="cb8-19"><a href=""></a>      <span class="co"># Backward pass</span></span>
<span id="cb8-20"><a href=""></a>      loss.backward()</span>
<span id="cb8-21"><a href=""></a>      optimizer.step()</span>
<span id="cb8-22"><a href=""></a></span>
<span id="cb8-23"><a href=""></a>      <span class="co"># Reset the computed gradients back to zero</span></span>
<span id="cb8-24"><a href=""></a>      optimizer.zero_grad()</span>
<span id="cb8-25"><a href=""></a></span>
<span id="cb8-26"><a href=""></a>      <span class="co"># Output stats</span></span>
<span id="cb8-27"><a href=""></a>      total_loss <span class="op">+=</span> loss</span>
<span id="cb8-28"><a href=""></a>      total_batches <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb8-29"><a href=""></a>      ep_tqdm.set_postfix(average_batch_loss<span class="op">=</span>(total_loss<span class="op">/</span>total_batches).item())</span>
<span id="cb8-30"><a href=""></a></span>
<span id="cb8-31"><a href=""></a><span class="kw">def</span> eval_epoch(dataloader, model, loss_fn):</span>
<span id="cb8-32"><a href=""></a>  model.<span class="bu">eval</span>() <span class="co"># Set model to inference mode</span></span>
<span id="cb8-33"><a href=""></a>  </span>
<span id="cb8-34"><a href=""></a>  total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-35"><a href=""></a>  total_batches <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-36"><a href=""></a>  total_samples <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-37"><a href=""></a>  total_correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-38"><a href=""></a></span>
<span id="cb8-39"><a href=""></a>  <span class="cf">with</span> torch.no_grad(): <span class="co"># Do not compute gradients</span></span>
<span id="cb8-40"><a href=""></a>    <span class="cf">with</span> tqdm(dataloader, unit<span class="op">=</span><span class="st">"batch"</span>, <span class="bu">file</span><span class="op">=</span>sys.stdout) <span class="im">as</span> ep_tqdm:</span>
<span id="cb8-41"><a href=""></a>      ep_tqdm.set_description(<span class="st">"Val"</span>)</span>
<span id="cb8-42"><a href=""></a>      <span class="cf">for</span> X, y <span class="kw">in</span> ep_tqdm:</span>
<span id="cb8-43"><a href=""></a>        X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb8-44"><a href=""></a>        pred <span class="op">=</span> model(X)</span>
<span id="cb8-45"><a href=""></a></span>
<span id="cb8-46"><a href=""></a>        total_loss <span class="op">+=</span> loss_fn(pred, y)</span>
<span id="cb8-47"><a href=""></a>        total_correct <span class="op">+=</span> (pred.argmax(dim<span class="op">=</span><span class="dv">1</span>) <span class="op">==</span> y.argmax(dim<span class="op">=</span><span class="dv">1</span>)).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>()</span>
<span id="cb8-48"><a href=""></a>        total_samples <span class="op">+=</span> <span class="bu">len</span>(X)</span>
<span id="cb8-49"><a href=""></a>        total_batches <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb8-50"><a href=""></a></span>
<span id="cb8-51"><a href=""></a>        ep_tqdm.set_postfix(average_batch_loss<span class="op">=</span>(total_loss<span class="op">/</span>total_batches).item(), accuracy<span class="op">=</span>(total_correct<span class="op">/</span>total_samples).item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="cnns-for-nlp-4" class="slide level2">
<h2>CNNs For NLP</h2>
<p>Let’s train the model.</p>
<div id="36b032bc" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="74a510e5-40f9-468e-fd85-f0b87748e510" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href=""></a><span class="co"># Hyperparameters</span></span>
<span id="cb9-2"><a href=""></a>learning_rate <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb9-3"><a href=""></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb9-4"><a href=""></a></span>
<span id="cb9-5"><a href=""></a>device <span class="op">=</span> torch.accelerator.current_accelerator().<span class="bu">type</span> <span class="cf">if</span> torch.accelerator.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb9-6"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss"> device"</span>)</span>
<span id="cb9-7"><a href=""></a></span>
<span id="cb9-8"><a href=""></a>model <span class="op">=</span> TextClassifier(train_data.no_tokens, seq_length).to(device)</span>
<span id="cb9-9"><a href=""></a></span>
<span id="cb9-10"><a href=""></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss().to(device)</span>
<span id="cb9-11"><a href=""></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb9-12"><a href=""></a></span>
<span id="cb9-13"><a href=""></a><span class="co"># Organize the training loop</span></span>
<span id="cb9-14"><a href=""></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb9-15"><a href=""></a>  <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>t<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">-------------------------------"</span>)</span>
<span id="cb9-16"><a href=""></a>  train_epoch(train_dataloader, model, loss_fn, optimizer)</span>
<span id="cb9-17"><a href=""></a>  eval_epoch(test_dataloader, model, loss_fn)</span>
<span id="cb9-18"><a href=""></a></span>
<span id="cb9-19"><a href=""></a><span class="bu">print</span>(<span class="st">"Done!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

</section>
<section id="cnns-for-nlp-4-output" class="slide level2 output-location-slide"><h2>CNNs For NLP</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="74a510e5-40f9-468e-fd85-f0b87748e510" data-execution_count="8">
<div class="cell-output cell-output-stdout">
<pre><code>Using cuda device
Epoch 1
-------------------------------
Train: 100%|██████████| 299/299 [00:01&lt;00:00, 151.84batch/s, average_batch_loss=0.789]
Val: 100%|██████████| 75/75 [00:00&lt;00:00, 207.76batch/s, accuracy=0.715, average_batch_loss=0.694]
Epoch 2
-------------------------------
Train: 100%|██████████| 299/299 [00:01&lt;00:00, 151.71batch/s, average_batch_loss=0.617]
Val: 100%|██████████| 75/75 [00:00&lt;00:00, 206.01batch/s, accuracy=0.744, average_batch_loss=0.642]
Epoch 3
-------------------------------
Train: 100%|██████████| 299/299 [00:01&lt;00:00, 151.85batch/s, average_batch_loss=0.518]
Val: 100%|██████████| 75/75 [00:00&lt;00:00, 205.37batch/s, accuracy=0.755, average_batch_loss=0.609]
Epoch 4
-------------------------------
Train: 100%|██████████| 299/299 [00:01&lt;00:00, 152.20batch/s, average_batch_loss=0.424]
Val: 100%|██████████| 75/75 [00:00&lt;00:00, 187.48batch/s, accuracy=0.783, average_batch_loss=0.569]
Epoch 5
-------------------------------
Train: 100%|██████████| 299/299 [00:02&lt;00:00, 145.95batch/s, average_batch_loss=0.312]
Val: 100%|██████████| 75/75 [00:00&lt;00:00, 192.65batch/s, accuracy=0.788, average_batch_loss=0.562]
Epoch 6
-------------------------------
Train: 100%|██████████| 299/299 [00:02&lt;00:00, 144.46batch/s, average_batch_loss=0.224]
Val: 100%|██████████| 75/75 [00:00&lt;00:00, 184.45batch/s, accuracy=0.788, average_batch_loss=0.547]
Epoch 7
-------------------------------
Train: 100%|██████████| 299/299 [00:02&lt;00:00, 144.14batch/s, average_batch_loss=0.161]
Val: 100%|██████████| 75/75 [00:00&lt;00:00, 203.91batch/s, accuracy=0.794, average_batch_loss=0.532]
Epoch 8
-------------------------------
Train: 100%|██████████| 299/299 [00:01&lt;00:00, 150.69batch/s, average_batch_loss=0.118]
Val: 100%|██████████| 75/75 [00:00&lt;00:00, 196.36batch/s, accuracy=0.796, average_batch_loss=0.536]
Epoch 9
-------------------------------
Train: 100%|██████████| 299/299 [00:02&lt;00:00, 135.23batch/s, average_batch_loss=0.087] 
Val: 100%|██████████| 75/75 [00:00&lt;00:00, 184.24batch/s, accuracy=0.799, average_batch_loss=0.563]
Epoch 10
-------------------------------
Train: 100%|██████████| 299/299 [00:02&lt;00:00, 143.02batch/s, average_batch_loss=0.0645]
Val: 100%|██████████| 75/75 [00:00&lt;00:00, 184.60batch/s, accuracy=0.801, average_batch_loss=0.578]
Done!</code></pre>
</div>
</div></section><section id="practice-task" class="slide level2">
<h2>Practice Task</h2>
<p>Try performing sentiment analysis on this <a href="https://github.com/jputrius/ml_intro/tree/main/data/reviewpolarity">dataset</a>. The input is movie reviews and your goal is to classify whether the review is positive or negative.</p>

</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="06_natural_language_processing_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="06_natural_language_processing_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="06_natural_language_processing_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="06_natural_language_processing_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="06_natural_language_processing_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="06_natural_language_processing_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="06_natural_language_processing_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="06_natural_language_processing_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="06_natural_language_processing_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="06_natural_language_processing_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>