{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08936596",
   "metadata": {},
   "source": [
    "# Chapter 8: Recurrent Neural Networks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this chapter we introduce recurrent neural networks, specifically LSTMs.\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "Up to now we have been covering feedforward neural networks. These are networks, whose graph representations have no loops in them.\n",
    "\n",
    "One major drawback of feedforward NNs is that their inputs are of fixed dimension. They cannot, for example, handle sequences of arbitrary length.\n",
    "\n",
    "To handle sequences of arbitrary length, one approach is to have a model which processes the sequence element by element. The input to the model is then the current element of the input sequence and the previous model output.\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "The simplest possible NN that has this structure can be defined as follows. Suppose $x_1, x_2, \\dots, x_n$ is the input sequence of vectors and we wish to produce an output sequence $o_1, o_2, \\dots, o_n.$ We can then compute the output sequence recursively as follows:\n",
    "\n",
    "$$\n",
    "  o_k = h(W_1x_k + W_0o_{k-1}),\n",
    "$$\n",
    "where $W_1, W_0$ are weight matrices and $h$ is an activation function.\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "The following diagram illustrates this architecture:\n",
    "\n",
    "![](../images/rnn.svg){fig-align=\"center\"}\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "Neural networks with this structure are called **recurrent neural networks** (RNNs).\n",
    "\n",
    "The major advantage of RNNs over feedforward NNs is that they can handle inputs of arbitrary length. The major disadvantage of RNNs over feedforward NNs is that they are less efficient when training.\n",
    "\n",
    "For example, when we were generating text with a transformer we could get the predictions for the entire input sequence at once in parallel. However, if we were training a RNN we would have to compute predictions one at a time, sequentially.\n",
    "\n",
    "## Long Short-Term Memory\n",
    "\n",
    "In practice, the simple RNNs as in the previous example are not used. The problem that arises is that such simple models have very short memory. For example, by the time the model gets to the end of a sentence it has already forgotten the start of it.\n",
    "\n",
    "To remedy this **long short-term memory models** (LSTMs) were introduced. \n",
    "\n",
    "## Long Short-Term Memory\n",
    "\n",
    "As the name suggests LSTMs are supposed to have *long* short-term memory. They achieve this by having an extra vector, called model's hidden state, that they update during each step and pass it to the next.\n",
    "\n",
    "LSTM cell looks like this:\n",
    "\n",
    "![](../images/lstm.png){fig-align=\"center\"}\n",
    "\n",
    "## Extras\n",
    "\n",
    "Another popular RNN architecture is called [gated recurrent units (GRUs)](https://en.wikipedia.org/wiki/Gated_recurrent_unit).\n",
    "\n",
    "The formulas and derivation of backpropagation on RNNs is a bit more complicated when compared to the feedforward case. If you are interested in the math, check out this expository [paper](https://arxiv.org/abs/1610.02583).\n",
    "\n",
    "## Practice Task\n",
    "\n",
    "Build a LSTM model for generating lithuanian names character by character.\n",
    "\n",
    "You can find the names dataset [here](https://github.com/jputrius/ml_intro/tree/main/data/names). Even better, try writing a script that downloads the names from [VLKK](https://vardai.vlkk.lt/) and cleans it. [Beutiful soup](https://beautiful-soup-4.readthedocs.io/en/latest/) package should come in handy for this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
