{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428020f2",
   "metadata": {},
   "source": [
    "# Chapter 3: Prerequisites for Neural Networks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this chapter we will go over tensors and gradient descent - two pieces of math that we will need when discussing neural networks.\n",
    "\n",
    "## Linear Algebra I\n",
    "\n",
    "In this course we will not need abstract vector spaces so we will formulate all the definition for the Euclidean space $\\mathbb{R}^n$ only.\n",
    "\n",
    "The Euclidean space $\\mathbb{R}^n$ is the space of all $n$ dimensional vectors. For us a vector will always be a column vector, for example\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{pmatrix} \\in \\mathbb{R}^3.\n",
    "$$\n",
    "\n",
    "## Linear Algebra II\n",
    "\n",
    "We will denote the $i$-th component of the vector $x$ by $x^i$, that is we will denote vector components by upper indices. For example, if $x \\in \\mathbb{R}^3,$ then\n",
    "$$\n",
    "x = \\begin{pmatrix}\n",
    "x^1 \\\\\n",
    "x^2 \\\\\n",
    "x^3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "This notation might seem strange at first sight but we will soon see that it is very convenient.\n",
    "\n",
    "## Linear Algebra III\n",
    "\n",
    "We cannot raise vectors to a power, so the notation $x^i$ should not cause confusion. If we want to raise the $i$-th component of $x$ to the $k$-th power we will write $(x^i)^k.$\n",
    "\n",
    "## Linear Algebra IV\n",
    "\n",
    "Vector addition and multiplication by a scalar is defined componentwise. For example,\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{pmatrix} + \\begin{pmatrix}\n",
    "4 \\\\\n",
    "5 \\\\\n",
    "6\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "1+4 \\\\\n",
    "2+5 \\\\\n",
    "3+6\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "5 \\\\\n",
    "7 \\\\\n",
    "9\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "and\n",
    "$$\n",
    "4\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "4*1 \\\\\n",
    "4*2 \\\\\n",
    "4*3\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "4 \\\\\n",
    "8 \\\\\n",
    "12\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "## Linear Algebra V\n",
    "\n",
    "On $\\mathbb{R}^n$ we also have the standard Euclidean inner product\n",
    "$$\n",
    "  \\langle x, y \\rangle = \\sum_{i=1}^n x^iy^i.\n",
    "$$\n",
    "\n",
    "We will denote by $e_i$ the standard basis of $\\mathbb{R}^n,$ so\n",
    "$$\n",
    "  e_1 = \\begin{pmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\dots \\\\\n",
    "0\n",
    "\\end{pmatrix}, \\\n",
    "e_2 = \\begin{pmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\dots \\\\\n",
    "0\n",
    "\\end{pmatrix}, \\ \\dots, \\\n",
    "e_n = \\begin{pmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\dots \\\\\n",
    "1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "## Linear Algebra VI\n",
    "\n",
    "We can uniquely write any vector $x \\in \\mathbb{R^n}$ as a linear combination of $e_i'$s:\n",
    "$$\n",
    "x = \\sum_{i=1}^n x^i e_i.\n",
    "$$\n",
    "This is what is meant when it is said that $e_i'$s form a basis.\n",
    "\n",
    "## Linear Algebra VII\n",
    "\n",
    "A map $T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is called **linear** if for all $x, y \\in \\mathbb{R}^n$ and $a, b \\in \\mathbb{R}$ we have\n",
    "$$\n",
    "T(ax+by)=aT(x)+bT(y).\n",
    "$$\n",
    "\n",
    "A linear map $T: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is usually called **a functional** or **a covector** depending on context.\n",
    "\n",
    "## Linear Algebra VIII\n",
    "\n",
    "When basis is fixed, linear maps $T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ are in one to one correspondence with $m$ by $n$ real matrices $A=(A^j_i),$ where the $A^j_i$ are $A'$s coefficients. In this course, we will always (implicitly) fix the basis of $\\mathbb{R}^n$ to be the standard one, that is the one given by the $e_i'$s.\n",
    "\n",
    "Then $T(x)=Ax$. I.e. if $y=T(x),$ then $y^j = \\sum_{i=1}^n A^j_ix^i.$\n",
    "\n",
    "## Linear Algebra IX\n",
    "\n",
    "If we know $T$ we can compute $A$. We have that $A_i^j = \\langle T(e_i), e_j\\rangle.$ Indeed, if $y=T(x),$ then\n",
    "$$\n",
    "\\sum_{i=1}^n A_i^j x^i = y^j = \\langle T(x), e_j\\rangle = \\langle Ax, e_j\\rangle = \\sum_{i=1}^n x^i \\langle Ae_i, e_j\\rangle.\n",
    "$$\n",
    "This holds for any $x$ and thus, by comparing the coefficients of $x^i$, we get\n",
    "$$\n",
    "A_i^j = \\langle Ae_i, e_j\\rangle = \\langle T(e_i), e_j\\rangle.\n",
    "$$\n",
    "\n",
    "## Linear Algebra X\n",
    "\n",
    "**Einstein notation** is a very convenient convention that massively simplifies linear algebra formulas. It says that when an index variable appears twice in a formula, once as an upper index and once as a lower index, we sum over that index. For example, we can skip the sum notation in the formula\n",
    "$$\n",
    "y^j = \\sum_{i=1}^n A^j_ix^i\n",
    "$$\n",
    "and write it as\n",
    "$$\n",
    "y^j = A^j_ix^i.\n",
    "$$\n",
    "\n",
    "## Linear Algebra XI\n",
    "\n",
    "Hopefully, now it is clear why we chose our convention for indices.\n",
    "\n",
    "Einstein notation works for all standard linear algebra formulas except inner product, as then we have two upper indices:\n",
    "$$\n",
    "\\langle x, y\\rangle = \\sum_{i=1}^n x^iy^i.\n",
    "$$\n",
    "\n",
    "## Linear Algebra XII\n",
    "\n",
    "We could make it work by defining Dirac delta to be\n",
    "$$\n",
    "\\delta_{ij} = \\begin{cases}\n",
    "1, \\text{ if } i=j\\\\\n",
    "0, \\text{ if } i\\ne j\n",
    "\\end{cases}\n",
    "$$\n",
    "and then\n",
    "$$\n",
    "\\langle x, y\\rangle = \\delta_{ij}x^iy^j.\n",
    "$$\n",
    "\n",
    "## Multilinear Maps I\n",
    "\n",
    "A map $T: \\mathbb{R}^{n_1} \\times \\mathbb{R}^{n_2} \\times \\dots \\times \\mathbb{R}^{n_m} \\rightarrow \\mathbb{R}^l$ is called **multilinear** if it is linear in each argument separately.\n",
    "\n",
    "For example, a map $T: \\mathbb{R}^n \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^l$ is multilinear if for all $x_1, x_2 \\in \\mathbb{R}^n,$ $y_1, y_2 \\in \\mathbb{R}^m$ and $a, b \\in \\mathbb{R}$ we have\n",
    "$$\n",
    "  T(ax_1 + bx_2, y_1) = aT(x_1, y_1) + bT(x_2, y_1),\n",
    "$$\n",
    "and\n",
    "$$\n",
    " T(x_1, ay_1 + by_2) = aT(x_1, y_1) + bT(x_1, y_2).\n",
    "$$\n",
    "\n",
    "## Multilinear Maps II\n",
    "\n",
    "A multilinear map $T: \\mathbb{R}^{n_1} \\times \\mathbb{R}^{n_2} \\times \\dots \\times \\mathbb{R}^{n_m} \\rightarrow \\mathbb{R}$ is called a **tensor**.\n",
    "\n",
    "If you study differential geometry or physics you will see that a distinction between covariant, contravariant and mixed tensors is made. In those subjects this distinction is useful for bookkeeping as the different types of tensors have different change of variable formulas.\n",
    "\n",
    "In ML applications there usually is a fixed basis that we never change, so we will not need this distinction and thus I will not define what these scary words mean.\n",
    "\n",
    "## Multilinear Maps III\n",
    "\n",
    "Similarly as a linear map, a multilinear map $T: \\mathbb{R}^{n_1} \\times \\mathbb{R}^{n_2} \\times \\dots \\times \\mathbb{R}^{n_m} \\rightarrow \\mathbb{R}^l$ can be fully described by a multidimensional array if the basis is fixed.\n",
    "\n",
    "For example, a multilinear map $T: \\mathbb{R}^n \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^l$ can be fully described by a multidimensional array $(T^k_{ij}),$ where $1 \\le i \\le n,$ $1 \\le j \\le m$ and $1 \\le k \\le l.$\n",
    "\n",
    "The numbers $T^k_{ij}$ are such that if $z = T(x, y),$ then\n",
    "$$\n",
    "  z^k = T^k_{ij}x^iy^j.\n",
    "$$\n",
    "\n",
    "## Multilinear Maps IV\n",
    "\n",
    "We can compute the numbers $T^k_{ij}$ from $T$ using the formula\n",
    "$$\n",
    "T^k_{ij} = \\langle T(e_i, e_j), e_k \\rangle.\n",
    "$$\n",
    "\n",
    "Indeed, if $z = T(x, y)$ then\n",
    "$$\n",
    "T^k_{ij}x^iy^j = z^k = \\langle T(x, y), e_k \\rangle = x^iy^j \\langle T(e_i, e_j), e_k \\rangle.\n",
    "$$\n",
    "Since $x$ and $y$ were arbitrary, we get the desired equality by comparing coefficients.\n",
    "\n",
    "## Multilinear Maps V\n",
    "\n",
    "In fact, any multilinear map can be thought of as a tensor. We can describe tensors as multidimensional arrays with some specific rule saying how the arrays transform under change of basis.\n",
    "\n",
    "In ML applications we usually do not need to change the basis, so in ML libraries any multidimensional array is called a tensor.\n",
    "\n",
    "## Multilinear Maps VI\n",
    "\n",
    "Note for mathematicians: Multilinear maps $\\mathbb{R}^{n_1} \\times \\mathbb{R}^{n_2} \\times \\dots \\times \\mathbb{R}^{n_m} \\rightarrow \\mathbb{R}^l$ are in one to one correspondence with tensors $T: \\mathbb{R}^{n_1} \\times \\mathbb{R}^{n_2} \\times \\dots \\times \\mathbb{R}^{n_m} \\times (\\mathbb{R}^l)^* \\rightarrow \\mathbb{R},$ where $(\\mathbb{R}^l)^*$ is the dual space to $\\mathbb{R}^l$.\n",
    "\n",
    "Indeed the linear spaces of such multilinear maps and tensors are both naturally isomorphic  to\n",
    "$$\n",
    "  (\\mathbb{R}^{n_1})^* \\otimes (\\mathbb{R}^{n_2})^* \\otimes \\dots \\otimes (\\mathbb{R}^{n_m})^* \\otimes \\mathbb{R}^l.\n",
    "$$\n",
    "But be aware that this does not necessarily hold if the linear spaces are infinite dimensional.\n",
    "\n",
    "## Gradient Descent I\n",
    "\n",
    "When training most ML models we are minimizing a loss function.\n",
    "\n",
    "For example, let's say our model is described by a differentiable function $f$ of two arguments $x$ and $w,$ where $x$ is the vector of features and $w$ is the vector of weights and output is a single number $y$.\n",
    "\n",
    "## Gradient Descent II\n",
    "\n",
    "Let's say we are trying to solve a regression problem. Then we could train our model $f$ by optimizing the following loss function\n",
    "\n",
    "$$\n",
    "  L(w) = \\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i, w))^2,\n",
    "$$\n",
    "where $(x_i, y_i)_{i=1}^n$ represents our training data. This loss function is called **mean squared error** (MSE).\n",
    "\n",
    "That is, we are trying to find the weights $w$ such that our model would match the training data as best as possible.\n",
    "\n",
    "## Gradient Descent III\n",
    "\n",
    "Note that the loss function is a function of weights only.\n",
    "\n",
    "Since we assumed that $f$ is differentiable we can apply **Gradient Descent** (GD) algorithm.\n",
    "\n",
    "The idea behind GD is very simple, since we assumed that $f$ is differentiable, then $L$ is also differentiable and its gradient $\\nabla L$ (read as \"del L\") is defined:\n",
    "$$\n",
    "\\nabla L (w) = \\left( \\frac{\\partial L}{\\partial w^1}(w), \\dots, \\frac{\\partial L}{\\partial w^m}(w) \\right),\n",
    "$$\n",
    "where $m$ is dimension of $w$.\n",
    "\n",
    "## Gradient Descent IV\n",
    "\n",
    "The gradient is a vector. The geometric interpretation of the gradient $\\nabla L$ is that it points in the direction in which $L$ increases the most quickly at point $w$. The magnitude of $\\nabla L$ is the rate of increase of $L$ at $w$ in that direction. Then $- \\nabla L$ is the direction of fastest decrease.\n",
    "\n",
    "The idea of GD is to minimize $L$ by making small steps in the direction $-\\nabla L.$\n",
    "\n",
    "## Gradient Descent V\n",
    "\n",
    "Define a parameter $\\eta$ called **learning rate**. It has to be a small positive number. Suppose the initial approximation of the minimum is $w_0$. Then in GD the $w_{i+1}$ approximation is computed from the $w_i$ approximation by using the simple formula\n",
    "$$\n",
    "w_{i+1} = w_i - \\eta \\nabla L(w_i).\n",
    "$$\n",
    "\n",
    "## Gradient Descent VI\n",
    "\n",
    "Here is an illustration of GD, note that in the picture the loss function is called cost.\n",
    "\n",
    "![](../images/GD.png){fig-align=\"center\"}\n",
    "\n",
    "## Gradient Descent VII\n",
    "\n",
    "Lets implement GD in the case of simple linear regression. That is we will try to predict a continuous variable $y$ from one input variable $x$ where our $f$ will be\n",
    "$$\n",
    "f(x; w^0, w^1) = w^0+w^1x.\n",
    "$$\n",
    "\n",
    "We will use MSE as a loss function, so $L$ will be\n",
    "$$\n",
    "L(w^0, w^1) = \\frac{1}{n} \\sum_{i=1}^n (y_i - (w^0+w^1x))^2. \n",
    "$$\n",
    "\n",
    "## Gradient Descent VIII\n",
    "\n",
    "We need to compute $\\nabla L.$ This can be done with a pen and some paper\n",
    "$$\n",
    "\\frac{\\partial L(w^0, w^1)}{\\partial w^0} = -\\frac{2}{n} \\sum_{i=1}^n (y_i - (w^0 + w^1 x_i))\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial L(w^0, w^1)}{\\partial w^1} = -\\frac{2}{n} \\sum_{i=1}^n x_i(y_i - (w^0 + w^1 x_i)).\n",
    "$$\n",
    "\n",
    "## Gradient Descent IX\n",
    "\n",
    "Now we can write the code. First define loss and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa6a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss(w, y, x):\n",
    "  return np.mean(np.square(y-w[0]-w[1]*x))\n",
    "\n",
    "def gradient(w, y, x):\n",
    "  return np.array([\n",
    "    -2*np.mean(y-w[0]-w[1]*x),\n",
    "    -2*np.mean(x*(y-w[0]-w[1]*x))\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205a6b0",
   "metadata": {},
   "source": [
    "## Gradient Descent X\n",
    "\n",
    "Now let's see if we can learn what the input linear function is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a2ceec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 12.501683501683504\n",
      "Loss after training: 4.935822348948857e-12\n",
      "Learnt weights: [2.         4.99999619]\n"
     ]
    }
   ],
   "source": [
    "# Let's see if we can learn that w^0 = 2 and w^1 = 5\n",
    "x = np.linspace(-1, 1, 100)\n",
    "y = 2+5*x\n",
    "\n",
    "learning_rate = 0.1\n",
    "w = np.array([0, 0]) # Initial weights\n",
    "\n",
    "print(f\"Initial loss: {loss(w, y, x)}\")\n",
    "for iter in range(200):\n",
    "  w = w - learning_rate*gradient(w, y, x)\n",
    "print(f\"Loss after training: {loss(w, y, x)}\")\n",
    "print(f\"Learnt weights: {w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f56e27",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent I\n",
    "\n",
    "In ML applications loss functions usually have the following form:\n",
    "$$\n",
    "  L(w) = \\sum_{j=1}^nL_j(w),\n",
    "$$\n",
    "where the sum is either over the samples of our training set or batches of samples.\n",
    "\n",
    "## Stochastic Gradient Descent II\n",
    "\n",
    "For example, when we were discussing logistic regression we saw that the loss function had the form\n",
    "$$\n",
    "  L(w) = \\frac{1}{n}\\sum_{j=1}^n -y_j\\log(f(x_j, w))-(1-y_j)\\log(1-f(x_j, w)),\n",
    "$$\n",
    "where the sum is over the training samples.\n",
    "\n",
    "By the way, this loss function is called cross-entropy.\n",
    "\n",
    "## Stochastic Gradient Descent III\n",
    "\n",
    "Suppose we have two finite discrete random variables $X$ and $Y$ taking the same values $x_1, \\dots, x_n$. Let's say that the pmf of $X$ is $p$ and pmf of $Y$ is $q$. Then cross-entropy is defined to be\n",
    "$$\n",
    "  H(X, Y) = -\\sum_{i=1}^n p(x_i) \\log(q(x_i)).\n",
    "$$\n",
    "\n",
    "When $X$ and $Y$ have the same distribution cross-entropy is equal to regular entropy, if they do not have the same distribution then cross-entropy is strictly larger.\n",
    "\n",
    "## Stochastic Gradient Descent IV\n",
    "\n",
    "Cross entropy measures how similar Y is to X. This is not the fully correct interpretation, but it is sufficient. The correct interpretation is a bit more subtle, you can read it [here](https://en.wikipedia.org/wiki/Cross-entropy).\n",
    "\n",
    "Cross entropy is used as a loss function for classification problems.\n",
    "\n",
    "Also note that it is not symmetric, that is\n",
    "$$\n",
    "H(X, Y) \\ne H(Y, X)\n",
    "$$\n",
    "in general.\n",
    "\n",
    "## Stochastic Gradient Descent V\n",
    "\n",
    "Getting back on topic, suppose our loss function has the following form\n",
    "$$\n",
    "  L(w) = \\sum_{j=1}^nL_j(w).\n",
    "$$\n",
    "\n",
    "If there are a lot of training samples or the model has a lot of weights, then computing the full gradient $\\nabla L$ becomes very computationally expensive.\n",
    "\n",
    "## Stochastic Gradient Descent VI\n",
    "\n",
    "It would instead be much nicer if we could compute the gradient only on a batch of our training data and use that to update the weights. Mathematically we would like our minimization step to look like \n",
    "$$\n",
    "  w_{i+1}=w_i -\\eta \\nabla L_j(w)\n",
    "$$\n",
    "where now we only compute the gradient of the $j$-th component of our loss function. When making subsequent minimization steps we then iterate over the $L_j$.\n",
    "\n",
    "## Stochastic Gradient Descent VII\n",
    "\n",
    "This algorithm indeed works and is called **Stochastic Gradient Descent** (SGD).\n",
    "\n",
    "SGD is probably the most popular algorithm for training neural networks.\n",
    "\n",
    "When applying SGD we loop over our training set, usually in small batches. One loop over the full training set is called an **epoch**.\n",
    "\n",
    "## Stochastic Gradient Descent VIII\n",
    "\n",
    "There is one simple improvement we can make, that is adding **momentum**.\n",
    "\n",
    "Define the momentum parameter $\\alpha,$ it has to be a smaller than 1 positive number.\n",
    "\n",
    "Recursively define $\\Delta w_{i+1} = \\alpha \\Delta w_i - \\eta \\nabla L_j(w_i)$ and then our minimization step is now\n",
    "$$\n",
    "  w_{i+1} = w_i + \\Delta w_{i+1}.\n",
    "$$\n",
    "\n",
    "## Stochastic Gradient Descent IX\n",
    "\n",
    "The idea is that if we stepped in the $\\Delta w_{i}$ direction in the previous step we should continue going in that direction in the current step since the minimum is probably still that way. Hence the name momentum.\n",
    "\n",
    "Since $\\alpha < 1$ the influence of the $i$-th step will eventually decay to nothing and we won't overshoot the minimum.\n",
    "\n",
    "## Stochastic Gradient Descent X\n",
    "\n",
    "One last thing, GD has a drawback that it is only able to find local minimums instead of global ones.\n",
    "\n",
    "When doing SGD, it is best practice to shuffle your training set after each epoch, because doing this minimizes the problem.\n",
    "\n",
    "## Practice task I\n",
    "\n",
    "Try to write your own implementation of logistic regression using SGD with momentum for training.\n",
    "\n",
    "Some tips and reminders:\n",
    "\n",
    "1. Logistic regression has the following form:\n",
    "$$\n",
    "  f(x; w) = \\frac{1}{1+e^{w^0+w^1x^1 + \\dots + w^nx^n}}.\n",
    "$$\n",
    "\n",
    "2. The logistic function $f$ satisfies the following nice identity:\n",
    "$$\n",
    "  f(-x; w) = 1-f(x; w).\n",
    "$$\n",
    "\n",
    "## Practice task II\n",
    "\n",
    "3. Use cross-entropy as a loss function:\n",
    "$$\n",
    "L(w) = \\frac{1}{n}\\sum_{j=1}^n -y_j\\log(f(x_j, w))-(1-y_j)\\log(1-f(x_j, w)),\n",
    "$$\n",
    "\n",
    "4. You can derive all the partial derivatives that you need quite easily by using the [chain rule of differentiation](https://en.wikipedia.org/wiki/Chain_rule).\n",
    "\n",
    "5. To learn how to shuffle numpy arrays, see answers [here](https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison).\n",
    "\n",
    "## Practice task III\n",
    "\n",
    "6. Initially, set your learning rate and momentum to be very small, something like $0.0001.$\n",
    "\n",
    "7. You can generate some mock data for testing your implementation using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2caedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10000, 20), y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=10, n_classes=2, n_redundant=10, random_state=34)\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed1c70",
   "metadata": {},
   "source": [
    "## Practice task IV\n",
    "\n",
    "Keep in mind that this is a toy example. If you implement both SGD and GD running times for GD will probably be lower. Performance benefits of SGD start to show up when you have a model with many more weights and more training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
