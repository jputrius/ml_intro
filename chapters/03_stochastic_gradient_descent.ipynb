{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428020f2",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this chapter we will go over stochastic gradient descent (SGD). SGD is an optimization algorithm used to train (artificial) neural networks.\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "Before we get into the main topic of today we would need to recall what (partial) derivatives are.\n",
    "\n",
    "If $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is a differentiable function then its derivative is a function denoted $f': \\mathbb{R} \\rightarrow \\mathbb{R}$ and computed as\n",
    "$$\n",
    "  f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}.\n",
    "$$\n",
    "\n",
    "In this course we will assume that all functions are differentiable everywhere where they are defined.\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "The derivative represents the rate of change of the function.\n",
    "If the derivative is positive at a point then the function is increasing at that point and if the derivative is negative at a point it means that a function is decreasing at that point.\n",
    "The magnitude of the derivative shows how fast the function is changing.\n",
    "\n",
    "Also recall the chain rule of differentiation:\n",
    "$$\n",
    "  (f(g(x)))' = f'(g(x))g'(x).\n",
    "$$\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "If we have a multi-variable function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ then we can compute the partial derivatives:\n",
    "$$\n",
    "  \\frac{\\partial f}{\\partial x_i}(x) = \\lim_{h \\rightarrow 0} \\frac{f(x + he_i) - f(x)}{h},\n",
    "$$\n",
    "here $e_i \\in \\mathbb{R}^n$ is a vector with $0$ in all components except the $i$-th one where it has the value 1.\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "The $i$-th partial derivative represents the rate of change of the function along the $i$-th axis.\n",
    "\n",
    "Combining all the partial derivatives into a vector we get the gradient:\n",
    "$$\\nabla f (x) = \\left( \\frac{\\partial f}{\\partial x_1}(x), \\dots, \\frac{\\partial f}{\\partial x_n}(x) \\right),$$\n",
    "the symbol $\\nabla$ is read \"del\" (this symbol is *not* a letter in the Greek alphabet!).\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "The way to think about the gradient is that to every point of $\\mathbb{R}^n$ it attaches an arrow which points in the direction in which the function increases the fastest.\n",
    "The magnitude of the arrow shows how fast the function increases.\n",
    "Also $-\\nabla f$ points in the direction in which the function decreases the fastest.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "We train most ML models by minimizing a loss function.\n",
    "\n",
    "For example, let's say our model is described by a differentiable function $f$ of two arguments $x$ and $w,$ where $x$ is the vector of features and $w$ is the vector of weights and output is a single number $y$.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Let's say we are trying to solve a regression problem. Then we could train our model $f$ by optimizing the following loss function\n",
    "\n",
    "$$\n",
    "  L(w) = \\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i, w))^2,\n",
    "$$\n",
    "where $(x_i, y_i)_{i=1}^n$ represents our training data. This loss function is called **mean squared error** (MSE).\n",
    "\n",
    "That is, we are trying to find the weights $w$ such that our model would match the training data as best as possible.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Note that the loss function is a function of weights only.\n",
    "\n",
    "Since we assumed that $f$ is differentiable we can apply the **Gradient Descent** (GD) algorithm.\n",
    "\n",
    "The idea behind GD is very simple. Since we assumed that $f$ is differentiable, then $L$ is also differentiable and its gradient $\\nabla L$ is defined:\n",
    "$$\n",
    "\\nabla L (w) = \\left( \\frac{\\partial L}{\\partial w_1}(w), \\dots, \\frac{\\partial L}{\\partial w_m}(w) \\right),\n",
    "$$\n",
    "where $m$ is dimension of $w$.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "The idea of GD is to minimize $L$ by making small steps in the direction $-\\nabla L,$ i.e. in the direction in which $L$ decreases the fastest.\n",
    "\n",
    "Define a parameter $\\eta$ called **learning rate**. It has to be a small positive number. Suppose the initial approximation of the minimum is $w^0$. Then in GD the $w^{i+1}$ approximation is computed from the $w^i$ approximation by using the simple formula\n",
    "$$\n",
    "w^{i+1} = w^i - \\eta \\nabla L(w^i).\n",
    "$$\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Here is an illustration of GD, note that in the picture the loss function is called cost.\n",
    "\n",
    "![](../images/GD.png){fig-align=\"center\"}\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Lets implement GD in the case of simple linear regression. That is we will try to predict a continuous variable $y$ from one input variable $x$ where our $f$ will be\n",
    "$$\n",
    "f(x; w_0, w_1) = w_0+w_1x.\n",
    "$$\n",
    "\n",
    "We will use MSE as a loss function, so $L$ will be\n",
    "$$\n",
    "L(w_0, w_1) = \\frac{1}{n} \\sum_{i=1}^n (y_i - (w_0+w_1x))^2. \n",
    "$$\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "We need to compute $\\nabla L.$ We get\n",
    "$$\n",
    "\\frac{\\partial L(w_0, w_1)}{\\partial w_0} = -\\frac{2}{n} \\sum_{i=1}^n (y_i - (w_0 + w_1 x_i))\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial L(w_0, w_1)}{\\partial w_1} = -\\frac{2}{n} \\sum_{i=1}^n x_i(y_i - (w_0 + w_1 x_i)).\n",
    "$$\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Now we can write the code. First define loss and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa6a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss(w, y, x):\n",
    "  return np.mean(np.square(y-w[0]-w[1]*x))\n",
    "\n",
    "def gradient(w, y, x):\n",
    "  return np.array([\n",
    "    -2*np.mean(y-w[0]-w[1]*x),\n",
    "    -2*np.mean(x*(y-w[0]-w[1]*x))\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205a6b0",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Now let's see if we can learn what the input linear function is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a2ceec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 12.501683501683504\n",
      "Loss after training: 4.935822348948857e-12\n",
      "Learnt weights: [2.         4.99999619]\n"
     ]
    }
   ],
   "source": [
    "# Let's see if we can learn that w^0 = 2 and w^1 = 5\n",
    "x = np.linspace(-1, 1, 100)\n",
    "y = 2+5*x\n",
    "\n",
    "learning_rate = 0.1\n",
    "w = np.array([0, 0]) # Initial weights\n",
    "\n",
    "print(f\"Initial loss: {loss(w, y, x)}\")\n",
    "for iter in range(200):\n",
    "  w = w - learning_rate*gradient(w, y, x)\n",
    "print(f\"Loss after training: {loss(w, y, x)}\")\n",
    "print(f\"Learnt weights: {w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f56e27",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "In ML applications loss functions usually have the following form:\n",
    "$$\n",
    "  L(w) = \\sum_{j=1}^nL_j(w),\n",
    "$$\n",
    "where the sum is either over the samples of our training set or batches of samples.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "For example, when we were discussing logistic regression we saw that the loss function had the form\n",
    "$$\n",
    "  L(w) = \\frac{1}{n}\\sum_{j=1}^n -y_j\\log(f(x_j, w))-(1-y_j)\\log(1-f(x_j, w)),\n",
    "$$\n",
    "where the sum is over the training samples.\n",
    "\n",
    "By the way, this loss function is called cross-entropy.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "Suppose we have two finite discrete random variables $X$ and $Y$ taking the same values $x_1, \\dots, x_n$. Let's say that the pmf of $X$ is $p$ and pmf of $Y$ is $q$. Then cross-entropy is defined to be\n",
    "$$\n",
    "  H(X, Y) = -\\sum_{i=1}^n p(x_i) \\log(q(x_i)).\n",
    "$$\n",
    "\n",
    "When $X$ and $Y$ have the same distribution cross-entropy is equal to regular entropy, if they do not have the same distribution then cross-entropy is strictly larger.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "Cross entropy measures how similar Y is to X. This is not the fully correct interpretation, but it is sufficient. The correct interpretation is a bit more subtle, you can read it [here](https://en.wikipedia.org/wiki/Cross-entropy).\n",
    "\n",
    "Cross entropy is used as a loss function for classification problems.\n",
    "\n",
    "Also note that it is not symmetric, that is\n",
    "$$\n",
    "H(X, Y) \\ne H(Y, X)\n",
    "$$\n",
    "in general.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "Getting back on topic, suppose our loss function has the following form\n",
    "$$\n",
    "  L(w) = \\frac{1}{n}\\sum_{j=1}^nL_j(w).\n",
    "$$\n",
    "\n",
    "If there are a lot of training samples or the model has a lot of weights, then computing the full gradient $\\nabla L$ becomes very computationally expensive.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "It would instead be much nicer if we could compute the gradient only on a batch of our training data and use that to update the weights. Mathematically we would like our minimization step to look like \n",
    "$$\n",
    "  w_{i+1}=w_i -\\eta \\nabla L_j(w)\n",
    "$$\n",
    "where now we only compute the gradient of the $j$-th component of our loss function. When making subsequent minimization steps we then iterate over the $L_j$.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "This algorithm indeed works and is called **Stochastic Gradient Descent** (SGD).\n",
    "\n",
    "When applying SGD we loop over our training set, usually in small batches. One loop over the full training set is called an **epoch**.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "There is one simple improvement we can make, that is adding **momentum**.\n",
    "\n",
    "Define the momentum parameter $\\alpha,$ it has to be a smaller than 1 positive number.\n",
    "\n",
    "Recursively define $\\Delta w^{i+1} = \\alpha \\Delta w^i - \\eta \\nabla L_j(w^i)$ and then our minimization step is now\n",
    "$$\n",
    "  w^{i+1} = w^i + \\Delta w^{i+1}.\n",
    "$$\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "The idea is that if we stepped in the $\\Delta w^{i}$ direction in the previous step we should continue going in that direction in the current step since the minimum is probably still that way. Hence the name momentum.\n",
    "\n",
    "Since $\\alpha < 1$ the influence of the $i$-th step will eventually decay to nothing and we won't overshoot the minimum.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "One last thing, GD has a drawback that it is only able to find local minimums instead of global ones.\n",
    "\n",
    "When doing SGD, it is best practice to shuffle your training set after each epoch, because doing this minimizes this problem. Shuffling the dataset also reduces overfitting.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "To understand why SGD is called \"stochastic\" we need to look at how it is analyzed mathematically.\n",
    "The formulation of the problem is as follows.\n",
    "\n",
    "Suppose we have a function $L: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ that is a random variable, for example $L$ depends on a parameter that is a random variable (in our case the random variable is the batch of data we show to the function).\n",
    "Also suppose we have a sequence $L_1, L_2, L_3, \\dots$ of realizations of $L$.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "The question is how do we come up with an algorithm to find a $w \\in \\mathbb{R}^n$ such that it would be a minimum of $\\mathbb{E}[L(w)]$ if we have a restriction where at the $T$-th timestep of optimization we only know the realizations $L_1, L_2, \\dots, L_T$?\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "The main shortcoming of SGD is that the learning rate is fixed. Ideally we would like the learning rate to vary a bit, because when we are far away from the minimum we could take bigger steps to train more efficiently and when we are near the minimum we would like to take smaller steps to not overshoot.\n",
    "\n",
    "So one easy way to improve SGD is to add a mechanism for adjusting the learning rate automatically during training. Probably the most popular algorithm that implements this is [Adam](https://arxiv.org/abs/1412.6980).\n",
    "\n",
    "## Stuff to watch\n",
    "\n",
    "- [Explanation of Gradient Descent](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "- [Explanation of Stochastic Gradient Descent](https://www.youtube.com/watch?v=vMh0zPT0tLI)\n",
    "\n",
    "## Practice task\n",
    "\n",
    "Try to write your own implementation of logistic regression using SGD with momentum for training.\n",
    "\n",
    "Some tips and reminders:\n",
    "\n",
    "1. Logistic regression has the following form:\n",
    "$$\n",
    "  f(x; w) = \\frac{1}{1+e^{-(w_0+w_1x_1 + \\dots + w_nx_n)}}.\n",
    "$$\n",
    "\n",
    "2. The logistic function $f$ satisfies the following nice identity:\n",
    "$$\n",
    "  f(-x; w) = 1-f(x; w).\n",
    "$$\n",
    "\n",
    "## Practice task\n",
    "\n",
    "3. Use cross-entropy as a loss function:\n",
    "$$\n",
    "L(w) = \\frac{1}{n}\\sum_{j=1}^n -y_j\\log(f(x_j, w))-(1-y_j)\\log(1-f(x_j, w)),\n",
    "$$\n",
    "\n",
    "4. You can derive all the partial derivatives that you need quite easily by using the [chain rule of differentiation](https://en.wikipedia.org/wiki/Chain_rule).\n",
    "\n",
    "5. To learn how to shuffle numpy arrays, see answers [here](https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison).\n",
    "\n",
    "## Practice task\n",
    "\n",
    "6. Initially, set your learning rate and momentum to be very small, something like $0.0001.$\n",
    "\n",
    "7. If you succeeded in implementing SGD try implementing Adam as well.\n",
    "\n",
    "8. You can generate some mock data for testing your implementation using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2caedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10000, 20), y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=10, n_classes=2, n_redundant=10, random_state=34)\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed1c70",
   "metadata": {},
   "source": [
    "## Practice task\n",
    "\n",
    "Keep in mind that this is a toy example. If you implement both SGD and GD running times for GD will probably be lower. Performance benefits of SGD start to show up when you have a model with many more weights and more training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
