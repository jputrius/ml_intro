{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428020f2",
   "metadata": {},
   "source": [
    "# Chapter 3: Stochastic Gradient Descent\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this chapter we will go over stochastic gradient descent (SGD). SGD is an optimization algorithm used to train (artificial) neural networks.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "We train most ML models by minimizing a loss function.\n",
    "\n",
    "For example, let's say our model is described by a differentiable function $f$ of two arguments $x$ and $w,$ where $x$ is the vector of features and $w$ is the vector of weights and output is a single number $y$.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Let's say we are trying to solve a regression problem. Then we could train our model $f$ by optimizing the following loss function\n",
    "\n",
    "$$\n",
    "  L(w) = \\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i, w))^2,\n",
    "$$\n",
    "where $(x_i, y_i)_{i=1}^n$ represents our training data. This loss function is called **mean squared error** (MSE).\n",
    "\n",
    "That is, we are trying to find the weights $w$ such that our model would match the training data as best as possible.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Note that the loss function is a function of weights only.\n",
    "\n",
    "Since we assumed that $f$ is differentiable we can apply the **Gradient Descent** (GD) algorithm.\n",
    "\n",
    "The idea behind GD is very simple. Since we assumed that $f$ is differentiable, then $L$ is also differentiable and its gradient $\\nabla L$ (read as \"del L\") is defined:\n",
    "$$\n",
    "\\nabla L (w) = \\left( \\frac{\\partial L}{\\partial w_1}(w), \\dots, \\frac{\\partial L}{\\partial w_m}(w) \\right),\n",
    "$$\n",
    "where $m$ is dimension of $w$.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "The gradient is a vector. The geometric interpretation of the gradient $\\nabla L$ is that it points in the direction in which $L$ increases the most quickly at point $w$. The magnitude of $\\nabla L$ is the rate of increase of $L$ at $w$ in that direction. Then $- \\nabla L$ is the direction of fastest decrease.\n",
    "\n",
    "The idea of GD is to minimize $L$ by making small steps in the direction $-\\nabla L.$\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Define a parameter $\\eta$ called **learning rate**. It has to be a small positive number. Suppose the initial approximation of the minimum is $w^0$. Then in GD the $w^{i+1}$ approximation is computed from the $w^i$ approximation by using the simple formula\n",
    "$$\n",
    "w^{i+1} = w^i - \\eta \\nabla L(w^i).\n",
    "$$\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Here is an illustration of GD, note that in the picture the loss function is called cost.\n",
    "\n",
    "![](../images/GD.png){fig-align=\"center\"}\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Lets implement GD in the case of simple linear regression. That is we will try to predict a continuous variable $y$ from one input variable $x$ where our $f$ will be\n",
    "$$\n",
    "f(x; w_0, w_1) = w_0+w_1x.\n",
    "$$\n",
    "\n",
    "We will use MSE as a loss function, so $L$ will be\n",
    "$$\n",
    "L(w_0, w_1) = \\frac{1}{n} \\sum_{i=1}^n (y_i - (w_0+w_1x))^2. \n",
    "$$\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "We need to compute $\\nabla L.$ We get\n",
    "$$\n",
    "\\frac{\\partial L(w_0, w_1)}{\\partial w_0} = -\\frac{2}{n} \\sum_{i=1}^n (y_i - (w_0 + w_1 x_i))\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial L(w_0, w_1)}{\\partial w_1} = -\\frac{2}{n} \\sum_{i=1}^n x_i(y_i - (w_0 + w_1 x_i)).\n",
    "$$\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Now we can write the code. First define loss and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa6a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss(w, y, x):\n",
    "  return np.mean(np.square(y-w[0]-w[1]*x))\n",
    "\n",
    "def gradient(w, y, x):\n",
    "  return np.array([\n",
    "    -2*np.mean(y-w[0]-w[1]*x),\n",
    "    -2*np.mean(x*(y-w[0]-w[1]*x))\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205a6b0",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Now let's see if we can learn what the input linear function is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a2ceec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 12.501683501683504\n",
      "Loss after training: 4.935822348948857e-12\n",
      "Learnt weights: [2.         4.99999619]\n"
     ]
    }
   ],
   "source": [
    "# Let's see if we can learn that w^0 = 2 and w^1 = 5\n",
    "x = np.linspace(-1, 1, 100)\n",
    "y = 2+5*x\n",
    "\n",
    "learning_rate = 0.1\n",
    "w = np.array([0, 0]) # Initial weights\n",
    "\n",
    "print(f\"Initial loss: {loss(w, y, x)}\")\n",
    "for iter in range(200):\n",
    "  w = w - learning_rate*gradient(w, y, x)\n",
    "print(f\"Loss after training: {loss(w, y, x)}\")\n",
    "print(f\"Learnt weights: {w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f56e27",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "In ML applications loss functions usually have the following form:\n",
    "$$\n",
    "  L(w) = \\sum_{j=1}^nL_j(w),\n",
    "$$\n",
    "where the sum is either over the samples of our training set or batches of samples.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "For example, when we were discussing logistic regression we saw that the loss function had the form\n",
    "$$\n",
    "  L(w) = \\frac{1}{n}\\sum_{j=1}^n -y_j\\log(f(x_j, w))-(1-y_j)\\log(1-f(x_j, w)),\n",
    "$$\n",
    "where the sum is over the training samples.\n",
    "\n",
    "By the way, this loss function is called cross-entropy.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "Suppose we have two finite discrete random variables $X$ and $Y$ taking the same values $x_1, \\dots, x_n$. Let's say that the pmf of $X$ is $p$ and pmf of $Y$ is $q$. Then cross-entropy is defined to be\n",
    "$$\n",
    "  H(X, Y) = -\\sum_{i=1}^n p(x_i) \\log(q(x_i)).\n",
    "$$\n",
    "\n",
    "When $X$ and $Y$ have the same distribution cross-entropy is equal to regular entropy, if they do not have the same distribution then cross-entropy is strictly larger.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "Cross entropy measures how similar Y is to X. This is not the fully correct interpretation, but it is sufficient. The correct interpretation is a bit more subtle, you can read it [here](https://en.wikipedia.org/wiki/Cross-entropy).\n",
    "\n",
    "Cross entropy is used as a loss function for classification problems.\n",
    "\n",
    "Also note that it is not symmetric, that is\n",
    "$$\n",
    "H(X, Y) \\ne H(Y, X)\n",
    "$$\n",
    "in general.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "Getting back on topic, suppose our loss function has the following form\n",
    "$$\n",
    "  L(w) = \\sum_{j=1}^nL_j(w).\n",
    "$$\n",
    "\n",
    "If there are a lot of training samples or the model has a lot of weights, then computing the full gradient $\\nabla L$ becomes very computationally expensive.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "It would instead be much nicer if we could compute the gradient only on a batch of our training data and use that to update the weights. Mathematically we would like our minimization step to look like \n",
    "$$\n",
    "  w_{i+1}=w_i -\\eta \\nabla L_j(w)\n",
    "$$\n",
    "where now we only compute the gradient of the $j$-th component of our loss function. When making subsequent minimization steps we then iterate over the $L_j$.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "This algorithm indeed works and is called **Stochastic Gradient Descent** (SGD).\n",
    "\n",
    "When applying SGD we loop over our training set, usually in small batches. One loop over the full training set is called an **epoch**.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "There is one simple improvement we can make, that is adding **momentum**.\n",
    "\n",
    "Define the momentum parameter $\\alpha,$ it has to be a smaller than 1 positive number.\n",
    "\n",
    "Recursively define $\\Delta w^{i+1} = \\alpha \\Delta w^i - \\eta \\nabla L_j(w^i)$ and then our minimization step is now\n",
    "$$\n",
    "  w^{i+1} = w^i + \\Delta w^{i+1}.\n",
    "$$\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "The idea is that if we stepped in the $\\Delta w^{i}$ direction in the previous step we should continue going in that direction in the current step since the minimum is probably still that way. Hence the name momentum.\n",
    "\n",
    "Since $\\alpha < 1$ the influence of the $i$-th step will eventually decay to nothing and we won't overshoot the minimum.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "One last thing, GD has a drawback that it is only able to find local minimums instead of global ones.\n",
    "\n",
    "When doing SGD, it is best practice to shuffle your training set after each epoch, because doing this minimizes this problem. Shuffling the dataset also reduces overfitting.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "The main shortcoming of SGD is that the learning rate is fixed. Ideally we would like the learning rate to vary a bit, because when we are far away from the minimum we could take bigger steps to train more efficiently and when we are near the minimum we would like to take smaller steps to not overshoot.\n",
    "\n",
    "So one easy way to improve SGD is to add a mechanism for adjusting the learning rate automatically during training. Probably the most popular algorithm that implements it is [Adam](https://arxiv.org/abs/1412.6980).\n",
    "\n",
    "## Practice task\n",
    "\n",
    "Try to write your own implementation of logistic regression using SGD with momentum for training.\n",
    "\n",
    "Some tips and reminders:\n",
    "\n",
    "1. Logistic regression has the following form:\n",
    "$$\n",
    "  f(x; w) = \\frac{1}{1+e^{-(w_0+w_1x_1 + \\dots + w_nx_n)}}.\n",
    "$$\n",
    "\n",
    "2. The logistic function $f$ satisfies the following nice identity:\n",
    "$$\n",
    "  f(-x; w) = 1-f(x; w).\n",
    "$$\n",
    "\n",
    "## Practice task\n",
    "\n",
    "3. Use cross-entropy as a loss function:\n",
    "$$\n",
    "L(w) = \\frac{1}{n}\\sum_{j=1}^n -y_j\\log(f(x_j, w))-(1-y_j)\\log(1-f(x_j, w)),\n",
    "$$\n",
    "\n",
    "4. You can derive all the partial derivatives that you need quite easily by using the [chain rule of differentiation](https://en.wikipedia.org/wiki/Chain_rule).\n",
    "\n",
    "5. To learn how to shuffle numpy arrays, see answers [here](https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison).\n",
    "\n",
    "## Practice task\n",
    "\n",
    "6. Initially, set your learning rate and momentum to be very small, something like $0.0001.$\n",
    "\n",
    "7. If you succeeded in implementing SGD try implementing Adam as well.\n",
    "\n",
    "8. You can generate some mock data for testing your implementation using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2caedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10000, 20), y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=10, n_classes=2, n_redundant=10, random_state=34)\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed1c70",
   "metadata": {},
   "source": [
    "## Practice task\n",
    "\n",
    "Keep in mind that this is a toy example. If you implement both SGD and GD running times for GD will probably be lower. Performance benefits of SGD start to show up when you have a model with many more weights and more training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
