{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2a61aa",
   "metadata": {},
   "source": [
    "# Miscellanea\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this chapter we will go through various topics that did not find their own place in previous chapters.\n",
    "\n",
    "## Multilabel classification\n",
    "\n",
    "There is the following rough classification of classification tasks:\n",
    "\n",
    "1. Binary - you assign to each sample one of two classes\n",
    "2. Multiclass - you assign to each sample one of $N > 2$ classes\n",
    "3. Multilabel - you assign to each sample a **subset** of $N$ classes, i.e. one sample can belong to multiple classes\n",
    "\n",
    "## Multilabel classification\n",
    "\n",
    "We have covered binary and multiclass classification but have not touched multilabel.\n",
    "So let's do it now.\n",
    "\n",
    "## Multilabel classification\n",
    "\n",
    "The way to do multilabel classification with NNs in `Pytorch` is not much different then the multiclass case.\n",
    "\n",
    "1. Instead of one encoding your target you encode it using a binary vector of lentgh $N$ (number of classes), where $1$ is in the $n$-th position if the sample is of class $n$ and $0$ otherwise.\n",
    "2. Your model still outputs a $N$ dimensional vector.\n",
    "\n",
    "## Multilabel classification\n",
    "\n",
    "3. You use `nn.BCEWithLogitsLoss` loss function (binary cross-entropy with logit loss). This is sigmoid applied component-wise + cross-entropy.\n",
    "4. When inferencing, to get class confidences, you apply sigmoid component-wise to the output.\n",
    "\n",
    "## Multilabel classification\n",
    "\n",
    "As an example, let's use this [dataset](https://huggingface.co/datasets/google-research-datasets/go_emotions).\n",
    "It contains reddit comments and the goal is to predict what emotions the comment exhibits.\n",
    "Of course, a tweet might exhibit multiple emotions, so this is a multilabel exercise.\n",
    "\n",
    "Let's load the data first.\n",
    "\n",
    "## Multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78852ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jp/projects/ml_intro/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>[27]</td>\n",
       "      <td>eebbqej</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>[27]</td>\n",
       "      <td>ed00q6i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>[2]</td>\n",
       "      <td>eezlygj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>[14]</td>\n",
       "      <td>ed7ypvh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>[3]</td>\n",
       "      <td>ed0bdzj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43405</th>\n",
       "      <td>Added you mate well I’ve just got the bow and ...</td>\n",
       "      <td>[18]</td>\n",
       "      <td>edsb738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43406</th>\n",
       "      <td>Always thought that was funny but is it a refe...</td>\n",
       "      <td>[6]</td>\n",
       "      <td>ee7fdou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43407</th>\n",
       "      <td>What are you talking about? Anything bad that ...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>efgbhks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43408</th>\n",
       "      <td>More like a baptism, with sexy results!</td>\n",
       "      <td>[13]</td>\n",
       "      <td>ed1naf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43409</th>\n",
       "      <td>Enjoy the ride!</td>\n",
       "      <td>[17]</td>\n",
       "      <td>eecwmbq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43410 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text labels       id\n",
       "0      My favourite food is anything I didn't have to...   [27]  eebbqej\n",
       "1      Now if he does off himself, everyone will thin...   [27]  ed00q6i\n",
       "2                         WHY THE FUCK IS BAYLESS ISOING    [2]  eezlygj\n",
       "3                            To make her feel threatened   [14]  ed7ypvh\n",
       "4                                 Dirty Southern Wankers    [3]  ed0bdzj\n",
       "...                                                  ...    ...      ...\n",
       "43405  Added you mate well I’ve just got the bow and ...   [18]  edsb738\n",
       "43406  Always thought that was funny but is it a refe...    [6]  ee7fdou\n",
       "43407  What are you talking about? Anything bad that ...    [3]  efgbhks\n",
       "43408            More like a baptism, with sexy results!   [13]  ed1naf8\n",
       "43409                                    Enjoy the ride!   [17]  eecwmbq\n",
       "\n",
       "[43410 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.lang.en import English\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "CLASS_LABELS = [\n",
    "  'admiration', 'amusement', 'anger', 'annoyance',\n",
    "  'approval', 'caring', 'confusion', 'curiosity',\n",
    "  'desire', 'disappointment', 'disapproval', 'disgust',\n",
    "  'embarrassment', 'excitement', 'fear', 'gratitude',\n",
    "  'grief', 'joy', 'love', 'nervousness',\n",
    "  'optimism', 'pride', 'realization', 'relief',\n",
    "  'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "\n",
    "TRAIN = pd.read_parquet(\"hf://datasets/google-research-datasets/go_emotions/simplified/train-00000-of-00001.parquet\")\n",
    "TEST = pd.read_parquet(\"hf://datasets/google-research-datasets/go_emotions/simplified/test-00000-of-00001.parquet\")\n",
    "\n",
    "TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638f1bc",
   "metadata": {},
   "source": [
    "## Transformers for classification\n",
    "\n",
    "Transformers can be used for classification.\n",
    "\n",
    "Let's train a transformer for this multilabel classification task.\n",
    "There are two modifications we need to make to adapt the transformer we used for text generation to a classification task.\n",
    "\n",
    "## Transformers for classification\n",
    "\n",
    "First, a transformer block produces a sequence of $e$ dimensional vectors, where $e$ is the embedding dimension.\n",
    "What is usually done in classification is that at the end the output of transformer blocks is pooled over the sequence length dimension.\n",
    "Doing this you get a tensor of shape $(b, e),$ where $b$ is the batch dimension.\n",
    "You then project this tensor to a tensor of shape $(b, c),$ where $c$ is the number of classes, using a matrix.\n",
    "\n",
    "## Transformers for classification\n",
    "\n",
    "Second, we need to change the mask that we use to mask attention weights. If our true sequence length is $l$ (without padding) and the padded sequence length is $n$, then we can mask the weights in attention layers responsible for generating the last $n-l$ outputs. This mask essentially allows you to work with arbitrary length sequences that are bounded by some fixed length.\n",
    "\n",
    "This mask improves performance significantly so you should always use it!\n",
    "\n",
    "## Transformers for classification\n",
    "\n",
    "First let's make a Dataset class for the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224c639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Dictionary:\n",
    "  def __init__(self, min_count=10, init_tokens=None):\n",
    "    self.nlp = English()\n",
    "    self.min_count = min_count\n",
    "    self.init_tokens = init_tokens\n",
    "    self.i2t, self.t2i, self.no_tokens = self._default_maps()\n",
    "    self.pad_idx = 0\n",
    "    self.unk_idx = 1\n",
    "\n",
    "  def _default_maps(self):\n",
    "    # <pad> - token used for padding\n",
    "    # <unk> - unknown, used for tokens not encountered in dictionary building\n",
    "    i2t = ['<pad>', '<unk>']\n",
    "    if self.init_tokens != None:\n",
    "      i2t = [*i2t, *self.init_tokens]\n",
    "    t2i = {token:index for index, token in enumerate(i2t)}\n",
    "    return i2t, t2i, len(i2t)\n",
    "\n",
    "  def build(self, corpus):\n",
    "    tokens = {}\n",
    "    for idx, row in enumerate(corpus):\n",
    "      for token in self.nlp(row):\n",
    "        if token.text.lower() not in tokens:\n",
    "          tokens[token.text.lower()] = 1\n",
    "        else:\n",
    "          tokens[token.text.lower()] += 1\n",
    "    i2t, _, _ = self._default_maps()\n",
    "    self.i2t = [\n",
    "      *i2t,\n",
    "      *[token for token, count in tokens.items() if count >= self.min_count]\n",
    "    ]\n",
    "    self.t2i = {token:index for index, token in enumerate(self.i2t)}\n",
    "    self.no_tokens = len(self.i2t)\n",
    "\n",
    "  def string_to_idx(self, string, seq_length=None):\n",
    "    tokens = [token.text.lower() for token in self.nlp(string) if not token.is_punct]\n",
    "    return self.tokens_to_idx(tokens, seq_length)\n",
    "\n",
    "  def tokens_to_idx(self, tokens, seq_length=None):\n",
    "    idxs = [self.t2i[token] if token in self.t2i else self.unk_idx for token in tokens]\n",
    "    if seq_length is not None:\n",
    "      idxs = idxs + [self.pad_idx] * (seq_length - len(idxs))\n",
    "      idxs = idxs[:seq_length]\n",
    "    return idxs\n",
    "\n",
    "  def idx_to_string(self, indices, ignore_pad=True):\n",
    "    tokens = self.idx_to_tokens(indices, ignore_pad)\n",
    "    return tokens.join(' ')\n",
    "\n",
    "  def idx_to_tokens(self, indices, ignore_pad=True):\n",
    "    if ignore_pad:\n",
    "      return [self.i2t[idx] for idx in indices if idx != self.pad_idx]\n",
    "    return [self.i2t[idx] for idx in indices]\n",
    "\n",
    "class Comments(Dataset):\n",
    "  def __init__(self, seq_length, train=False, dictionary=None):\n",
    "    self.seq_length = seq_length\n",
    "\n",
    "    if train:\n",
    "      self.dataset = TRAIN\n",
    "    else:\n",
    "      self.dataset = TEST\n",
    "\n",
    "    if dictionary is None:\n",
    "      self.dictionary = Dictionary()\n",
    "      self.dictionary.build(self.dataset[\"text\"])\n",
    "    else:\n",
    "      self.dictionary = dictionary\n",
    "\n",
    "    self.no_tokens = self.dictionary.no_tokens\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    tokens = self.dictionary.string_to_idx(self.dataset.iloc[idx][\"text\"], seq_length=self.seq_length)\n",
    "    tokens = torch.LongTensor(tokens)\n",
    "    mask = ~(tokens == self.dictionary.pad_idx)\n",
    "\n",
    "    target = self.dataset.iloc[idx][\"labels\"]\n",
    "    target = torch.zeros(len(CLASS_LABELS), dtype=torch.float).scatter_(0, torch.tensor(target), value=1)\n",
    "    return tokens, mask, target\n",
    "\n",
    "seq_length = 50\n",
    "batch_size = 128\n",
    "\n",
    "train_data = Comments(\n",
    "  seq_length=seq_length,\n",
    "  train=True\n",
    ")\n",
    "\n",
    "test_data = Comments(\n",
    "  seq_length=seq_length,\n",
    "  train=False,\n",
    "  dictionary=train_data.dictionary\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "  train_data,\n",
    "  batch_size=batch_size,\n",
    "  shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "  test_data,\n",
    "  batch_size=batch_size,\n",
    "  shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b866b0",
   "metadata": {},
   "source": [
    "## Transformers for classification\n",
    "\n",
    "Let's build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "  def __init__(self, no_classes, seq_length, no_tokens, embed_dim, no_heads, depth):\n",
    "    super().__init__()\n",
    "    self.embed_dim = embed_dim\n",
    "    self.no_heads = no_heads\n",
    "    self.depth = depth\n",
    "\n",
    "    self.token_embedding = nn.Embedding(embedding_dim=embed_dim, num_embeddings=no_tokens)\n",
    "    self.pos_embedding = nn.Embedding(embedding_dim=embed_dim, num_embeddings=seq_length)\n",
    "\n",
    "    self.tblocks = nn.ModuleList([\n",
    "      nn.TransformerEncoderLayer(d_model=embed_dim, nhead=no_heads, dim_feedforward=3072, batch_first=True) # Implements GPT style transformer block\n",
    "    ])\n",
    "\n",
    "    self.toprobs = nn.Linear(embed_dim, no_classes)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    tokens = self.token_embedding(x)\n",
    "    \n",
    "    b, n, e = tokens.size()\n",
    "    positions = self.pos_embedding(torch.arange(n, device=tokens.device)).unsqueeze(0).expand(b, n, e)\n",
    "    x = tokens + positions\n",
    "\n",
    "    for tblock in self.tblocks:\n",
    "      x = tblock(x, src_key_padding_mask=mask)\n",
    "\n",
    "    x, _ = torch.max(x, dim=1) # Max pool across the seq_length dimension, conveniently this also\n",
    "                               # removes the seq_length dimension from the tensor so we do not need to flatten\n",
    "\n",
    "    return self.toprobs(x)\n",
    "\n",
    "  def predict(self, x, mask):\n",
    "    # We can abuse the fact that if x > 0 then sigmoid(x) > 0.5\n",
    "    x = self.forward(x, mask)\n",
    "    return torch.heaviside(x, torch.tensor(0, dtype=torch.float32)) # Returns 1 if x > 0 and 0 if x <= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16864de6",
   "metadata": {},
   "source": [
    "## Transformers for classification\n",
    "\n",
    "Let's copy over the model training code. Note that we edit it a bit to accomodate the mask and also to account for the multilabel task.\n",
    "\n",
    "## Transformers for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1e4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "def train_epoch(dataloader, model, loss_fn, optimizer):\n",
    "  model.train() # Set model to training mode\n",
    "\n",
    "  total_loss = 0\n",
    "  total_batches = 0\n",
    "\n",
    "  with tqdm(dataloader, unit=\"batch\", file=sys.stdout) as ep_tqdm:\n",
    "    ep_tqdm.set_description(\"Train\")\n",
    "    for X, mask, y in ep_tqdm:\n",
    "      X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      pred = model(X, mask)\n",
    "      loss = loss_fn(pred, y)\n",
    "        \n",
    "      # Backward pass\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Reset the computed gradients back to zero\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Output stats\n",
    "      total_loss += loss\n",
    "      total_batches += 1\n",
    "      ep_tqdm.set_postfix(average_batch_loss=(total_loss/total_batches).item())\n",
    "\n",
    "def eval_epoch(dataloader, model, loss_fn):\n",
    "  model.eval() # Set model to inference mode\n",
    "  \n",
    "  total_loss = 0\n",
    "  total_batches = 0\n",
    "\n",
    "  with torch.no_grad(): # Do not compute gradients\n",
    "    with tqdm(dataloader, unit=\"batch\", file=sys.stdout) as ep_tqdm:\n",
    "      ep_tqdm.set_description(\"Val\")\n",
    "      for X, mask, y in ep_tqdm:\n",
    "        X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "        pred = model(X, mask)\n",
    "\n",
    "        total_loss += loss_fn(pred, y)\n",
    "        total_batches += 1\n",
    "\n",
    "        ep_tqdm.set_postfix(average_batch_loss=(total_loss/total_batches).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de70b318",
   "metadata": {},
   "source": [
    "## Transformers for classification\n",
    "\n",
    "Now we can train the model! Remember to use the appropriate loss function for multilabel tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d39d57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.56batch/s, average_batch_loss=0.148]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 32.53batch/s, average_batch_loss=0.125]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.81batch/s, average_batch_loss=0.116]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 34.09batch/s, average_batch_loss=0.11] \n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.78batch/s, average_batch_loss=0.106]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.78batch/s, average_batch_loss=0.104]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.93batch/s, average_batch_loss=0.1]  \n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.76batch/s, average_batch_loss=0.102]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.75batch/s, average_batch_loss=0.0961]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 34.14batch/s, average_batch_loss=0.101]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.68batch/s, average_batch_loss=0.0927]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.92batch/s, average_batch_loss=0.0993]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.81batch/s, average_batch_loss=0.0894]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.81batch/s, average_batch_loss=0.0983]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.72batch/s, average_batch_loss=0.0863]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.93batch/s, average_batch_loss=0.0997]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.72batch/s, average_batch_loss=0.083] \n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.56batch/s, average_batch_loss=0.0977]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.82batch/s, average_batch_loss=0.0798]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.99batch/s, average_batch_loss=0.0993]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.81batch/s, average_batch_loss=0.0765]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.99batch/s, average_batch_loss=0.0983]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.77batch/s, average_batch_loss=0.073] \n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.99batch/s, average_batch_loss=0.1]   \n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.79batch/s, average_batch_loss=0.0696]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.87batch/s, average_batch_loss=0.1]   \n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.62batch/s, average_batch_loss=0.0662]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.84batch/s, average_batch_loss=0.104]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.42batch/s, average_batch_loss=0.0628]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.45batch/s, average_batch_loss=0.104]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.57batch/s, average_batch_loss=0.0592]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 34.36batch/s, average_batch_loss=0.107]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.33batch/s, average_batch_loss=0.0558]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 32.88batch/s, average_batch_loss=0.108]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.41batch/s, average_batch_loss=0.0525]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.70batch/s, average_batch_loss=0.11] \n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.55batch/s, average_batch_loss=0.0493]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.58batch/s, average_batch_loss=0.111]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.58batch/s, average_batch_loss=0.0458]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.31batch/s, average_batch_loss=0.112]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#| output-location: slide\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "epochs = 20\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = TransformerClassifier(len(CLASS_LABELS), seq_length, train_data.no_tokens, 1024, 16, 6).to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Organize the training loop\n",
    "for t in range(epochs):\n",
    "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "  train_epoch(train_dataloader, model, loss_fn, optimizer)\n",
    "  eval_epoch(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a39093",
   "metadata": {},
   "source": [
    "## Transformers for classification\n",
    "\n",
    "Let's compute precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0fcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "    admiration       0.68      0.51      0.58       504\n",
      "     amusement       0.77      0.75      0.76       264\n",
      "         anger       0.52      0.27      0.36       198\n",
      "     annoyance       0.43      0.15      0.22       320\n",
      "      approval       0.40      0.25      0.31       351\n",
      "        caring       0.35      0.17      0.23       135\n",
      "     confusion       0.35      0.24      0.29       153\n",
      "     curiosity       0.42      0.25      0.31       284\n",
      "        desire       0.54      0.25      0.34        83\n",
      "disappointment       0.36      0.13      0.19       151\n",
      "   disapproval       0.40      0.08      0.13       267\n",
      "       disgust       0.59      0.38      0.46       123\n",
      " embarrassment       0.44      0.19      0.26        37\n",
      "    excitement       0.45      0.27      0.34       103\n",
      "          fear       0.70      0.49      0.58        78\n",
      "     gratitude       0.94      0.91      0.93       352\n",
      "         grief       0.00      0.00      0.00         6\n",
      "           joy       0.64      0.43      0.51       161\n",
      "          love       0.77      0.72      0.74       238\n",
      "   nervousness       0.33      0.13      0.19        23\n",
      "      optimism       0.65      0.46      0.54       186\n",
      "         pride       0.80      0.25      0.38        16\n",
      "   realization       0.44      0.11      0.18       145\n",
      "        relief       0.00      0.00      0.00        11\n",
      "       remorse       0.61      0.73      0.67        56\n",
      "       sadness       0.60      0.42      0.49       156\n",
      "      surprise       0.52      0.33      0.40       141\n",
      "       neutral       0.56      0.62      0.59      1787\n",
      "\n",
      "     micro avg       0.59      0.46      0.52      6329\n",
      "     macro avg       0.51      0.34      0.39      6329\n",
      "  weighted avg       0.56      0.46      0.49      6329\n",
      "   samples avg       0.48      0.48      0.47      6329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| output-location: slide\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_classification_report(model, dataloader):\n",
    "  with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for X, mask, y in dataloader:\n",
    "      X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "      y_pred = [*y_pred, *model.predict(X, mask).cpu()]\n",
    "      y_true = [*y_true, *y.cpu()]\n",
    "    print(classification_report(y_true, y_pred, target_names=CLASS_LABELS, zero_division=0))\n",
    "\n",
    "compute_classification_report(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5ea953",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "If you check the training statistics you will see that the model overfit.\n",
    "\n",
    "It is a good idea to stop training when your model starts to overfit, i.e. the performance degrades on the validation set.\n",
    "This is called early stopping.\n",
    "\n",
    "To implement early stopping in `pytorch` we first need to make our model evaluation function output the statistics.\n",
    "\n",
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25668b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(dataloader, model, loss_fn):\n",
    "  model.eval() # Set model to inference mode\n",
    "  \n",
    "  total_loss = 0\n",
    "  total_batches = 0\n",
    "\n",
    "  with torch.no_grad(): # Do not compute gradients\n",
    "    with tqdm(dataloader, unit=\"batch\", file=sys.stdout) as ep_tqdm:\n",
    "      ep_tqdm.set_description(\"Val\")\n",
    "      for X, mask, y in ep_tqdm:\n",
    "        X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "        pred = model(X, mask)\n",
    "\n",
    "        total_loss += loss_fn(pred, y)\n",
    "        total_batches += 1\n",
    "\n",
    "        ep_tqdm.set_postfix(average_batch_loss=(total_loss/total_batches).item())\n",
    "  \n",
    "  return (total_loss/total_batches).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1083b42",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "To decide whether we should stop early we are going to need to keep some extra state.\n",
    "So the cleanest solution would be to have an extra object that monitors training and decides whether we should stop.\n",
    "\n",
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd0629a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "  def __init__(self, patience=1, threshold=0):\n",
    "    self.patience = patience\n",
    "    self.annoyance = 0\n",
    "\n",
    "    self.threshold = threshold\n",
    "    self.best_epoch = 0\n",
    "    self.min_loss = 99999999999\n",
    "\n",
    "  def should_early_stop(self, loss, epoch):\n",
    "    if loss < self.min_loss:\n",
    "      self.min_loss = loss\n",
    "      self.annoyance = 0\n",
    "      self.best_epoch = epoch\n",
    "    elif loss > (self.min_loss + self.threshold):\n",
    "      self.annoyance += 1\n",
    "      if self.annoyance >= self.patience:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5651d0",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "Next we update our training routine to monitor overfitting.\n",
    "We also save our model every epoch so that we could go back to the best iteration.\n",
    "\n",
    "Saving your model every training epoch is called model checkpointing and its a good thing to do in general.\n",
    "For example, if the machine you are training on decides to crash you will not loose all your progress if you saved some checkpoints.\n",
    "\n",
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4654d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:13<00:00, 25.21batch/s, average_batch_loss=0.154]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 41.25batch/s, average_batch_loss=0.134]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:13<00:00, 25.35batch/s, average_batch_loss=0.123]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 41.09batch/s, average_batch_loss=0.115]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:13<00:00, 25.54batch/s, average_batch_loss=0.11] \n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 42.58batch/s, average_batch_loss=0.107]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:13<00:00, 25.40batch/s, average_batch_loss=0.104]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 40.71batch/s, average_batch_loss=0.103]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:13<00:00, 25.56batch/s, average_batch_loss=0.0993]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 40.74batch/s, average_batch_loss=0.101] \n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:13<00:00, 25.87batch/s, average_batch_loss=0.0958]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 42.06batch/s, average_batch_loss=0.0992]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:13<00:00, 25.88batch/s, average_batch_loss=0.0926]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 42.31batch/s, average_batch_loss=0.0984]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:13<00:00, 25.37batch/s, average_batch_loss=0.0898]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 41.35batch/s, average_batch_loss=0.0977]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:13<00:00, 25.13batch/s, average_batch_loss=0.0869]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 40.05batch/s, average_batch_loss=0.0974]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:13<00:00, 25.15batch/s, average_batch_loss=0.0839]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 40.52batch/s, average_batch_loss=0.0978]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:13<00:00, 24.74batch/s, average_batch_loss=0.0813]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 40.25batch/s, average_batch_loss=0.0981]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:13<00:00, 24.97batch/s, average_batch_loss=0.0784]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 39.53batch/s, average_batch_loss=0.0986]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:13<00:00, 24.91batch/s, average_batch_loss=0.0757]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 37.64batch/s, average_batch_loss=0.0998]\n",
      "Stopping early\n",
      "Loading checkpoint for best epoch 9\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#| output-location: slide\n",
    "import os\n",
    "\n",
    "# Create a directory for checkpoints\n",
    "if not os.path.exists(\"./checkpoints\"):\n",
    "  os.makedirs(\"./checkpoints\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "epochs = 20\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = TransformerClassifier(len(CLASS_LABELS), seq_length, train_data.no_tokens, 768, 12, 6).to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "stopper = EarlyStopper(patience=2, threshold=0.001)\n",
    "\n",
    "# Organize the training loop\n",
    "for t in range(epochs):\n",
    "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "  train_epoch(train_dataloader, model, loss_fn, optimizer)\n",
    "  loss = eval_epoch(test_dataloader, model, loss_fn)\n",
    "  torch.save(model, f\"./checkpoints/model_epoch_{t+1}.pth\")\n",
    "\n",
    "  if stopper.should_early_stop(loss, t+1):\n",
    "    print(\"Stopping early\")\n",
    "    break\n",
    "\n",
    "print(f\"Loading checkpoint for best epoch {stopper.best_epoch}\")\n",
    "model = torch.load(f\"./checkpoints/model_epoch_{stopper.best_epoch}.pth\", weights_only=False)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db95e0f",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "Let's compute precision and recall again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a44a016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "    admiration       0.64      0.57      0.61       504\n",
      "     amusement       0.75      0.82      0.78       264\n",
      "         anger       0.62      0.19      0.29       198\n",
      "     annoyance       0.44      0.15      0.22       320\n",
      "      approval       0.57      0.19      0.29       351\n",
      "        caring       0.41      0.13      0.20       135\n",
      "     confusion       0.45      0.07      0.11       153\n",
      "     curiosity       0.48      0.18      0.26       284\n",
      "        desire       0.58      0.23      0.33        83\n",
      "disappointment       0.50      0.01      0.01       151\n",
      "   disapproval       0.37      0.06      0.10       267\n",
      "       disgust       0.65      0.27      0.38       123\n",
      " embarrassment       0.00      0.00      0.00        37\n",
      "    excitement       0.54      0.26      0.35       103\n",
      "          fear       0.76      0.37      0.50        78\n",
      "     gratitude       0.93      0.90      0.91       352\n",
      "         grief       0.00      0.00      0.00         6\n",
      "           joy       0.66      0.48      0.55       161\n",
      "          love       0.77      0.76      0.76       238\n",
      "   nervousness       0.00      0.00      0.00        23\n",
      "      optimism       0.71      0.46      0.56       186\n",
      "         pride       0.00      0.00      0.00        16\n",
      "   realization       0.75      0.04      0.08       145\n",
      "        relief       0.00      0.00      0.00        11\n",
      "       remorse       0.61      0.61      0.61        56\n",
      "       sadness       0.72      0.28      0.41       156\n",
      "      surprise       0.59      0.29      0.39       141\n",
      "       neutral       0.64      0.55      0.59      1787\n",
      "\n",
      "     micro avg       0.66      0.41      0.51      6329\n",
      "     macro avg       0.50      0.28      0.33      6329\n",
      "  weighted avg       0.62      0.41      0.47      6329\n",
      "   samples avg       0.45      0.43      0.44      6329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compute_classification_report(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce0a08",
   "metadata": {},
   "source": [
    "## Balancing class weights\n",
    "\n",
    "Now let's return to a multiclass classification problem.\n",
    "\n",
    "Very often the distribution of the labels in your training dataset will not match the distribution that you will encouter in the \"wild\" (i.e. production).\n",
    "This happens due to a variety of factors.\n",
    "For example, it might be that indentifying one specific label is much easier then the rest.\n",
    "So, over time your training set will gather unproportionally more examples of that label.\n",
    "\n",
    "## Balancing class weights\n",
    "\n",
    "The problem is that the model will learn this incorrect distribution. For example, if 90% of your training set is made up of one label then the model will be very trigger happy when assigning that label.\n",
    "\n",
    "In practice, if the distribution of your training set is skewed, it is better to rebalance the distribution such that it is uniform.\n",
    "That is, make the model learn that each label is as likely as the next one.\n",
    "\n",
    "## Balancing class weights\n",
    "\n",
    "This can be done by balancing the class weights.\n",
    "That is, you assign a bigger weight to a sample if its label is more rare in the training dataset.\n",
    "\n",
    "The formula you can use is\n",
    "$$\n",
    "  \\text{weight}_i = \\frac{\\text{total no of samples}}{\\text{no of classes}\\times\\text{no of samples of class i}}.\n",
    "$$\n",
    "\n",
    "## Balancing class weights\n",
    "\n",
    "Note that doing this will probably reduce the performance of the model on your validation and test datasets (if these have the same wrong distribution), however the point is that it will improve performance in production.\n",
    "\n",
    "## Balancing class weights\n",
    "\n",
    "In `sklearn`, models have a `class_weight` parameter, which you can set to `'balanced'` to apply the above formula.\n",
    "\n",
    "Here is one way to do this in `pytorch`:\n",
    "\n",
    "## Balancing class weights\n",
    "\n",
    "```\n",
    "def compute_class_weights(dataloader):\n",
    "  classes = []\n",
    "  for _, _, y in dataloader:\n",
    "    classes = [*classes, *y.argmax(dim=1)]\n",
    "  \n",
    "  no_classes = len(np.unique(classes))\n",
    "  no_total = len(classes)\n",
    "  no_in_class = np.unique_counts(classes).counts\n",
    "  return torch.tensor(no_total/(no_classes*no_in_class))\n",
    "\n",
    "weights = compute_class_weights(some_dataloader)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d49ab",
   "metadata": {},
   "source": [
    "## Learning rate scheduling and warmup\n",
    "\n",
    "Neural networks with many layers might have trouble converging when you start training on fresh random weights.\n",
    "I.e. if you had weights that were approximately correct the training would converge, however when you start with random weights training diverges.\n",
    "\n",
    "There is a standard technique for mitigating this called warmup. The idea is that you start training with a very low learning rate for the first few epochs and then crank the learning rate back up to a normal level.\n",
    "\n",
    "Our model is not really deep enough to benefit from warmup, but we can still checkout how to implement it in `pytorch`.\n",
    "\n",
    "## Learning rate scheduling and warmup\n",
    "\n",
    "Also, if you are using SGD to train it might be a good idea to periodically reduce the learning rate as you are training. This is not that necessary when using Adam or its variants.\n",
    "\n",
    "In general, if you are tweaking the learning rate during training this is called learning rate scheduling.\n",
    "\n",
    "## Learning rate scheduling and warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72ad92be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.71batch/s, average_batch_loss=0.178]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.97batch/s, average_batch_loss=0.147]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.48batch/s, average_batch_loss=0.148]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.55batch/s, average_batch_loss=0.145]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.54batch/s, average_batch_loss=0.146]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.15batch/s, average_batch_loss=0.143]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.25batch/s, average_batch_loss=0.142]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.09batch/s, average_batch_loss=0.138]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.37batch/s, average_batch_loss=0.137]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.04batch/s, average_batch_loss=0.132]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.50batch/s, average_batch_loss=0.121]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.29batch/s, average_batch_loss=0.112]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.45batch/s, average_batch_loss=0.108]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.50batch/s, average_batch_loss=0.109]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.57batch/s, average_batch_loss=0.101]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.54batch/s, average_batch_loss=0.101]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.51batch/s, average_batch_loss=0.0964]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.55batch/s, average_batch_loss=0.0996]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.52batch/s, average_batch_loss=0.0927]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.46batch/s, average_batch_loss=0.0984]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.55batch/s, average_batch_loss=0.0895]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.62batch/s, average_batch_loss=0.0971]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.57batch/s, average_batch_loss=0.0863]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.39batch/s, average_batch_loss=0.0974]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.70batch/s, average_batch_loss=0.083] \n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.80batch/s, average_batch_loss=0.0993]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.56batch/s, average_batch_loss=0.0803]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.11batch/s, average_batch_loss=0.0985]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.32batch/s, average_batch_loss=0.0771]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.24batch/s, average_batch_loss=0.0994]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.35batch/s, average_batch_loss=0.0738]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.06batch/s, average_batch_loss=0.101]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.25batch/s, average_batch_loss=0.0703]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 32.93batch/s, average_batch_loss=0.101]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.26batch/s, average_batch_loss=0.0672]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 33.08batch/s, average_batch_loss=0.102]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.23batch/s, average_batch_loss=0.0638]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 32.86batch/s, average_batch_loss=0.103]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Train: 100%|██████████| 340/340 [00:17<00:00, 19.32batch/s, average_batch_loss=0.0605]\n",
      "Val: 100%|██████████| 43/43 [00:01<00:00, 32.71batch/s, average_batch_loss=0.105]\n",
      "Loading checkpoint for best epoch 11\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#| output-location: slide\n",
    "import os\n",
    "\n",
    "# Create a directory for checkpoints\n",
    "if not os.path.exists(\"./checkpoints\"):\n",
    "  os.makedirs(\"./checkpoints\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "epochs = 20\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = TransformerClassifier(len(CLASS_LABELS), seq_length, train_data.no_tokens, 1024, 16, 6).to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=0.1, total_iters=5) # Multiplies the LR by factor for total_iters iterations, there are other schedulers available in Pytorch\n",
    "stopper = EarlyStopper(patience=2, threshold=0.1)\n",
    "\n",
    "# Organize the training loop\n",
    "for t in range(epochs):\n",
    "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "  train_epoch(train_dataloader, model, loss_fn, optimizer)\n",
    "  loss = eval_epoch(test_dataloader, model, loss_fn)\n",
    "  torch.save(model, f\"./checkpoints/model_epoch_{t+1}.pth\")\n",
    "  scheduler.step()\n",
    "\n",
    "  if stopper.should_early_stop(loss, t+1):\n",
    "    print(\"Stopping early\")\n",
    "    break\n",
    "\n",
    "print(f\"Loading checkpoint for best epoch {stopper.best_epoch}\")\n",
    "model = torch.load(f\"./checkpoints/model_epoch_{stopper.best_epoch}.pth\", weights_only=False)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a15a53",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "There is a standard way of reducing overfitting in neural networks called dropout.\n",
    "The idea of dropout is that you randomly kill some inputs to a layer during training.\n",
    "So the model cannot focus on using specific weights during training and therefore is forced to train all weights.\n",
    "\n",
    "![](../images/dropout.png){fig-align=\"center\"}\n",
    "\n",
    "## Dropout\n",
    "\n",
    "`nn.TransformerEncoderLayer` already adds dropout by default with probabily to kill an input with probability $0.1$. You can control this using the `dropout` parameter.\n",
    "\n",
    "You can add dropout to your `Pytorch` model using [nn.Dropout](https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout).\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "The more data you have the better.\n",
    "However, getting more data to train your model might be expensive.\n",
    "Instead you can try generating synthetic data out of the data that you already have.\n",
    "This is called data augmentation.\n",
    "\n",
    "Of course getting completely novel data is better, but adding synthetic data can also boost the performance of your model with almost no overhead cost.\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "If you are dealing with images you can try:\n",
    "\n",
    "1. Cropping\n",
    "2. Flipping\n",
    "3. Zooming\n",
    "4. Rotation\n",
    "5. Hue adjustment\n",
    "\n",
    "to get images that are slightly different then the original.\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "For example, if you are doing image classification then performing the above operations will not change the class the image is in (unless you go very wild), so you will have a new sample of the class.\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "In image classification there is also an interesting data augmentation technique called mixup.\n",
    "Suppose matrices $A$ and $B$ represent your images.\n",
    "Then take some $t \\in (0, 1)$ and create a new image $C = tA+(1-t)B$.\n",
    "The class of $C$ will also be a linear combination of the classes of $A$ and $B$, that is if the one hot encoded labels of $A$ and $B$ are $y_A$ and $y_B$, then $y_C = ty_A+(1-t)y_B$.\n",
    "\n",
    "You can also mixup three or more images.\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "You can also augment natural text, for example you can try:\n",
    "\n",
    "1. Replacing words with synonyms\n",
    "2. Removing random words from sentences\n",
    "3. Adding random words to sentences\n",
    "4. Machine translating to a different language and then translating back\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "There are two ways of implementing data augmentation in your training pipeline:\n",
    "\n",
    "1. Offline - you augment the data before training\n",
    "2. Online - you perform data augmentation during training on a batch before feeding it to the model\n",
    "\n",
    "Be aware that in NLP if your data augmentation technique has the possibility of adding new tokens to the dataset then you can't do it in an online way.\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "This is not strictly data augmentation, but in NLP nowadays you can generate new labels using LLMs.\n",
    "\n",
    "For example if you are doing text classification and have a bunch of data that is not labelled then you can get a LLM to label some of it.\n",
    "\n",
    "If you write a good prompt then the accuracy of those labels will probably be around the same as you would get if you paid some company to do mass labelling for you.\n",
    "At least this is the case from my own personal experience.\n",
    "\n",
    "## Extra tools\n",
    "\n",
    "Here are some extra tools that are worth looking at but we will not cover in this course:\n",
    "\n",
    "1. [LangChain](https://github.com/langchain-ai/langchain) - a framework for working with LLMs programatically.\n",
    "2. [Apache Beam](https://beam.apache.org/) - a thing that helps you write parallel data processing pipelines.\n",
    "3. Also getting used to Linux might be useful at some point.\n",
    "\n",
    "## Practice task\n",
    "\n",
    "1. Start working on your homework project if you have not already!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
