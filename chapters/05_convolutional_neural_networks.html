<!DOCTYPE html>
<html lang="en"><head>
<script src="05_convolutional_neural_networks_files/libs/clipboard/clipboard.min.js"></script>
<script src="05_convolutional_neural_networks_files/libs/quarto-html/tabby.min.js"></script>
<script src="05_convolutional_neural_networks_files/libs/quarto-html/popper.min.js"></script>
<script src="05_convolutional_neural_networks_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="05_convolutional_neural_networks_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="05_convolutional_neural_networks_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="05_convolutional_neural_networks_files/libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.24">

  <title>Chapter 5: Convolutional Neural Network</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="05_convolutional_neural_networks_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="05_convolutional_neural_networks_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="05_convolutional_neural_networks_files/libs/revealjs/dist/theme/quarto-9df06d9b3e1683bd31835b8738a1bbfc.css">
  <link href="05_convolutional_neural_networks_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="05_convolutional_neural_networks_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="05_convolutional_neural_networks_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="05_convolutional_neural_networks_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Chapter 5: Convolutional Neural Network</h1>

<div class="quarto-title-authors">
</div>

</section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
<p>Today we discuss convolutional neural networks (CNNs). But first we are going to give a quick introduction to PyTorch.</p>
<p>PyTorch is a python library for building machine learning models.</p>
</section>
<section id="introduction-1" class="slide level2">
<h2>Introduction</h2>
<p>PyTorch has two main features:</p>
<ol type="1">
<li>Tensors</li>
<li>Backpropagation</li>
</ol>
</section>
<section id="introduction-2" class="slide level2">
<h2>Introduction</h2>
<p>Tensors in PyTorch are basically the same thing as numpy ndarrays except that you can do operations on PyTorch tensors with a GPU.</p>
<p>By convention, the first dimension of a PyTorch tensor is usually the mini-batch dimension.</p>
<p>For example you should interpret a tensor of shape <code>(2, 10)</code> as an array that contains two vectors with 10 components each and a tensor of shape <code>(3, 5, 5)</code> as an array that contains three 5 by 5 matrices.</p>
</section>
<section id="loading-data" class="slide level2">
<h2>Loading data</h2>
<p>We first need to learn how to load data in PyTorch.</p>
<p>PyTorch has two abstractions: a <code>Dataset</code> class and a <code>DataLoader</code> class. The <code>Dataset</code> class represents your data, while <code>DataLoader</code> wraps <code>Dataset</code> and allows you to iterate over it.</p>
</section>
<section id="loading-data-1" class="slide level2">
<h2>Loading data</h2>
<p>In this chapter we will use the CIFAR10 dataset:</p>

<img data-src="../images/cifar10.png" class="quarto-figure quarto-figure-center r-stretch"><p>This dataset ships with Torchvision which is a sub library of PyTorch that adds utils for handling pictures.</p>
<p>Here is how to load the dataset:</p>
</section>
<section id="loading-data-2" class="slide level2">
<h2>Loading data</h2>
<div id="db636723" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:406}}" data-outputid="f68ce3ba-8500-48f3-94db-9a4b9af43dc1" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href=""></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href=""></a><span class="im">from</span> torchvision <span class="im">import</span> datasets</span>
<span id="cb1-3"><a href=""></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb1-4"><a href=""></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> ToTensor, Lambda</span>
<span id="cb1-5"><a href=""></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href=""></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href=""></a></span>
<span id="cb1-8"><a href=""></a><span class="kw">def</span> show(img):</span>
<span id="cb1-9"><a href=""></a>  <span class="co">"""Function for displaying image"""</span></span>
<span id="cb1-10"><a href=""></a>  npimg <span class="op">=</span> img.numpy()</span>
<span id="cb1-11"><a href=""></a>  plt.imshow(np.transpose(npimg, (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb1-12"><a href=""></a>  plt.axis(<span class="st">'off'</span>)</span>
<span id="cb1-13"><a href=""></a></span>
<span id="cb1-14"><a href=""></a>train_data <span class="op">=</span> datasets.CIFAR10(</span>
<span id="cb1-15"><a href=""></a>  root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb1-16"><a href=""></a>  train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-17"><a href=""></a>  download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-18"><a href=""></a>  transform<span class="op">=</span>ToTensor(),</span>
<span id="cb1-19"><a href=""></a>  <span class="co"># One hot encodes the target</span></span>
<span id="cb1-20"><a href=""></a>  target_transform<span class="op">=</span>Lambda(<span class="kw">lambda</span> y: torch.zeros(<span class="dv">10</span>, dtype<span class="op">=</span>torch.<span class="bu">float</span>).scatter_(<span class="dv">0</span>, torch.tensor(y), value<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb1-21"><a href=""></a>)</span>
<span id="cb1-22"><a href=""></a></span>
<span id="cb1-23"><a href=""></a>test_data <span class="op">=</span> datasets.CIFAR10(</span>
<span id="cb1-24"><a href=""></a>  root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb1-25"><a href=""></a>  train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-26"><a href=""></a>  download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-27"><a href=""></a>  transform<span class="op">=</span>ToTensor(),</span>
<span id="cb1-28"><a href=""></a>  <span class="co"># One hot encodes the target</span></span>
<span id="cb1-29"><a href=""></a>  target_transform<span class="op">=</span>Lambda(<span class="kw">lambda</span> y: torch.zeros(<span class="dv">10</span>, dtype<span class="op">=</span>torch.<span class="bu">float</span>).scatter_(<span class="dv">0</span>, torch.tensor(y), value<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb1-30"><a href=""></a>)</span>
<span id="cb1-31"><a href=""></a></span>
<span id="cb1-32"><a href=""></a>img, target <span class="op">=</span> train_data[<span class="dv">0</span>]</span>
<span id="cb1-33"><a href=""></a>show(img)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>

</div>
<img data-src="05_convolutional_neural_networks_files/figure-revealjs/cell-2-output-1.png" class="r-stretch"></section>
<section id="loading-data-3" class="slide level2">
<h2>Loading data</h2>
<div id="c206e1c3" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="02e0d19d-46fe-42e2-f6eb-e967f1213eb5" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href=""></a><span class="bu">print</span>(target)</span>
<span id="cb2-2"><a href=""></a>channels, height, width <span class="op">=</span> img.shape</span>
<span id="cb2-3"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Dimensions of image: C:</span><span class="sc">{</span>channels<span class="sc">}</span><span class="ss">, H:</span><span class="sc">{</span>height<span class="sc">}</span><span class="ss">, W:</span><span class="sc">{</span>width<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])
Dimensions of image: C:3, H:32, W:32</code></pre>
</div>
</div>
</section>
<section id="building-the-model" class="slide level2">
<h2>Building the model</h2>
<p>To build a model in PyTorch we need to define a class that inherits from the PyTorch’s <code>Module</code> class.</p>
<p>In the constructor we define the architecture of our model. We also need to implement the <code>forward</code> method where we define the forward pass of our model. We do not need to define the backward pass for backpropagation, PyTorch figures it out automatically.</p>
<p>Also, for classification models, do not apply softmax to the output layer as the PyTorch’s cross-entropy loss implementation already does that. We discussed why this is done in the last chapter.</p>
</section>
<section id="building-the-model-1" class="slide level2">
<h2>Building the model</h2>
<div id="4e3fa95d" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="bfbd9e6c-880c-47d8-ed10-80fd610d496c" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href=""></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb4-2"><a href=""></a></span>
<span id="cb4-3"><a href=""></a><span class="kw">class</span> NeuralNetwork(nn.Module):</span>
<span id="cb4-4"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb4-5"><a href=""></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-6"><a href=""></a>    <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten() <span class="co"># Flattens out the image into a vector</span></span>
<span id="cb4-7"><a href=""></a>    <span class="va">self</span>.linear_relu_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb4-8"><a href=""></a>      nn.Linear(<span class="dv">3</span><span class="op">*</span><span class="dv">32</span><span class="op">*</span><span class="dv">32</span>, <span class="dv">512</span>),</span>
<span id="cb4-9"><a href=""></a>      nn.ReLU(),</span>
<span id="cb4-10"><a href=""></a>      nn.Linear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb4-11"><a href=""></a>      nn.ReLU(),</span>
<span id="cb4-12"><a href=""></a>      nn.Linear(<span class="dv">512</span>, <span class="dv">10</span>),</span>
<span id="cb4-13"><a href=""></a>    )</span>
<span id="cb4-14"><a href=""></a></span>
<span id="cb4-15"><a href=""></a>  <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-16"><a href=""></a>    x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb4-17"><a href=""></a>    <span class="cf">return</span> <span class="va">self</span>.linear_relu_stack(x)</span>
<span id="cb4-18"><a href=""></a></span>
<span id="cb4-19"><a href=""></a><span class="bu">print</span>(NeuralNetwork())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>

</section>
<section id="building-the-model-1-output" class="slide level2 output-location-slide"><h2>Building the model</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="bfbd9e6c-880c-47d8-ed10-80fd610d496c" data-execution_count="3">
<div class="cell-output cell-output-stdout">
<pre><code>NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=3072, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div></section><section id="dataloaders" class="slide level2">
<h2>Dataloaders</h2>
<p>Let’s define our dataloaders.</p>
<div id="8b85b441" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href=""></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb6-2"><a href=""></a></span>
<span id="cb6-3"><a href=""></a>train_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb6-4"><a href=""></a>  train_data,</span>
<span id="cb6-5"><a href=""></a>  batch_size<span class="op">=</span>batch_size,</span>
<span id="cb6-6"><a href=""></a>  shuffle<span class="op">=</span><span class="va">True</span> <span class="co"># Whether to shuffle the data every loop, useful for SGD</span></span>
<span id="cb6-7"><a href=""></a>)</span>
<span id="cb6-8"><a href=""></a></span>
<span id="cb6-9"><a href=""></a>test_dataloader <span class="op">=</span> DataLoader(test_data, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="training-the-model" class="slide level2">
<h2>Training the model</h2>
<p>Let’s define functions to train the model for 1 epoch.</p>
<div id="08d207d4" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href=""></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm <span class="co"># This is a library that implements loading bars</span></span>
<span id="cb7-2"><a href=""></a><span class="im">import</span> sys</span>
<span id="cb7-3"><a href=""></a></span>
<span id="cb7-4"><a href=""></a><span class="kw">def</span> train_epoch(dataloader, model, loss_fn, optimizer):</span>
<span id="cb7-5"><a href=""></a>  model.train() <span class="co"># Set model to training mode</span></span>
<span id="cb7-6"><a href=""></a></span>
<span id="cb7-7"><a href=""></a>  total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-8"><a href=""></a>  total_batches <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-9"><a href=""></a></span>
<span id="cb7-10"><a href=""></a>  <span class="cf">with</span> tqdm(dataloader, unit<span class="op">=</span><span class="st">"batch"</span>, <span class="bu">file</span><span class="op">=</span>sys.stdout) <span class="im">as</span> ep_tqdm:</span>
<span id="cb7-11"><a href=""></a>    ep_tqdm.set_description(<span class="st">"Train"</span>)</span>
<span id="cb7-12"><a href=""></a>    <span class="cf">for</span> X, y <span class="kw">in</span> ep_tqdm:</span>
<span id="cb7-13"><a href=""></a>      X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb7-14"><a href=""></a></span>
<span id="cb7-15"><a href=""></a>      <span class="co"># Forward pass</span></span>
<span id="cb7-16"><a href=""></a>      pred <span class="op">=</span> model(X)</span>
<span id="cb7-17"><a href=""></a>      loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb7-18"><a href=""></a>        </span>
<span id="cb7-19"><a href=""></a>      <span class="co"># Backward pass</span></span>
<span id="cb7-20"><a href=""></a>      loss.backward()</span>
<span id="cb7-21"><a href=""></a>      optimizer.step()</span>
<span id="cb7-22"><a href=""></a></span>
<span id="cb7-23"><a href=""></a>      <span class="co"># Reset the computed gradients back to zero</span></span>
<span id="cb7-24"><a href=""></a>      optimizer.zero_grad()</span>
<span id="cb7-25"><a href=""></a></span>
<span id="cb7-26"><a href=""></a>      <span class="co"># Output stats</span></span>
<span id="cb7-27"><a href=""></a>      total_loss <span class="op">+=</span> loss</span>
<span id="cb7-28"><a href=""></a>      total_batches <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb7-29"><a href=""></a>      ep_tqdm.set_postfix(average_batch_loss<span class="op">=</span>(total_loss<span class="op">/</span>total_batches).item())</span>
<span id="cb7-30"><a href=""></a></span>
<span id="cb7-31"><a href=""></a><span class="kw">def</span> eval_epoch(dataloader, model, loss_fn):</span>
<span id="cb7-32"><a href=""></a>  model.<span class="bu">eval</span>() <span class="co"># Set model to inference mode</span></span>
<span id="cb7-33"><a href=""></a>  </span>
<span id="cb7-34"><a href=""></a>  total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-35"><a href=""></a>  total_batches <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-36"><a href=""></a>  total_samples <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-37"><a href=""></a>  total_correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-38"><a href=""></a></span>
<span id="cb7-39"><a href=""></a>  <span class="cf">with</span> torch.no_grad(): <span class="co"># Do not compute gradients</span></span>
<span id="cb7-40"><a href=""></a>    <span class="cf">with</span> tqdm(dataloader, unit<span class="op">=</span><span class="st">"batch"</span>, <span class="bu">file</span><span class="op">=</span>sys.stdout) <span class="im">as</span> ep_tqdm:</span>
<span id="cb7-41"><a href=""></a>      ep_tqdm.set_description(<span class="st">"Val"</span>)</span>
<span id="cb7-42"><a href=""></a>      <span class="cf">for</span> X, y <span class="kw">in</span> ep_tqdm:</span>
<span id="cb7-43"><a href=""></a>        X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb7-44"><a href=""></a>        pred <span class="op">=</span> model(X)</span>
<span id="cb7-45"><a href=""></a></span>
<span id="cb7-46"><a href=""></a>        total_loss <span class="op">+=</span> loss_fn(pred, y)</span>
<span id="cb7-47"><a href=""></a>        total_correct <span class="op">+=</span> (pred.argmax(dim<span class="op">=</span><span class="dv">1</span>) <span class="op">==</span> y.argmax(dim<span class="op">=</span><span class="dv">1</span>)).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>()</span>
<span id="cb7-48"><a href=""></a>        total_samples <span class="op">+=</span> <span class="bu">len</span>(X)</span>
<span id="cb7-49"><a href=""></a>        total_batches <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb7-50"><a href=""></a></span>
<span id="cb7-51"><a href=""></a>        ep_tqdm.set_postfix(average_batch_loss<span class="op">=</span>(total_loss<span class="op">/</span>total_batches).item(), accuracy<span class="op">=</span>(total_correct<span class="op">/</span>total_samples).item())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="training-the-model-1" class="slide level2">
<h2>Training the model</h2>
<p>Now all that there is left to do is to define the loss function and organize the training loop.</p>
<div id="5f5eec37" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="01edd6ff-a2d4-435d-e50f-70fc77e9655e" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href=""></a><span class="co"># Use GPU if available</span></span>
<span id="cb8-2"><a href=""></a>device <span class="op">=</span> torch.accelerator.current_accelerator().<span class="bu">type</span> <span class="cf">if</span> torch.accelerator.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb8-3"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss"> device"</span>)</span>
<span id="cb8-4"><a href=""></a></span>
<span id="cb8-5"><a href=""></a>model <span class="op">=</span> NeuralNetwork().to(device) <span class="co"># This tells pytorch which device to use when training and inferencing</span></span>
<span id="cb8-6"><a href=""></a></span>
<span id="cb8-7"><a href=""></a><span class="co"># Hyperparameters</span></span>
<span id="cb8-8"><a href=""></a>learning_rate <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb8-9"><a href=""></a>epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb8-10"><a href=""></a></span>
<span id="cb8-11"><a href=""></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss().to(device) <span class="co"># Initialize the loss function</span></span>
<span id="cb8-12"><a href=""></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate) <span class="co"># Initialize the optimizer</span></span>
<span id="cb8-13"><a href=""></a></span>
<span id="cb8-14"><a href=""></a><span class="co"># Organize the training loop</span></span>
<span id="cb8-15"><a href=""></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb8-16"><a href=""></a>  <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>t<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb8-17"><a href=""></a>  train_epoch(train_dataloader, model, loss_fn, optimizer)</span>
<span id="cb8-18"><a href=""></a>  eval_epoch(test_dataloader, model, loss_fn)</span>
<span id="cb8-19"><a href=""></a>  <span class="bu">print</span>(<span class="st">"-------------------------------"</span>)</span>
<span id="cb8-20"><a href=""></a></span>
<span id="cb8-21"><a href=""></a><span class="bu">print</span>(<span class="st">"Done!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>

</section>
<section id="training-the-model-1-output" class="slide level2 output-location-slide"><h2>Training the model</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="01edd6ff-a2d4-435d-e50f-70fc77e9655e" data-execution_count="6">
<div class="cell-output cell-output-stdout">
<pre><code>Using cuda device
Epoch 1

Train: 100%|██████████| 391/391 [00:02&lt;00:00, 171.05batch/s, average_batch_loss=1.93]
Val: 100%|██████████| 79/79 [00:00&lt;00:00, 180.31batch/s, accuracy=0.367, average_batch_loss=1.79]
-------------------------------
Epoch 2

Train: 100%|██████████| 391/391 [00:02&lt;00:00, 177.21batch/s, average_batch_loss=1.73]
Val: 100%|██████████| 79/79 [00:00&lt;00:00, 198.39batch/s, accuracy=0.407, average_batch_loss=1.69]
-------------------------------
Epoch 3

Train: 100%|██████████| 391/391 [00:02&lt;00:00, 177.61batch/s, average_batch_loss=1.64]
Val: 100%|██████████| 79/79 [00:00&lt;00:00, 196.90batch/s, accuracy=0.437, average_batch_loss=1.6] 
-------------------------------
Done!</code></pre>
</div>
</div></section><section id="training-the-model-2" class="slide level2">
<h2>Training the model</h2>
<p>We can also save the trained model to disk and load it as follows.</p>
<div id="a09e151a" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="8aec7fcb-7265-4830-c703-70038af3bd6b" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href=""></a>torch.save(model, <span class="st">'model.pth'</span>)</span>
<span id="cb10-2"><a href=""></a>model1 <span class="op">=</span> torch.load(<span class="st">'model.pth'</span>, weights_only<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-3"><a href=""></a><span class="bu">print</span>(model1)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=3072, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
</section>
<section id="training-the-model-3" class="slide level2">
<h2>Training the model</h2>
<p>You can also save only the weights of the model and not the model structure as follows.</p>
<div id="3202406c" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="fe7fa842-09c6-477e-a6de-b73d0a71ad00" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href=""></a>torch.save(model.state_dict(), <span class="st">'model_weights.pth'</span>)</span>
<span id="cb12-2"><a href=""></a>model1 <span class="op">=</span> NeuralNetwork()</span>
<span id="cb12-3"><a href=""></a>model1.load_state_dict(torch.load(<span class="st">'model_weights.pth'</span>, weights_only<span class="op">=</span><span class="va">True</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>&lt;All keys matched successfully&gt;</code></pre>
</div>
</div>
</section>
<section id="convolution" class="slide level2">
<h2>Convolution</h2>
<p>Convolutional Neural Networks (CNNs) are networks that have convolutional layers. Convolutional layers perform the convolution operation.</p>
<p>Convolution operation is pretty simple. You take your input matrix and then you take a smaller matrix called the filter. Then you slide your filter matrix along the input matrix and at each step you multiply the matrices componentwise and sum up. This way you get a matrix as output.</p>
</section>
<section id="convolution-1" class="slide level2">
<h2>Convolution</h2>
<p>Its easier to understand the operation by looking at a picture:</p>

<img data-src="../images/convolution.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="convolution-2" class="slide level2">
<h2>Convolution</h2>
<p>We’ve defined the 2d convolution operation. You can define 1d or <span class="math inline">\(n\)</span>d convolution operations similarly.</p>
<p>Convolutions let the model group up inputs that are spatially close to each other and then learn patterns.</p>
<p>Also convolutions can learn patterns irrespective of where they appear in the input. This can be quite difficult for a model with only fully connected layers.</p>
<p>These two properties make convolutions work very well on images, as you can imagine.</p>
</section>
<section id="convolution-3" class="slide level2">
<h2>Convolution</h2>
<p>Let’s see how to apply the convolution operation in PyTorch.</p>
<div id="05d11988" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:349}}" data-outputid="63d3a9df-aca0-474e-8f17-e479d7225760" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href=""></a><span class="im">import</span> torch</span>
<span id="cb14-2"><a href=""></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb14-3"><a href=""></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-4"><a href=""></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb14-5"><a href=""></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-6"><a href=""></a></span>
<span id="cb14-7"><a href=""></a><span class="kw">def</span> show(img):</span>
<span id="cb14-8"><a href=""></a>  <span class="co">"""Function for displaying image"""</span></span>
<span id="cb14-9"><a href=""></a>  plt.imshow(img.squeeze(), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb14-10"><a href=""></a>  plt.axis(<span class="st">'off'</span>)</span>
<span id="cb14-11"><a href=""></a></span>
<span id="cb14-12"><a href=""></a>img <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">'../images/castle.jpg'</span>)</span>
<span id="cb14-13"><a href=""></a>img.load()</span>
<span id="cb14-14"><a href=""></a>img <span class="op">=</span> torch.from_numpy(np.asarray(img, dtype<span class="op">=</span><span class="st">"int32"</span>) <span class="op">/</span> <span class="dv">255</span>)</span>
<span id="cb14-15"><a href=""></a></span>
<span id="cb14-16"><a href=""></a><span class="bu">print</span>(<span class="st">"Input image:"</span>)</span>
<span id="cb14-17"><a href=""></a>show(img)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>

</section>
<section id="convolution-3-output" class="slide level2 output-location-slide"><h2>Convolution</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:349}}" data-outputid="63d3a9df-aca0-474e-8f17-e479d7225760" data-execution_count="9">
<div class="cell-output cell-output-stdout">
<pre><code>Input image:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="05_convolutional_neural_networks_files/figure-revealjs/cell-10-output-2.png"></p>
</figure>
</div>
</div>
</div></section><section id="convolution-4" class="slide level2">
<h2>Convolution</h2>
<p>Let’s apply the edge detection filter.</p>
<div id="9fd03109" class="cell" data-outputid="b5b62f99-bef1-460c-95a2-af85054d6a2b" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href=""></a>conv <span class="op">=</span> nn.Conv2d(</span>
<span id="cb16-2"><a href=""></a>  in_channels<span class="op">=</span><span class="dv">1</span>, <span class="co"># How many channel our input has, we have a monochrome image so its 1 in our case</span></span>
<span id="cb16-3"><a href=""></a>  out_channels<span class="op">=</span><span class="dv">1</span>, <span class="co"># How many filters to use</span></span>
<span id="cb16-4"><a href=""></a>  kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), <span class="co"># Filter matrix size</span></span>
<span id="cb16-5"><a href=""></a>  bias<span class="op">=</span><span class="va">False</span></span>
<span id="cb16-6"><a href=""></a>)</span>
<span id="cb16-7"><a href=""></a>filter_matrix <span class="op">=</span> torch.tensor(</span>
<span id="cb16-8"><a href=""></a>  [[<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb16-9"><a href=""></a>  [<span class="op">-</span><span class="dv">1</span>,  <span class="dv">8</span>, <span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb16-10"><a href=""></a>  [<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]], dtype<span class="op">=</span>torch.float64</span>
<span id="cb16-11"><a href=""></a>).unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>) <span class="co"># The unsqueezes are there to have the correct dimensions</span></span>
<span id="cb16-12"><a href=""></a>conv.weight <span class="op">=</span> nn.Parameter(filter_matrix) <span class="co"># Manually set the filter</span></span>
<span id="cb16-13"><a href=""></a><span class="bu">print</span>(<span class="st">"Edge detection applied:"</span>)</span>
<span id="cb16-14"><a href=""></a>show(conv(img.unsqueeze(<span class="dv">0</span>)).detach().numpy()[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>

</section>
<section id="convolution-4-output" class="slide level2 output-location-slide"><h2>Convolution</h2><div class="cell output-location-slide" data-outputid="b5b62f99-bef1-460c-95a2-af85054d6a2b" data-execution_count="10">
<div class="cell-output cell-output-stdout">
<pre><code>Edge detection applied:</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="05_convolutional_neural_networks_files/figure-revealjs/cell-11-output-2.png"></p>
</figure>
</div>
</div>
</div></section><section id="convolutional-neural-networks" class="slide level2">
<h2>Convolutional Neural Networks</h2>
<p>Let’s build a CNN.</p>
<div id="1b1d1013" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="47950b54-fe7d-46e4-af9a-e8b5f7304c1d" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href=""></a><span class="kw">class</span> CNN(nn.Module):</span>
<span id="cb18-2"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb18-3"><a href=""></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-4"><a href=""></a>    <span class="va">self</span>.convolution <span class="op">=</span> nn.Sequential(</span>
<span id="cb18-5"><a href=""></a>      nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>, out_channels<span class="op">=</span><span class="dv">12</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>)),</span>
<span id="cb18-6"><a href=""></a>      nn.MaxPool2d(kernel_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>)),</span>
<span id="cb18-7"><a href=""></a>      nn.BatchNorm2d(num_features<span class="op">=</span><span class="dv">12</span>), <span class="co"># num_features - how many channels the input has</span></span>
<span id="cb18-8"><a href=""></a>      nn.Conv2d(in_channels<span class="op">=</span><span class="dv">12</span>, out_channels<span class="op">=</span><span class="dv">24</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>)),</span>
<span id="cb18-9"><a href=""></a>      nn.MaxPool2d(kernel_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb18-10"><a href=""></a>    )</span>
<span id="cb18-11"><a href=""></a>    <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb18-12"><a href=""></a>    <span class="va">self</span>.linear_relu_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb18-13"><a href=""></a>      nn.Linear(<span class="dv">864</span>, <span class="dv">1028</span>),</span>
<span id="cb18-14"><a href=""></a>      nn.ReLU(),</span>
<span id="cb18-15"><a href=""></a>      nn.Linear(<span class="dv">1028</span>, <span class="dv">1028</span>),</span>
<span id="cb18-16"><a href=""></a>      nn.ReLU(),</span>
<span id="cb18-17"><a href=""></a>      nn.Linear(<span class="dv">1028</span>, <span class="dv">10</span>),</span>
<span id="cb18-18"><a href=""></a>    )</span>
<span id="cb18-19"><a href=""></a></span>
<span id="cb18-20"><a href=""></a>  <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb18-21"><a href=""></a>    x <span class="op">=</span> <span class="va">self</span>.convolution(x)</span>
<span id="cb18-22"><a href=""></a>    x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb18-23"><a href=""></a>    <span class="cf">return</span> <span class="va">self</span>.linear_relu_stack(x)</span>
<span id="cb18-24"><a href=""></a></span>
<span id="cb18-25"><a href=""></a><span class="bu">print</span>(CNN())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>

</section>
<section id="convolutional-neural-networks-output" class="slide level2 output-location-slide"><h2>Convolutional Neural Networks</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="47950b54-fe7d-46e4-af9a-e8b5f7304c1d" data-execution_count="11">
<div class="cell-output cell-output-stdout">
<pre><code>CNN(
  (convolution): Sequential(
    (0): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1))
    (1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
    (2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1))
    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  )
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=864, out_features=1028, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1028, out_features=1028, bias=True)
    (3): ReLU()
    (4): Linear(in_features=1028, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div></section><section id="convolutional-neural-networks-1" class="slide level2">
<h2>Convolutional Neural Networks</h2>
<p>We used two layers we have not discussed yet:</p>
<ol type="1">
<li>Max pooling</li>
<li>Batch normalization</li>
</ol>
</section>
<section id="convolutional-neural-networks-2" class="slide level2">
<h2>Convolutional Neural Networks</h2>
<p>Max pooling constructs a lower resolution image from a given one by keeping only the max value from a region. These layers are commonly used with convolution layers to make the image slightly more translation invariant.</p>
<p>This image summarizes max pooling:</p>

<img data-src="../images/maxpooling.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="convolutional-neural-networks-3" class="slide level2">
<h2>Convolutional Neural Networks</h2>
<p>Batch normalization layer is used during training to normalize the batch (that is make the mean of the batch 0 and standard deviation 1). This layer helps normalize the gradient so that it is not too big or too small so that the model is able to learn efficiently.</p>
<p>The batch normalization layer should only be used during training and turned off during inference.</p>
<p>Calling <code>model.train()</code> sets the model to training mode and turns on layers like batch normalization while calling <code>model.eval()</code> sets model to evaluation mode and turns these layers off.</p>
</section>
<section id="convolutional-neural-networks-4" class="slide level2">
<h2>Convolutional Neural Networks</h2>
<p>Let’s train our CNN. We can copy paste the previous code.</p>
<div id="36c207b5" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="6b8c795b-763f-42f6-a425-ae41439f3fee" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href=""></a><span class="co"># Use GPU if available</span></span>
<span id="cb20-2"><a href=""></a>device <span class="op">=</span> torch.accelerator.current_accelerator().<span class="bu">type</span> <span class="cf">if</span> torch.accelerator.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb20-3"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss"> device"</span>)</span>
<span id="cb20-4"><a href=""></a></span>
<span id="cb20-5"><a href=""></a>model <span class="op">=</span> CNN().to(device) <span class="co"># This tells pytorch which device to use when training and inferencing</span></span>
<span id="cb20-6"><a href=""></a></span>
<span id="cb20-7"><a href=""></a><span class="co"># Hyperparameters</span></span>
<span id="cb20-8"><a href=""></a>learning_rate <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb20-9"><a href=""></a>epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb20-10"><a href=""></a></span>
<span id="cb20-11"><a href=""></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss().to(device) <span class="co"># Initialize the loss function</span></span>
<span id="cb20-12"><a href=""></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate) <span class="co"># Initialize the optimizer</span></span>
<span id="cb20-13"><a href=""></a></span>
<span id="cb20-14"><a href=""></a><span class="co"># Organize the training loop</span></span>
<span id="cb20-15"><a href=""></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb20-16"><a href=""></a>  <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>t<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb20-17"><a href=""></a>  train_epoch(train_dataloader, model, loss_fn, optimizer)</span>
<span id="cb20-18"><a href=""></a>  eval_epoch(test_dataloader, model, loss_fn)</span>
<span id="cb20-19"><a href=""></a>  <span class="bu">print</span>(<span class="st">"-------------------------------"</span>)</span>
<span id="cb20-20"><a href=""></a></span>
<span id="cb20-21"><a href=""></a><span class="bu">print</span>(<span class="st">"Done!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>

</section>
<section id="convolutional-neural-networks-4-output" class="slide level2 output-location-slide"><h2>Convolutional Neural Networks</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="6b8c795b-763f-42f6-a425-ae41439f3fee" data-execution_count="12">
<div class="cell-output cell-output-stdout">
<pre><code>Using cuda device
Epoch 1

Train: 100%|██████████| 391/391 [00:02&lt;00:00, 163.50batch/s, average_batch_loss=1.55]
Val: 100%|██████████| 79/79 [00:00&lt;00:00, 194.79batch/s, accuracy=0.523, average_batch_loss=1.34]
-------------------------------
Epoch 2

Train: 100%|██████████| 391/391 [00:02&lt;00:00, 160.16batch/s, average_batch_loss=1.25]
Val: 100%|██████████| 79/79 [00:00&lt;00:00, 192.54batch/s, accuracy=0.579, average_batch_loss=1.19]
-------------------------------
Epoch 3

Train: 100%|██████████| 391/391 [00:02&lt;00:00, 163.64batch/s, average_batch_loss=1.1] 
Val: 100%|██████████| 79/79 [00:00&lt;00:00, 191.92batch/s, accuracy=0.609, average_batch_loss=1.1] 
-------------------------------
Done!</code></pre>
</div>
</div></section><section id="fine-tuning" class="slide level2">
<h2>Fine tuning</h2>
<p>One nice thing about neural networks is that they are quite reusable. You can take a network that was pre trained on some dataset, change its output layers to fit your problem and then run a few epochs worth of training on your dataset. This process is called <strong>fine tuning</strong>.</p>
<p>Fine tuning pre trained networks reduces training costs as you will typically need less training time to get good results when compared to how long it would take you on a “fresh” model.</p>
</section>
<section id="fine-tuning-1" class="slide level2">
<h2>Fine tuning</h2>
<p>Let’s see how to do fine tuning in PyTorch.</p>
<p>We will use ResNet. ResNet is a CNN model. The innovation of ResNet are residual connections (res stands for “residual”). They look like this:</p>

<img data-src="../images/residuallayer.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="fine-tuning-2" class="slide level2">
<h2>Fine tuning</h2>
<p>That is in a residual connection we sum up the output of downstream layers with the output of the current layer.</p>
<p>This is again done to help with training. It was noticed that by the time backpropagation gets to the early layers of the network the gradient is almost zero, therefore it is hard to train especially deep neural networks. Residual layers help mitigate this problem.</p>
</section>
<section id="fine-tuning-3" class="slide level2">
<h2>Fine tuning</h2>
<p>Torchvision ships with some pre trained models for image processing tasks, ResNet included.</p>
<p>We are going to use ResNet18 which is the smallest version. You can see all available versions <a href="https://pytorch.org/vision/main/models/resnet.html">here</a>.</p>
<div id="V6agFYXba9fP" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ae744183-4a4a-40c5-f2e2-2db74235c68e" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href=""></a><span class="im">from</span> torchvision <span class="im">import</span> models</span>
<span id="cb22-2"><a href=""></a></span>
<span id="cb22-3"><a href=""></a>resnet <span class="op">=</span> models.resnet18(weights<span class="op">=</span><span class="st">"ResNet18_Weights.DEFAULT"</span>)</span>
<span id="cb22-4"><a href=""></a></span>
<span id="cb22-5"><a href=""></a><span class="co"># Change the output layer dimension</span></span>
<span id="cb22-6"><a href=""></a>resnet.fc <span class="op">=</span> nn.Linear(resnet.fc.in_features, <span class="dv">10</span>)</span>
<span id="cb22-7"><a href=""></a></span>
<span id="cb22-8"><a href=""></a>resnet</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>

</section>
<section id="fine-tuning-3-output" class="slide level2 output-location-slide"><h2>Fine tuning</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ae744183-4a4a-40c5-f2e2-2db74235c68e" data-execution_count="13">
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=10, bias=True)
)</code></pre>
</div>
</div></section><section id="fine-tuning-4" class="slide level2">
<h2>Fine tuning</h2>
<p>One more option in fine tuning is to freeze some of the weights of the model. This allows you to train only a part of the model. We are not going to do this as it is unnecessary in our case but this technique is good to keep in mind.</p>
</section>
<section id="fine-tuning-5" class="slide level2">
<h2>Fine tuning</h2>
<p>We can again reuse the code for training.</p>
<div id="bdl9HNkUbIWj" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1ee31da1-39dd-4a43-eab4-e118a040ec2f" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href=""></a><span class="co"># Use GPU if available</span></span>
<span id="cb24-2"><a href=""></a>device <span class="op">=</span> torch.accelerator.current_accelerator().<span class="bu">type</span> <span class="cf">if</span> torch.accelerator.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb24-3"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss"> device"</span>)</span>
<span id="cb24-4"><a href=""></a></span>
<span id="cb24-5"><a href=""></a>model <span class="op">=</span> resnet.to(device) <span class="co"># This tells pytorch which device to use when training and inferencing</span></span>
<span id="cb24-6"><a href=""></a></span>
<span id="cb24-7"><a href=""></a><span class="co"># Hyperparameters</span></span>
<span id="cb24-8"><a href=""></a>learning_rate <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb24-9"><a href=""></a>epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb24-10"><a href=""></a></span>
<span id="cb24-11"><a href=""></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss().to(device) <span class="co"># Initialize the loss function</span></span>
<span id="cb24-12"><a href=""></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate) <span class="co"># Initialize the optimizer</span></span>
<span id="cb24-13"><a href=""></a></span>
<span id="cb24-14"><a href=""></a><span class="co"># Organize the training loop</span></span>
<span id="cb24-15"><a href=""></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb24-16"><a href=""></a>  <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>t<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">-------------------------------"</span>)</span>
<span id="cb24-17"><a href=""></a>  train_epoch(train_dataloader, model, loss_fn, optimizer)</span>
<span id="cb24-18"><a href=""></a>  eval_epoch(test_dataloader, model, loss_fn)</span>
<span id="cb24-19"><a href=""></a></span>
<span id="cb24-20"><a href=""></a><span class="bu">print</span>(<span class="st">"Done!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>

</section>
<section id="fine-tuning-5-output" class="slide level2 output-location-slide"><h2>Fine tuning</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1ee31da1-39dd-4a43-eab4-e118a040ec2f" data-execution_count="14">
<div class="cell-output cell-output-stdout">
<pre><code>Using cuda device
Epoch 1
-------------------------------
Train: 100%|██████████| 391/391 [00:03&lt;00:00, 100.09batch/s, average_batch_loss=1.03]
Val: 100%|██████████| 79/79 [00:00&lt;00:00, 152.04batch/s, accuracy=0.755, average_batch_loss=0.706]
Epoch 2
-------------------------------
Train: 100%|██████████| 391/391 [00:03&lt;00:00, 99.27batch/s, average_batch_loss=0.562] 
Val: 100%|██████████| 79/79 [00:00&lt;00:00, 153.76batch/s, accuracy=0.78, average_batch_loss=0.647] 
Epoch 3
-------------------------------
Train: 100%|██████████| 391/391 [00:03&lt;00:00, 99.58batch/s, average_batch_loss=0.371] 
Val: 100%|██████████| 79/79 [00:00&lt;00:00, 155.17batch/s, accuracy=0.794, average_batch_loss=0.629]
Done!</code></pre>
</div>
</div></section><section id="practice-task" class="slide level2">
<h2>Practice task</h2>
<p>Try to improve the CNN model we created to get a better score on CIFAR10. Try to figure out how to implement residual connections in PyTorch.</p>
<p>Also training times will be slow, unless you use a GPU. You can use a GPU for free in Google Colab in <code>Runtime -&gt; Change runtime type</code>.</p>

</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="05_convolutional_neural_networks_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="05_convolutional_neural_networks_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="05_convolutional_neural_networks_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="05_convolutional_neural_networks_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="05_convolutional_neural_networks_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="05_convolutional_neural_networks_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="05_convolutional_neural_networks_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="05_convolutional_neural_networks_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="05_convolutional_neural_networks_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="05_convolutional_neural_networks_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>