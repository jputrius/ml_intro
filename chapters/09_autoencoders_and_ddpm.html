<!DOCTYPE html>
<html lang="en"><head>
<script src="09_autoencoders_and_ddpm_files/libs/clipboard/clipboard.min.js"></script>
<script src="09_autoencoders_and_ddpm_files/libs/quarto-html/tabby.min.js"></script>
<script src="09_autoencoders_and_ddpm_files/libs/quarto-html/popper.min.js"></script>
<script src="09_autoencoders_and_ddpm_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="09_autoencoders_and_ddpm_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="09_autoencoders_and_ddpm_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="09_autoencoders_and_ddpm_files/libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.24">

  <title>Autoencoders and DDPM</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="09_autoencoders_and_ddpm_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="09_autoencoders_and_ddpm_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="09_autoencoders_and_ddpm_files/libs/revealjs/dist/theme/quarto-9df06d9b3e1683bd31835b8738a1bbfc.css">
  <link href="09_autoencoders_and_ddpm_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="09_autoencoders_and_ddpm_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="09_autoencoders_and_ddpm_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="09_autoencoders_and_ddpm_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Autoencoders and DDPM</h1>

<div class="quarto-title-authors">
</div>

</section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
<p>Up to now we’ve been mostly discussing how to use neural networks in supervised learning. They can also be used in unsupervised learning tasks.</p>
<p>In this chapter we cover two ideas: autoencoders and denoising diffusion probabilistic models (DDPM).</p>
</section>
<section id="autoencoders" class="slide level2">
<h2>Autoencoders</h2>
<p><strong>Autoencoders</strong> are feed forward neural networks that have the following shape:</p>

<img data-src="../images/autoencoder.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="autoencoders-1" class="slide level2">
<h2>Autoencoders</h2>
<p>If you train an autoencoder by making it predict its input then the encoder part learns an efficient lower dimensional representation of your input data, thus achieving data compression.</p>
<p>There are many applications of the autoencoder idea. We will cover two.</p>
</section>
<section id="anomaly-detection" class="slide level2">
<h2>Anomaly Detection</h2>
<p>You can use autoencoders to detect anomalies.</p>
<p>Let <span class="math inline">\(p\)</span> be your autoencoder and define reconstruction error <span class="math inline">\(E\)</span> as <span class="math display">\[
  E(x) = \text{MSE}(x, p(x)),
\]</span> where <span class="math inline">\(MSE\)</span> is mean squared error. So, the reconstruction error measures how well the autoencoder is able to reproduce the input.</p>
<p>Note that you can define the reconstruction error in many different ways depending on your specific problem, this is just one possibility.</p>
</section>
<section id="anomaly-detection-1" class="slide level2">
<h2>Anomaly Detection</h2>
<p>The autoencoder should be much better at reconstructing data that it has been trained on.</p>
<p>You can use this for anomaly detection by training the autoencoder on typical examples of your data and then (hopefully) the reconstruction error of anomalies will be larger on average then the reconstruction error of typical data.</p>
<p>Let’s try out this idea. For examples in this chapter we will use the MNIST dataset. It is a dataset of handwritten digits.</p>
<p>Suppose we consider ones to be typical examples of our data and zeros to be anomalies. Let’s load the dataset.</p>
</section>
<section id="anomaly-detection-2" class="slide level2">
<h2>Anomaly Detection</h2>
<div id="35642459" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:371,&quot;referenced_widgets&quot;:[&quot;91cb262120b640dcb4d49951a9775c2f&quot;,&quot;e9675b56c3d24485969179a7b06e97f7&quot;,&quot;106f29fde12e48e7911e9ad8daa74238&quot;,&quot;a7142a68a35e43f494cbc222cd534a34&quot;,&quot;f8c27f75a68c46458e581c6e4ec4113d&quot;,&quot;33d07c3d29634cd6aa66c3db6d3f4f4c&quot;,&quot;d0996340ffbc4769b63e010d496789c8&quot;,&quot;25783082924e4188b902445981044f8d&quot;,&quot;108223a3c79743a7ae17240aab40783b&quot;,&quot;5992713f602948609997c9748c67b939&quot;,&quot;0e8f102d4da04732a6cb0d6e8cd018cf&quot;,&quot;c4b51bc3470d43d5bca24f6db1802562&quot;,&quot;b08c20567474405dbc10ebe9ebb193a8&quot;,&quot;1aafd93544dd4219bb99d584bd38cfc7&quot;,&quot;07cdc93c94d448c08c606c8c33ed27b7&quot;,&quot;9ea5dc7507fa47d2b9ddfba9dbe0db4e&quot;,&quot;6acba2eb6a664908b9067756aa49c726&quot;,&quot;50cfb97d824649cf81c3b7f0e7f8c902&quot;,&quot;60cc18fbc15d42309dfc74078b6b264b&quot;,&quot;6b839594b37b4d68abe14a4ac22e2857&quot;,&quot;b39a1e25388346889e4a1733fa190baf&quot;,&quot;87eb7e656ffd4e0e905b9e76db367094&quot;,&quot;4a15a69d1fb34b4abe33759660b09180&quot;,&quot;0e671043af2b4ccfa8152572b9f4140b&quot;,&quot;c65eb186a4cc4a728a9690c7cb496961&quot;,&quot;c7fbbdfe780a440baab343b5a16c38fc&quot;,&quot;534133cd68e2438c91a99ca1e4477b43&quot;,&quot;f914cbb644d94022ab6585833daf1f4d&quot;,&quot;5e0eda7ba5324940981132fa14658930&quot;,&quot;633402731d634696b18621ef8b9f57d1&quot;,&quot;a268b3608ee44cf79637f6e63bacdb1c&quot;,&quot;0745d7d97705471285713c5364bd3187&quot;,&quot;e3ffaa9ab91c44d694f3b6c8bce94635&quot;,&quot;191ced0f72fb47bf82214ae43de50efd&quot;,&quot;28841f6a14694a03b60e6f6eb1315069&quot;,&quot;e3d3f3a88911479da761b4c1993e48a6&quot;,&quot;444fd73ba9c04121acf9863ec0e4e88f&quot;,&quot;91063bf662c34852904d023b8f803906&quot;,&quot;7961a39842cd43eaa50f58c88de5f4c0&quot;,&quot;59a4732e21434df6b34ae602659d89a3&quot;,&quot;d95ba8d5b37e405a9dd81fee36e03575&quot;,&quot;6f711cbecdec4abbbe0c3325adcd4a0e&quot;,&quot;b2614db6bc794e228962031519450743&quot;,&quot;c23e403cc1764b9dac22ebc1b18beee0&quot;,&quot;366f579514ff49eeb76b69572c66ccad&quot;,&quot;449a6baa200640a088f8e7a1bea834b4&quot;,&quot;22954eee33294aea858990dda18e73b9&quot;,&quot;07d988be9a0a4924bf122328a44c4b30&quot;,&quot;8aafad5aea9c4815b4639d9599b70ad2&quot;,&quot;92f19c551b904aa0b925333d3eda73a4&quot;,&quot;626e13c7143b43128f65a8d3eea0de8d&quot;,&quot;f6b4da9135a848a0a39d5ff2eddab083&quot;,&quot;4d88c269b70e4a958f7a87175e4db8c5&quot;,&quot;a1ce0d32261a4e1698d32a5907f5ebac&quot;,&quot;26cda09f2cfe49a08bcd18f33d98a6d7&quot;,&quot;8dcf0d0dbe6e4516bc3df3b4d355b626&quot;,&quot;b5f75b64c00f441fa82f68ab5a5de810&quot;,&quot;1bbdb985a59b45c289c1461f1ba1a498&quot;,&quot;68bb1ceb4e9947cd9671521de88bc437&quot;,&quot;d6257bf0734e4b7680326dda4465074f&quot;,&quot;24ad1ff1c43442f9a9b3c6fcef7fa8a7&quot;,&quot;5ffb22156c164381a40c4f3a80c51f87&quot;,&quot;e3bd102d6c714a9ab52455e2ecde6efb&quot;,&quot;661e7cf364bd4ea9ac1ca520bebeb794&quot;,&quot;56660a0225304f6bbdbb84021eadaee3&quot;,&quot;2ec0b03df38244e9a83ac077923f898d&quot;,&quot;2335e2c516414e0495ca978f8ee4ac7b&quot;,&quot;a1316c00ef7b413395281bff7969e441&quot;,&quot;7528e51a7af246a5852f6b07bcb2c99a&quot;,&quot;728f5ac143fd4e2b8901b8ee73e27ee3&quot;,&quot;76b3721eb4a34cc1b05ac893686ed6d7&quot;,&quot;b992246ea0c0461a9195d62af9ee0d67&quot;,&quot;b1ffb9b1f43a47fa9a7c1643e9d9d8aa&quot;,&quot;34325a29bf934bb589d832ee4d8a8d53&quot;,&quot;8002df47b0b04537ab268ef04f7d2907&quot;,&quot;c3029c9065e44e20981a44fc9eca6bd9&quot;,&quot;ff4d1e9b2c3c48ea9c34fd4cb52f3d94&quot;]}}" data-outputid="d04081dd-e64c-4565-b65b-0e9f044fe1c8" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href=""></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href=""></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset</span>
<span id="cb1-3"><a href=""></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb1-4"><a href=""></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href=""></a></span>
<span id="cb1-6"><a href=""></a><span class="kw">class</span> MNISTAnomalies(Dataset):</span>
<span id="cb1-7"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, train<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb1-8"><a href=""></a>    <span class="va">self</span>.dataset <span class="op">=</span> load_dataset(<span class="st">"ylecun/mnist"</span>)</span>
<span id="cb1-9"><a href=""></a>    <span class="cf">if</span> train:</span>
<span id="cb1-10"><a href=""></a>      <span class="va">self</span>.dataset <span class="op">=</span> <span class="va">self</span>.dataset[<span class="st">"train"</span>]</span>
<span id="cb1-11"><a href=""></a>      <span class="va">self</span>.dataset <span class="op">=</span> <span class="va">self</span>.dataset.<span class="bu">filter</span>(<span class="kw">lambda</span> row: row[<span class="st">"label"</span>] <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb1-12"><a href=""></a>    <span class="cf">else</span>:</span>
<span id="cb1-13"><a href=""></a>      <span class="va">self</span>.dataset <span class="op">=</span> <span class="va">self</span>.dataset[<span class="st">"test"</span>]</span>
<span id="cb1-14"><a href=""></a>      <span class="va">self</span>.dataset <span class="op">=</span> <span class="va">self</span>.dataset.<span class="bu">filter</span>(<span class="kw">lambda</span> row: row[<span class="st">"label"</span>] <span class="op">==</span> <span class="dv">1</span> <span class="kw">or</span> row[<span class="st">"label"</span>] <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb1-15"><a href=""></a></span>
<span id="cb1-16"><a href=""></a>  <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb1-17"><a href=""></a>    <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.dataset)</span>
<span id="cb1-18"><a href=""></a></span>
<span id="cb1-19"><a href=""></a>  <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb1-20"><a href=""></a>    image <span class="op">=</span> torch.from_numpy(np.asarray(<span class="va">self</span>.dataset[idx][<span class="st">"image"</span>], dtype<span class="op">=</span><span class="st">"int32"</span>) <span class="op">/</span> <span class="dv">255</span>).to(torch.<span class="bu">float</span>)</span>
<span id="cb1-21"><a href=""></a>    image <span class="op">=</span> image.view(<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb1-22"><a href=""></a></span>
<span id="cb1-23"><a href=""></a>    label <span class="op">=</span> <span class="va">self</span>.dataset[idx][<span class="st">"label"</span>]</span>
<span id="cb1-24"><a href=""></a>    label <span class="op">=</span> torch.tensor(label)</span>
<span id="cb1-25"><a href=""></a>    <span class="cf">return</span> image, label</span>
<span id="cb1-26"><a href=""></a></span>
<span id="cb1-27"><a href=""></a>train_anomalies <span class="op">=</span> MNISTAnomalies(train<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-28"><a href=""></a>test_anomalies <span class="op">=</span> MNISTAnomalies(train<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="anomaly-detection-3" class="slide level2">
<h2>Anomaly Detection</h2>
<p>Now let’s build our autoencoder.</p>
<div id="629b928b" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="7943f468-838d-4270-d44e-abd9c6909f38" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href=""></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb2-2"><a href=""></a></span>
<span id="cb2-3"><a href=""></a><span class="kw">class</span> Autoencoder(nn.Module):</span>
<span id="cb2-4"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb2-5"><a href=""></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-6"><a href=""></a>    <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten() <span class="co"># Flattens out the image into a vector</span></span>
<span id="cb2-7"><a href=""></a>    <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb2-8"><a href=""></a>      nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">128</span>),</span>
<span id="cb2-9"><a href=""></a>      nn.ReLU(),</span>
<span id="cb2-10"><a href=""></a>      nn.Linear(<span class="dv">128</span>, <span class="dv">128</span>)</span>
<span id="cb2-11"><a href=""></a>    )</span>
<span id="cb2-12"><a href=""></a>    <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb2-13"><a href=""></a>      nn.Linear(<span class="dv">128</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>),</span>
<span id="cb2-14"><a href=""></a>      nn.Sigmoid(),</span>
<span id="cb2-15"><a href=""></a>    )</span>
<span id="cb2-16"><a href=""></a></span>
<span id="cb2-17"><a href=""></a>  <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-18"><a href=""></a>    b, c, h, w <span class="op">=</span> x.shape</span>
<span id="cb2-19"><a href=""></a>    x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb2-20"><a href=""></a>    x <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb2-21"><a href=""></a>    x <span class="op">=</span> <span class="va">self</span>.decoder(x)</span>
<span id="cb2-22"><a href=""></a>    <span class="cf">return</span> x.view(b, c, h, w)</span>
<span id="cb2-23"><a href=""></a></span>
<span id="cb2-24"><a href=""></a><span class="bu">print</span>(Autoencoder())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>

</section>
<section id="anomaly-detection-3-output" class="slide level2 output-location-slide"><h2>Anomaly Detection</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="7943f468-838d-4270-d44e-abd9c6909f38" data-execution_count="3">
<div class="cell-output cell-output-stdout">
<pre><code>Autoencoder(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (encoder): Sequential(
    (0): Linear(in_features=784, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
  )
  (decoder): Sequential(
    (0): Linear(in_features=128, out_features=784, bias=True)
    (1): Sigmoid()
  )
)</code></pre>
</div>
</div></section><section id="anomaly-detection-4" class="slide level2">
<h2>Anomaly Detection</h2>
<p>Now let’s train it.</p>
<div id="7fb39250" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="63afd5ef-0a83-4c54-b9d7-93a1ffeeb8b1" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href=""></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb4-2"><a href=""></a><span class="im">import</span> sys</span>
<span id="cb4-3"><a href=""></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb4-4"><a href=""></a></span>
<span id="cb4-5"><a href=""></a><span class="co"># Use GPU if available</span></span>
<span id="cb4-6"><a href=""></a>device <span class="op">=</span> torch.accelerator.current_accelerator().<span class="bu">type</span> <span class="cf">if</span> torch.accelerator.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb4-7"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss"> device"</span>)</span>
<span id="cb4-8"><a href=""></a></span>
<span id="cb4-9"><a href=""></a>model <span class="op">=</span> Autoencoder().to(device)</span>
<span id="cb4-10"><a href=""></a></span>
<span id="cb4-11"><a href=""></a><span class="co"># Some hyperparameters</span></span>
<span id="cb4-12"><a href=""></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb4-13"><a href=""></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb4-14"><a href=""></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb4-15"><a href=""></a></span>
<span id="cb4-16"><a href=""></a>loss_fn <span class="op">=</span> nn.MSELoss().to(device)</span>
<span id="cb4-17"><a href=""></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb4-18"><a href=""></a></span>
<span id="cb4-19"><a href=""></a>train_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb4-20"><a href=""></a>  train_anomalies,</span>
<span id="cb4-21"><a href=""></a>  batch_size<span class="op">=</span>batch_size,</span>
<span id="cb4-22"><a href=""></a>  shuffle<span class="op">=</span><span class="va">True</span></span>
<span id="cb4-23"><a href=""></a>)</span>
<span id="cb4-24"><a href=""></a></span>
<span id="cb4-25"><a href=""></a><span class="kw">def</span> train_epoch(dataloader, model, loss_fn, optimizer):</span>
<span id="cb4-26"><a href=""></a>  model.train() <span class="co"># Set model to training mode</span></span>
<span id="cb4-27"><a href=""></a></span>
<span id="cb4-28"><a href=""></a>  total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-29"><a href=""></a>  total_batches <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-30"><a href=""></a></span>
<span id="cb4-31"><a href=""></a>  <span class="cf">with</span> tqdm(dataloader, unit<span class="op">=</span><span class="st">"batch"</span>, <span class="bu">file</span><span class="op">=</span>sys.stdout) <span class="im">as</span> ep_tqdm:</span>
<span id="cb4-32"><a href=""></a>    ep_tqdm.set_description(<span class="st">"Train"</span>)</span>
<span id="cb4-33"><a href=""></a>    <span class="cf">for</span> X, _ <span class="kw">in</span> ep_tqdm:</span>
<span id="cb4-34"><a href=""></a>      X <span class="op">=</span> X.to(device)</span>
<span id="cb4-35"><a href=""></a></span>
<span id="cb4-36"><a href=""></a>      <span class="co"># Forward pass</span></span>
<span id="cb4-37"><a href=""></a>      pred <span class="op">=</span> model(X)</span>
<span id="cb4-38"><a href=""></a>      loss <span class="op">=</span> loss_fn(pred, X)</span>
<span id="cb4-39"><a href=""></a>        </span>
<span id="cb4-40"><a href=""></a>      <span class="co"># Backward pass</span></span>
<span id="cb4-41"><a href=""></a>      loss.backward()</span>
<span id="cb4-42"><a href=""></a>      optimizer.step()</span>
<span id="cb4-43"><a href=""></a></span>
<span id="cb4-44"><a href=""></a>      <span class="co"># Reset the computed gradients back to zero</span></span>
<span id="cb4-45"><a href=""></a>      optimizer.zero_grad()</span>
<span id="cb4-46"><a href=""></a></span>
<span id="cb4-47"><a href=""></a>      <span class="co"># Output stats</span></span>
<span id="cb4-48"><a href=""></a>      total_loss <span class="op">+=</span> loss</span>
<span id="cb4-49"><a href=""></a>      total_batches <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-50"><a href=""></a>      ep_tqdm.set_postfix(average_batch_loss<span class="op">=</span>(total_loss<span class="op">/</span>total_batches).item())</span>
<span id="cb4-51"><a href=""></a></span>
<span id="cb4-52"><a href=""></a><span class="co"># Organize the training loop</span></span>
<span id="cb4-53"><a href=""></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb4-54"><a href=""></a>  <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>t<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">-------------------------------"</span>)</span>
<span id="cb4-55"><a href=""></a>  train_epoch(train_dataloader, model, loss_fn, optimizer)</span>
<span id="cb4-56"><a href=""></a></span>
<span id="cb4-57"><a href=""></a><span class="bu">print</span>(<span class="st">"Done!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>

</section>
<section id="anomaly-detection-4-output" class="slide level2 output-location-slide"><h2>Anomaly Detection</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="63afd5ef-0a83-4c54-b9d7-93a1ffeeb8b1" data-execution_count="4">
<div class="cell-output cell-output-stdout">
<pre><code>Using cuda device
Epoch 1
-------------------------------
Train: 100%|██████████| 106/106 [00:01&lt;00:00, 74.78batch/s, average_batch_loss=0.045] 
Epoch 2
-------------------------------
Train: 100%|██████████| 106/106 [00:01&lt;00:00, 82.13batch/s, average_batch_loss=0.0158]
Epoch 3
-------------------------------
Train: 100%|██████████| 106/106 [00:01&lt;00:00, 80.53batch/s, average_batch_loss=0.0124]
Epoch 4
-------------------------------
Train: 100%|██████████| 106/106 [00:01&lt;00:00, 83.12batch/s, average_batch_loss=0.00896]
Epoch 5
-------------------------------
Train: 100%|██████████| 106/106 [00:01&lt;00:00, 81.57batch/s, average_batch_loss=0.00685]
Epoch 6
-------------------------------
Train: 100%|██████████| 106/106 [00:01&lt;00:00, 80.41batch/s, average_batch_loss=0.0055]
Epoch 7
-------------------------------
Train: 100%|██████████| 106/106 [00:01&lt;00:00, 81.06batch/s, average_batch_loss=0.00467]
Epoch 8
-------------------------------
Train: 100%|██████████| 106/106 [00:01&lt;00:00, 81.67batch/s, average_batch_loss=0.0042]
Epoch 9
-------------------------------
Train: 100%|██████████| 106/106 [00:01&lt;00:00, 80.47batch/s, average_batch_loss=0.00382]
Epoch 10
-------------------------------
Train: 100%|██████████| 106/106 [00:01&lt;00:00, 81.60batch/s, average_batch_loss=0.00354]
Done!</code></pre>
</div>
</div></section><section id="anomaly-detection-5" class="slide level2">
<h2>Anomaly Detection</h2>
<p>Let’s check if it works.</p>
<div id="98805476" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="8fb505b5-a840-4c22-d0a8-89df47ad27ef" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href=""></a><span class="kw">def</span> loss(real, pred):</span>
<span id="cb6-2"><a href=""></a>  <span class="cf">return</span> ((real<span class="op">-</span>pred)<span class="op">**</span><span class="dv">2</span>).mean(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-3"><a href=""></a></span>
<span id="cb6-4"><a href=""></a>test_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb6-5"><a href=""></a>  test_anomalies,</span>
<span id="cb6-6"><a href=""></a>  batch_size <span class="op">=</span> <span class="bu">len</span>(test_anomalies)</span>
<span id="cb6-7"><a href=""></a>)</span>
<span id="cb6-8"><a href=""></a></span>
<span id="cb6-9"><a href=""></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-10"><a href=""></a>  X, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(test_dataloader))</span>
<span id="cb6-11"><a href=""></a>  X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb6-12"><a href=""></a>  reconstructed <span class="op">=</span> model(X)</span>
<span id="cb6-13"><a href=""></a></span>
<span id="cb6-14"><a href=""></a>  losses <span class="op">=</span> loss(X.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>), reconstructed.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>))</span>
<span id="cb6-15"><a href=""></a></span>
<span id="cb6-16"><a href=""></a>  <span class="bu">print</span>(<span class="ss">f"Reconstruction error of anomalies: </span><span class="sc">{</span>losses[y<span class="op">==</span><span class="dv">0</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-17"><a href=""></a>  <span class="bu">print</span>(<span class="ss">f"Reconstruction error of typical data: </span><span class="sc">{</span>losses[y<span class="op">==</span><span class="dv">1</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Reconstruction error of anomalies: 0.10304060578346252
Reconstruction error of typical data: 0.0032625803723931313</code></pre>
</div>
</div>
</section>
<section id="anomaly-detection-6" class="slide level2">
<h2>Anomaly Detection</h2>
<p>As you can see the reconstruction error of typical data is much smaller. We can also visualize some reconstructions.</p>
</section>
<section id="anomaly-detection-7" class="slide level2">
<h2>Anomaly Detection</h2>
<div id="87d31203" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:86}}" data-outputid="50c57af9-bab9-4bb3-aae8-d8e2cfebe308" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href=""></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-2"><a href=""></a></span>
<span id="cb8-3"><a href=""></a><span class="kw">def</span> show(img):</span>
<span id="cb8-4"><a href=""></a>  <span class="co">"""Function for displaying image"""</span></span>
<span id="cb8-5"><a href=""></a>  npimg <span class="op">=</span> img.detach().cpu().numpy()</span>
<span id="cb8-6"><a href=""></a>  plt.imshow(np.transpose(npimg, (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb8-7"><a href=""></a>  plt.axis(<span class="st">'off'</span>)</span>
<span id="cb8-8"><a href=""></a></span>
<span id="cb8-9"><a href=""></a>test_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb8-10"><a href=""></a>  test_anomalies,</span>
<span id="cb8-11"><a href=""></a>  batch_size <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb8-12"><a href=""></a>)</span>
<span id="cb8-13"><a href=""></a></span>
<span id="cb8-14"><a href=""></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb8-15"><a href=""></a>  X, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(test_dataloader))</span>
<span id="cb8-16"><a href=""></a>  X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb8-17"><a href=""></a>  reconstructed <span class="op">=</span> model(X)</span>
<span id="cb8-18"><a href=""></a>  show(torch.concat([X, reconstructed], axis<span class="op">=</span><span class="dv">2</span>).transpose(<span class="dv">2</span>, <span class="dv">3</span>).reshape(<span class="dv">1</span>, <span class="dv">20</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">2</span><span class="op">*</span><span class="dv">28</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>

</div>
<img data-src="09_autoencoders_and_ddpm_files/figure-revealjs/cell-7-output-1.png" class="r-stretch"></section>
<section id="denoising-images" class="slide level2">
<h2>Denoising Images</h2>
<p>You can also use autoencoders to denoise images. You do this by adding some noise to your training images and then ask the autoencoder to predict the image without noise.</p>
<p>Let’s build a dataset that adds noise to images from MNIST.</p>
</section>
<section id="denoising-images-1" class="slide level2">
<h2>Denoising Images</h2>
<div id="732ae5b5" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href=""></a><span class="im">import</span> torch</span>
<span id="cb9-2"><a href=""></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset</span>
<span id="cb9-3"><a href=""></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb9-4"><a href=""></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-5"><a href=""></a></span>
<span id="cb9-6"><a href=""></a><span class="kw">class</span> MNISTNoise(Dataset):</span>
<span id="cb9-7"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, train<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb9-8"><a href=""></a>    <span class="va">self</span>.dataset <span class="op">=</span> load_dataset(<span class="st">"ylecun/mnist"</span>)</span>
<span id="cb9-9"><a href=""></a>    <span class="cf">if</span> train:</span>
<span id="cb9-10"><a href=""></a>      <span class="va">self</span>.dataset <span class="op">=</span> <span class="va">self</span>.dataset[<span class="st">"train"</span>]</span>
<span id="cb9-11"><a href=""></a>    <span class="cf">else</span>:</span>
<span id="cb9-12"><a href=""></a>      <span class="va">self</span>.dataset <span class="op">=</span> <span class="va">self</span>.dataset[<span class="st">"test"</span>]</span>
<span id="cb9-13"><a href=""></a></span>
<span id="cb9-14"><a href=""></a>  <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb9-15"><a href=""></a>    <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.dataset)</span>
<span id="cb9-16"><a href=""></a></span>
<span id="cb9-17"><a href=""></a>  <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb9-18"><a href=""></a>    target <span class="op">=</span> torch.from_numpy(np.asarray(<span class="va">self</span>.dataset[idx][<span class="st">"image"</span>], dtype<span class="op">=</span><span class="st">"int32"</span>) <span class="op">/</span> <span class="dv">255</span>).to(torch.<span class="bu">float</span>)</span>
<span id="cb9-19"><a href=""></a>    target <span class="op">=</span> target.view(<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb9-20"><a href=""></a></span>
<span id="cb9-21"><a href=""></a>    noise <span class="op">=</span> torch.zeros(<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb9-22"><a href=""></a>    image <span class="op">=</span> target <span class="op">+</span> (<span class="fl">0.1</span><span class="op">**</span><span class="fl">0.5</span>)<span class="op">*</span>torch.randn(<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb9-23"><a href=""></a></span>
<span id="cb9-24"><a href=""></a>    <span class="cf">return</span> image, target</span>
<span id="cb9-25"><a href=""></a></span>
<span id="cb9-26"><a href=""></a>train <span class="op">=</span> MNISTNoise(train<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-27"><a href=""></a>test <span class="op">=</span> MNISTNoise(train<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="denoising-images-2" class="slide level2">
<h2>Denoising Images</h2>
<p>Now we can train an autoencoder on this data, we will use the exact same architecture as we defined previously.</p>
<div id="377f7cc1" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="70bf2870-fb3c-4c0b-b884-811d665620e7" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href=""></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb10-2"><a href=""></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb10-3"><a href=""></a><span class="im">import</span> sys</span>
<span id="cb10-4"><a href=""></a></span>
<span id="cb10-5"><a href=""></a><span class="co"># Use GPU if available</span></span>
<span id="cb10-6"><a href=""></a>device <span class="op">=</span> torch.accelerator.current_accelerator().<span class="bu">type</span> <span class="cf">if</span> torch.accelerator.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb10-7"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss"> device"</span>)</span>
<span id="cb10-8"><a href=""></a></span>
<span id="cb10-9"><a href=""></a>model <span class="op">=</span> Autoencoder().to(device)</span>
<span id="cb10-10"><a href=""></a></span>
<span id="cb10-11"><a href=""></a><span class="co"># Some hyperparametes</span></span>
<span id="cb10-12"><a href=""></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb10-13"><a href=""></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb10-14"><a href=""></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb10-15"><a href=""></a></span>
<span id="cb10-16"><a href=""></a>loss_fn <span class="op">=</span> nn.MSELoss().to(device)</span>
<span id="cb10-17"><a href=""></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb10-18"><a href=""></a></span>
<span id="cb10-19"><a href=""></a>train_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb10-20"><a href=""></a>  train,</span>
<span id="cb10-21"><a href=""></a>  batch_size<span class="op">=</span>batch_size,</span>
<span id="cb10-22"><a href=""></a>  shuffle<span class="op">=</span><span class="va">True</span></span>
<span id="cb10-23"><a href=""></a>)</span>
<span id="cb10-24"><a href=""></a></span>
<span id="cb10-25"><a href=""></a>test_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb10-26"><a href=""></a>  test,</span>
<span id="cb10-27"><a href=""></a>  batch_size <span class="op">=</span> <span class="bu">len</span>(test)</span>
<span id="cb10-28"><a href=""></a>)</span>
<span id="cb10-29"><a href=""></a></span>
<span id="cb10-30"><a href=""></a><span class="kw">def</span> train_epoch(dataloader, model, loss_fn, optimizer):</span>
<span id="cb10-31"><a href=""></a>  model.train() <span class="co"># Set model to training mode</span></span>
<span id="cb10-32"><a href=""></a></span>
<span id="cb10-33"><a href=""></a>  total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-34"><a href=""></a>  total_batches <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-35"><a href=""></a></span>
<span id="cb10-36"><a href=""></a>  <span class="cf">with</span> tqdm(dataloader, unit<span class="op">=</span><span class="st">"batch"</span>, <span class="bu">file</span><span class="op">=</span>sys.stdout) <span class="im">as</span> ep_tqdm:</span>
<span id="cb10-37"><a href=""></a>    ep_tqdm.set_description(<span class="st">"Train"</span>)</span>
<span id="cb10-38"><a href=""></a>    <span class="cf">for</span> X, y <span class="kw">in</span> ep_tqdm:</span>
<span id="cb10-39"><a href=""></a>      X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb10-40"><a href=""></a></span>
<span id="cb10-41"><a href=""></a>      <span class="co"># Forward pass</span></span>
<span id="cb10-42"><a href=""></a>      pred <span class="op">=</span> model(X)</span>
<span id="cb10-43"><a href=""></a>      loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb10-44"><a href=""></a>        </span>
<span id="cb10-45"><a href=""></a>      <span class="co"># Backward pass</span></span>
<span id="cb10-46"><a href=""></a>      loss.backward()</span>
<span id="cb10-47"><a href=""></a>      optimizer.step()</span>
<span id="cb10-48"><a href=""></a></span>
<span id="cb10-49"><a href=""></a>      <span class="co"># Reset the computed gradients back to zero</span></span>
<span id="cb10-50"><a href=""></a>      optimizer.zero_grad()</span>
<span id="cb10-51"><a href=""></a></span>
<span id="cb10-52"><a href=""></a>      <span class="co"># Output stats</span></span>
<span id="cb10-53"><a href=""></a>      total_loss <span class="op">+=</span> loss</span>
<span id="cb10-54"><a href=""></a>      total_batches <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb10-55"><a href=""></a>      ep_tqdm.set_postfix(average_batch_loss<span class="op">=</span>(total_loss<span class="op">/</span>total_batches).item())</span>
<span id="cb10-56"><a href=""></a></span>
<span id="cb10-57"><a href=""></a><span class="kw">def</span> eval_epoch(dataloader, model, loss_fn):</span>
<span id="cb10-58"><a href=""></a>  model.<span class="bu">eval</span>() <span class="co"># Set model to inference mode</span></span>
<span id="cb10-59"><a href=""></a>  </span>
<span id="cb10-60"><a href=""></a>  total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-61"><a href=""></a>  total_batches <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-62"><a href=""></a>  total_samples <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-63"><a href=""></a>  total_correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-64"><a href=""></a></span>
<span id="cb10-65"><a href=""></a>  <span class="cf">with</span> torch.no_grad(): <span class="co"># Do not compute gradients</span></span>
<span id="cb10-66"><a href=""></a>    <span class="cf">with</span> tqdm(dataloader, unit<span class="op">=</span><span class="st">"batch"</span>, <span class="bu">file</span><span class="op">=</span>sys.stdout) <span class="im">as</span> ep_tqdm:</span>
<span id="cb10-67"><a href=""></a>      ep_tqdm.set_description(<span class="st">"Val"</span>)</span>
<span id="cb10-68"><a href=""></a>      <span class="cf">for</span> X, y <span class="kw">in</span> ep_tqdm:</span>
<span id="cb10-69"><a href=""></a>        X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb10-70"><a href=""></a>        pred <span class="op">=</span> model(X)</span>
<span id="cb10-71"><a href=""></a></span>
<span id="cb10-72"><a href=""></a>        total_loss <span class="op">+=</span> loss_fn(pred, y)</span>
<span id="cb10-73"><a href=""></a>        total_batches <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb10-74"><a href=""></a></span>
<span id="cb10-75"><a href=""></a>        ep_tqdm.set_postfix(average_batch_loss<span class="op">=</span>(total_loss<span class="op">/</span>total_batches).item())</span>
<span id="cb10-76"><a href=""></a></span>
<span id="cb10-77"><a href=""></a><span class="co"># Organize the training loop</span></span>
<span id="cb10-78"><a href=""></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb10-79"><a href=""></a>  <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>t<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">-------------------------------"</span>)</span>
<span id="cb10-80"><a href=""></a>  train_epoch(train_dataloader, model, loss_fn, optimizer)</span>
<span id="cb10-81"><a href=""></a>  eval_epoch(test_dataloader, model, loss_fn)</span>
<span id="cb10-82"><a href=""></a></span>
<span id="cb10-83"><a href=""></a><span class="bu">print</span>(<span class="st">"Done!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>

</section>
<section id="denoising-images-2-output" class="slide level2 output-location-slide"><h2>Denoising Images</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="70bf2870-fb3c-4c0b-b884-811d665620e7" data-execution_count="8">
<div class="cell-output cell-output-stdout">
<pre><code>Using cuda device
Epoch 1
-------------------------------
Train: 100%|██████████| 469/469 [00:06&lt;00:00, 69.73batch/s, average_batch_loss=0.0436]
Val: 100%|██████████| 1/1 [00:01&lt;00:00,  1.09s/batch, average_batch_loss=0.0215]
Epoch 2
-------------------------------
Train: 100%|██████████| 469/469 [00:06&lt;00:00, 68.60batch/s, average_batch_loss=0.0175]
Val: 100%|██████████| 1/1 [00:00&lt;00:00,  1.04batch/s, average_batch_loss=0.0143]
Epoch 3
-------------------------------
Train: 100%|██████████| 469/469 [00:06&lt;00:00, 71.03batch/s, average_batch_loss=0.0133]
Val: 100%|██████████| 1/1 [00:00&lt;00:00,  1.03batch/s, average_batch_loss=0.0119]
Epoch 4
-------------------------------
Train: 100%|██████████| 469/469 [00:06&lt;00:00, 72.02batch/s, average_batch_loss=0.0116]
Val: 100%|██████████| 1/1 [00:00&lt;00:00,  1.04batch/s, average_batch_loss=0.0109]
Epoch 5
-------------------------------
Train: 100%|██████████| 469/469 [00:06&lt;00:00, 71.95batch/s, average_batch_loss=0.0107]
Val: 100%|██████████| 1/1 [00:01&lt;00:00,  1.04s/batch, average_batch_loss=0.0101]
Epoch 6
-------------------------------
Train: 100%|██████████| 469/469 [00:06&lt;00:00, 71.78batch/s, average_batch_loss=0.0101]
Val: 100%|██████████| 1/1 [00:00&lt;00:00,  1.04batch/s, average_batch_loss=0.00961]
Epoch 7
-------------------------------
Train: 100%|██████████| 469/469 [00:06&lt;00:00, 71.93batch/s, average_batch_loss=0.00968]
Val: 100%|██████████| 1/1 [00:01&lt;00:00,  1.05s/batch, average_batch_loss=0.00934]
Epoch 8
-------------------------------
Train: 100%|██████████| 469/469 [00:06&lt;00:00, 72.10batch/s, average_batch_loss=0.00936]
Val: 100%|██████████| 1/1 [00:00&lt;00:00,  1.04batch/s, average_batch_loss=0.00902]
Epoch 9
-------------------------------
Train: 100%|██████████| 469/469 [00:06&lt;00:00, 72.08batch/s, average_batch_loss=0.00914]
Val: 100%|██████████| 1/1 [00:00&lt;00:00,  1.04batch/s, average_batch_loss=0.00887]
Epoch 10
-------------------------------
Train: 100%|██████████| 469/469 [00:06&lt;00:00, 71.29batch/s, average_batch_loss=0.00895]
Val: 100%|██████████| 1/1 [00:00&lt;00:00,  1.04batch/s, average_batch_loss=0.00866]
Done!</code></pre>
</div>
</div></section><section id="denoising-images-3" class="slide level2">
<h2>Denoising Images</h2>
<p>Let’s display and see if it works.</p>
<div id="4ca9a937" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:111}}" data-outputid="35fd2e49-10dc-4d08-c690-ec661970f3dc" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href=""></a>test_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb12-2"><a href=""></a>  test,</span>
<span id="cb12-3"><a href=""></a>  batch_size <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb12-4"><a href=""></a>)</span>
<span id="cb12-5"><a href=""></a></span>
<span id="cb12-6"><a href=""></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-7"><a href=""></a>  X, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(test_dataloader))</span>
<span id="cb12-8"><a href=""></a>  X <span class="op">=</span> X.to(device)</span>
<span id="cb12-9"><a href=""></a>  pred <span class="op">=</span> model(X).cpu()</span>
<span id="cb12-10"><a href=""></a>  X <span class="op">=</span> X.cpu()</span>
<span id="cb12-11"><a href=""></a>  show(torch.concat([X, pred, y], axis<span class="op">=</span><span class="dv">2</span>).transpose(<span class="dv">2</span>, <span class="dv">3</span>).reshape(<span class="dv">1</span>, <span class="dv">20</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">3</span><span class="op">*</span><span class="dv">28</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>

</div>
<img data-src="09_autoencoders_and_ddpm_files/figure-revealjs/cell-10-output-1.png" class="r-stretch"></section>
<section id="ddpm" class="slide level2">
<h2>DDPM</h2>
<p>We can take this denoising idea much further and generate images from random noise.</p>
<p>Here we will cover denoising diffusion probabilistic models (DDPM) as defined in this <a href="https://arxiv.org/abs/2006.11239">paper</a>. DDPM is the idea behind generative models such as DALLE-2 and Stable Diffusion.</p>
<p>The goal is to train a model capable of generating images that are similar to the ones seen during training.</p>
</section>
<section id="ddpm-1" class="slide level2">
<h2>DDPM</h2>
<p>Suppose <span class="math inline">\(x_0\)</span> is a vector representing an image in the training set. We first add gaussian noise to it in <span class="math inline">\(T\)</span> steps using the formula <span class="math display">\[
  x_t = \sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t} \varepsilon,
\]</span> where <span class="math inline">\(\varepsilon\)</span> is drawn from the standard normal distribution (so gaussian with mean zero and variance 1) and <span class="math inline">\(\beta_1, \dots, \beta_T\)</span> are parameters called the variance schedule. In the original paper the <span class="math inline">\(\beta_t\)</span> were taken to vary linearly from 0.0001 to 0.02 so we will do the same.</p>
</section>
<section id="ddpm-2" class="slide level2">
<h2>DDPM</h2>
<p>We can get <span class="math inline">\(x_t\)</span> in one formula straight from <span class="math inline">\(x_0.\)</span> Denote <span class="math inline">\(\alpha_t=1-\beta_t\)</span> and <span class="math inline">\(\overline{\alpha}_t = \prod_{s=1}^t \alpha_s.\)</span></p>
<p>Then iteratively applying the previous formula and using the fact that a linear combination of gaussians is again a gaussian we get <span class="math display">\[
  x_t = \sqrt{\overline{\alpha}_t}x_0 + \sqrt{1-\overline{\alpha}_t}\varepsilon.
\]</span></p>
</section>
<section id="ddpm-3" class="slide level2">
<h2>DDPM</h2>
<p>Now the idea of DDPM is to train a model that is able to reverse this “noisefication” process. I.e. we want a model <span class="math inline">\(p\)</span> that is able to predict <span class="math inline">\(x_{t-1}\)</span> given <span class="math inline">\(x_t.\)</span></p>
<p>Actually, if you shuffle the math around you’ll see that it is enough to predict the noise that was added during step <span class="math inline">\(t\)</span>.</p>
</section>
<section id="ddpm-4" class="slide level2">
<h2>DDPM</h2>
<p>The algorithm for training such a model and generating an image is a follows (taken from the original paper):</p>

<img data-src="../images/ddpm.png" class="quarto-figure quarto-figure-center r-stretch"><p>Note that what we’ve given here is an intuitive introduction to DDPM, if you want to fully understand the math behind it you’ll have to read the paper. Also, <span class="math inline">\(\sigma_t = \sqrt{\beta_t}\)</span> in our case.</p>
</section>
<section id="ddpm-5" class="slide level2">
<h2>DDPM</h2>
<p>Let’s implement DDPM, we’ll train it on MNIST.</p>
<div id="2d47fcc2" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href=""></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href=""></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset</span>
<span id="cb13-3"><a href=""></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset, concatenate_datasets</span>
<span id="cb13-4"><a href=""></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-5"><a href=""></a></span>
<span id="cb13-6"><a href=""></a><span class="kw">class</span> MNIST(Dataset):</span>
<span id="cb13-7"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb13-8"><a href=""></a>    <span class="va">self</span>.dataset <span class="op">=</span> load_dataset(<span class="st">"ylecun/mnist"</span>)</span>
<span id="cb13-9"><a href=""></a>    <span class="va">self</span>.dataset <span class="op">=</span> concatenate_datasets([<span class="va">self</span>.dataset[<span class="st">"train"</span>], <span class="va">self</span>.dataset[<span class="st">"test"</span>]])</span>
<span id="cb13-10"><a href=""></a></span>
<span id="cb13-11"><a href=""></a>  <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb13-12"><a href=""></a>    <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.dataset)</span>
<span id="cb13-13"><a href=""></a></span>
<span id="cb13-14"><a href=""></a>  <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb13-15"><a href=""></a>    image <span class="op">=</span> torch.from_numpy(np.asarray(<span class="va">self</span>.dataset[idx][<span class="st">"image"</span>], dtype<span class="op">=</span><span class="st">"int32"</span>) <span class="op">/</span> <span class="dv">255</span>).to(torch.<span class="bu">float</span>)</span>
<span id="cb13-16"><a href=""></a>    image <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>image<span class="op">-</span><span class="dv">1</span></span>
<span id="cb13-17"><a href=""></a>    image <span class="op">=</span> image.view(<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb13-18"><a href=""></a></span>
<span id="cb13-19"><a href=""></a>    <span class="cf">return</span> image</span>
<span id="cb13-20"><a href=""></a></span>
<span id="cb13-21"><a href=""></a>data <span class="op">=</span> MNIST()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="ddpm-6" class="slide level2">
<h2>DDPM</h2>
<p>Now let’s define a class that is going to add noise to images, predict the added noise and generate images.</p>
<div id="1556071f" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href=""></a><span class="kw">class</span> DDPM():</span>
<span id="cb14-2"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, image_dim, device, no_steps<span class="op">=</span><span class="dv">300</span>, min_beta<span class="op">=</span><span class="fl">0.0001</span>, max_beta<span class="op">=</span><span class="fl">0.02</span>):</span>
<span id="cb14-3"><a href=""></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-4"><a href=""></a>    <span class="va">self</span>.no_steps <span class="op">=</span> no_steps</span>
<span id="cb14-5"><a href=""></a>    <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb14-6"><a href=""></a>    <span class="va">self</span>.image_dim <span class="op">=</span> image_dim</span>
<span id="cb14-7"><a href=""></a>    <span class="va">self</span>.device <span class="op">=</span> device</span>
<span id="cb14-8"><a href=""></a>    <span class="va">self</span>.betas <span class="op">=</span> torch.linspace(min_beta, max_beta, no_steps).to(<span class="va">self</span>.device)</span>
<span id="cb14-9"><a href=""></a>    <span class="va">self</span>.alphas <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.betas</span>
<span id="cb14-10"><a href=""></a>    <span class="va">self</span>.alpha_bars <span class="op">=</span> torch.tensor([torch.prod(<span class="va">self</span>.alphas[:i <span class="op">+</span> <span class="dv">1</span>]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.alphas))]).to(<span class="va">self</span>.device)</span>
<span id="cb14-11"><a href=""></a></span>
<span id="cb14-12"><a href=""></a>  <span class="kw">def</span> add_noise(<span class="va">self</span>, x0, t, noise<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb14-13"><a href=""></a>    a_bar <span class="op">=</span> <span class="va">self</span>.alpha_bars[t]</span>
<span id="cb14-14"><a href=""></a></span>
<span id="cb14-15"><a href=""></a>    <span class="cf">if</span> noise <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb14-16"><a href=""></a>      noise <span class="op">=</span> torch.randn(x0.shape).to(<span class="va">self</span>.device)</span>
<span id="cb14-17"><a href=""></a></span>
<span id="cb14-18"><a href=""></a>    <span class="cf">return</span> a_bar.sqrt().reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>) <span class="op">*</span> x0 <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> a_bar).sqrt().reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>) <span class="op">*</span> noise</span>
<span id="cb14-19"><a href=""></a></span>
<span id="cb14-20"><a href=""></a>  <span class="kw">def</span> predict_noise(<span class="va">self</span>, x, t):</span>
<span id="cb14-21"><a href=""></a>    <span class="cf">return</span> <span class="va">self</span>.model(x, t)</span>
<span id="cb14-22"><a href=""></a></span>
<span id="cb14-23"><a href=""></a>  <span class="kw">def</span> sample(<span class="va">self</span>, n_samples<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb14-24"><a href=""></a>    <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb14-25"><a href=""></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb14-26"><a href=""></a>      c, h, w <span class="op">=</span> <span class="va">self</span>.image_dim</span>
<span id="cb14-27"><a href=""></a>      x <span class="op">=</span> torch.randn(n_samples, c, h, w).to(<span class="va">self</span>.device)</span>
<span id="cb14-28"><a href=""></a></span>
<span id="cb14-29"><a href=""></a>      <span class="cf">for</span> idx, t <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">list</span>(<span class="bu">range</span>(ddpm.no_steps))[::<span class="op">-</span><span class="dv">1</span>]):</span>
<span id="cb14-30"><a href=""></a>        t_tensor <span class="op">=</span> (torch.ones(n_samples, <span class="dv">1</span>) <span class="op">*</span> t).to(<span class="va">self</span>.device).<span class="bu">long</span>()</span>
<span id="cb14-31"><a href=""></a>        noise <span class="op">=</span> <span class="va">self</span>.predict_noise(x, t_tensor)</span>
<span id="cb14-32"><a href=""></a></span>
<span id="cb14-33"><a href=""></a>        <span class="co"># Partially denoising the image</span></span>
<span id="cb14-34"><a href=""></a>        x <span class="op">=</span> (<span class="dv">1</span> <span class="op">/</span> <span class="va">self</span>.alphas[t].sqrt()) <span class="op">*</span> (x <span class="op">-</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.alphas[t]) <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.alpha_bars[t]).sqrt() <span class="op">*</span> noise)</span>
<span id="cb14-35"><a href=""></a></span>
<span id="cb14-36"><a href=""></a>        <span class="cf">if</span> t <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb14-37"><a href=""></a>          x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.betas[t].sqrt() <span class="op">*</span> torch.randn(n_samples, c, h, w).to(<span class="va">self</span>.device)</span>
<span id="cb14-38"><a href=""></a></span>
<span id="cb14-39"><a href=""></a>      <span class="co"># Normalizing the image</span></span>
<span id="cb14-40"><a href=""></a>      <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x)):</span>
<span id="cb14-41"><a href=""></a>        x[i] <span class="op">-=</span> torch.<span class="bu">min</span>(x[i])</span>
<span id="cb14-42"><a href=""></a>        x[i] <span class="op">*=</span> <span class="dv">255</span> <span class="op">/</span> torch.<span class="bu">max</span>(x[i])</span>
<span id="cb14-43"><a href=""></a></span>
<span id="cb14-44"><a href=""></a>      <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="ddpm-7" class="slide level2">
<h2>DDPM</h2>
<p>Now we need some sort of model to predict the added noise. The only hard requirement is that the input shape would be the same as the output shape.</p>
<p>The original paper used a U-Net, so let’s use it as well. U-Nets are convolutional neural networks that look like this:</p>

<img data-src="../images/unet.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="ddpm-8" class="slide level2">
<h2>DDPM</h2>
<p>We also need to tell the network on which denoising step we are on. In the original paper sinusoidal embeddings were used so let’s use them as well.</p>
<p>Let <span class="math inline">\(d\)</span> denote the embedding dimension and <span class="math inline">\(t\)</span> the denoising step, then sinusoidal embedding is defined as follows: <span class="math display">\[
  \text{SinusoidalEmbedding}_{t, 2i} = \sin \left(\frac{t}{10000^{2i/d}}\right),
\]</span> <span class="math display">\[
  \text{SinusoidalEmbedding}_{t, 2i+} = \cos \left(\frac{t}{10000^{2i/d}}\right).
\]</span></p>
</section>
<section id="ddpm-9" class="slide level2">
<h2>DDPM</h2>
<p>Let’s build our U-net.</p>
<div id="e8947441" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e8eb7188-c7d8-4fef-80b9-f1dbe2e35c91" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href=""></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb15-2"><a href=""></a></span>
<span id="cb15-3"><a href=""></a><span class="kw">class</span> UNet(nn.Module):</span>
<span id="cb15-4"><a href=""></a>  <span class="kw">def</span> _make_fc(<span class="va">self</span>, dim_in, dim_out):</span>
<span id="cb15-5"><a href=""></a>    <span class="cf">return</span> nn.Sequential(</span>
<span id="cb15-6"><a href=""></a>      nn.Linear(dim_in, dim_out),</span>
<span id="cb15-7"><a href=""></a>      nn.ReLU(),</span>
<span id="cb15-8"><a href=""></a>      nn.Linear(dim_out, dim_out)</span>
<span id="cb15-9"><a href=""></a>    )</span>
<span id="cb15-10"><a href=""></a></span>
<span id="cb15-11"><a href=""></a>  <span class="kw">def</span> _make_conv(<span class="va">self</span>, in_channels, out_channels, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>)):</span>
<span id="cb15-12"><a href=""></a>    <span class="cf">return</span> nn.Sequential(</span>
<span id="cb15-13"><a href=""></a>      nn.BatchNorm2d(in_channels),</span>
<span id="cb15-14"><a href=""></a>      nn.Conv2d(</span>
<span id="cb15-15"><a href=""></a>        in_channels<span class="op">=</span>in_channels,</span>
<span id="cb15-16"><a href=""></a>        out_channels<span class="op">=</span>out_channels,</span>
<span id="cb15-17"><a href=""></a>        kernel_size<span class="op">=</span>kernel_size,</span>
<span id="cb15-18"><a href=""></a>        padding<span class="op">=</span><span class="dv">1</span></span>
<span id="cb15-19"><a href=""></a>      ),</span>
<span id="cb15-20"><a href=""></a>      nn.ReLU(),</span>
<span id="cb15-21"><a href=""></a>      nn.BatchNorm2d(out_channels),</span>
<span id="cb15-22"><a href=""></a>      nn.Conv2d(</span>
<span id="cb15-23"><a href=""></a>        in_channels<span class="op">=</span>out_channels,</span>
<span id="cb15-24"><a href=""></a>        out_channels<span class="op">=</span>out_channels,</span>
<span id="cb15-25"><a href=""></a>        kernel_size<span class="op">=</span>kernel_size,</span>
<span id="cb15-26"><a href=""></a>        padding<span class="op">=</span><span class="dv">1</span></span>
<span id="cb15-27"><a href=""></a>      ),</span>
<span id="cb15-28"><a href=""></a>      nn.ReLU(),</span>
<span id="cb15-29"><a href=""></a>    )</span>
<span id="cb15-30"><a href=""></a></span>
<span id="cb15-31"><a href=""></a>  <span class="kw">def</span> _sinusoidal_embedding(<span class="va">self</span>, n, d):</span>
<span id="cb15-32"><a href=""></a>    embedding <span class="op">=</span> torch.zeros(n, d)</span>
<span id="cb15-33"><a href=""></a>    wk <span class="op">=</span> torch.tensor([<span class="dv">1</span> <span class="op">/</span> <span class="dv">10_000</span> <span class="op">**</span> (<span class="dv">2</span> <span class="op">*</span> j <span class="op">/</span> d) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(d)])</span>
<span id="cb15-34"><a href=""></a>    wk <span class="op">=</span> wk.reshape((<span class="dv">1</span>, d))</span>
<span id="cb15-35"><a href=""></a>    t <span class="op">=</span> torch.arange(n).reshape((n, <span class="dv">1</span>))</span>
<span id="cb15-36"><a href=""></a>    embedding[:,::<span class="dv">2</span>] <span class="op">=</span> torch.sin(t <span class="op">*</span> wk[:,::<span class="dv">2</span>])</span>
<span id="cb15-37"><a href=""></a>    embedding[:,<span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(t <span class="op">*</span> wk[:,::<span class="dv">2</span>])</span>
<span id="cb15-38"><a href=""></a></span>
<span id="cb15-39"><a href=""></a>    <span class="cf">return</span> embedding</span>
<span id="cb15-40"><a href=""></a></span>
<span id="cb15-41"><a href=""></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_steps<span class="op">=</span><span class="dv">1000</span>, embed_dim<span class="op">=</span><span class="dv">128</span>):</span>
<span id="cb15-42"><a href=""></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb15-43"><a href=""></a></span>
<span id="cb15-44"><a href=""></a>    <span class="va">self</span>.time_embedding <span class="op">=</span> nn.Embedding.from_pretrained(<span class="va">self</span>._sinusoidal_embedding(n_steps, embed_dim), freeze<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-45"><a href=""></a></span>
<span id="cb15-46"><a href=""></a>    <span class="co"># Down</span></span>
<span id="cb15-47"><a href=""></a>    <span class="va">self</span>.time_embed1 <span class="op">=</span> <span class="va">self</span>._make_fc(embed_dim, <span class="dv">1</span>)</span>
<span id="cb15-48"><a href=""></a>    <span class="va">self</span>.block1 <span class="op">=</span> nn.Sequential(</span>
<span id="cb15-49"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-50"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb15-51"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb15-52"><a href=""></a>      ),</span>
<span id="cb15-53"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-54"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb15-55"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb15-56"><a href=""></a>      ),</span>
<span id="cb15-57"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-58"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb15-59"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb15-60"><a href=""></a>      ),</span>
<span id="cb15-61"><a href=""></a>    )</span>
<span id="cb15-62"><a href=""></a>    <span class="va">self</span>.down1 <span class="op">=</span> nn.Conv2d(</span>
<span id="cb15-63"><a href=""></a>      in_channels<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb15-64"><a href=""></a>      out_channels<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb15-65"><a href=""></a>      kernel_size<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>),</span>
<span id="cb15-66"><a href=""></a>      stride<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb15-67"><a href=""></a>      padding<span class="op">=</span><span class="dv">1</span></span>
<span id="cb15-68"><a href=""></a>    )</span>
<span id="cb15-69"><a href=""></a></span>
<span id="cb15-70"><a href=""></a>    <span class="va">self</span>.time_embed2 <span class="op">=</span> <span class="va">self</span>._make_fc(embed_dim, <span class="dv">32</span>)</span>
<span id="cb15-71"><a href=""></a>    <span class="va">self</span>.block2 <span class="op">=</span> nn.Sequential(</span>
<span id="cb15-72"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-73"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb15-74"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-75"><a href=""></a>      ),</span>
<span id="cb15-76"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-77"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-78"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-79"><a href=""></a>      ),</span>
<span id="cb15-80"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-81"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-82"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-83"><a href=""></a>      ),</span>
<span id="cb15-84"><a href=""></a>    )</span>
<span id="cb15-85"><a href=""></a>    <span class="va">self</span>.down2 <span class="op">=</span> nn.Conv2d(</span>
<span id="cb15-86"><a href=""></a>      in_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-87"><a href=""></a>      out_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-88"><a href=""></a>      kernel_size<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>),</span>
<span id="cb15-89"><a href=""></a>      stride<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb15-90"><a href=""></a>      padding<span class="op">=</span><span class="dv">1</span></span>
<span id="cb15-91"><a href=""></a>    )</span>
<span id="cb15-92"><a href=""></a></span>
<span id="cb15-93"><a href=""></a>    <span class="va">self</span>.time_embed3 <span class="op">=</span> <span class="va">self</span>._make_fc(embed_dim, <span class="dv">64</span>)</span>
<span id="cb15-94"><a href=""></a>    <span class="va">self</span>.block3 <span class="op">=</span> nn.Sequential(</span>
<span id="cb15-95"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-96"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-97"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb15-98"><a href=""></a>      ),</span>
<span id="cb15-99"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-100"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb15-101"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb15-102"><a href=""></a>      ),</span>
<span id="cb15-103"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-104"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb15-105"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb15-106"><a href=""></a>      ),</span>
<span id="cb15-107"><a href=""></a>    )</span>
<span id="cb15-108"><a href=""></a></span>
<span id="cb15-109"><a href=""></a>    <span class="co"># Up</span></span>
<span id="cb15-110"><a href=""></a>    <span class="va">self</span>.up1 <span class="op">=</span> nn.ConvTranspose2d(</span>
<span id="cb15-111"><a href=""></a>      in_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb15-112"><a href=""></a>      out_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb15-113"><a href=""></a>      kernel_size<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>),</span>
<span id="cb15-114"><a href=""></a>      stride<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb15-115"><a href=""></a>      padding<span class="op">=</span><span class="dv">1</span></span>
<span id="cb15-116"><a href=""></a>    )</span>
<span id="cb15-117"><a href=""></a>    <span class="va">self</span>.up_time_embed1 <span class="op">=</span> <span class="va">self</span>._make_fc(embed_dim, <span class="dv">192</span>)</span>
<span id="cb15-118"><a href=""></a>    <span class="va">self</span>.up_block1 <span class="op">=</span> nn.Sequential(</span>
<span id="cb15-119"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-120"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">192</span>,</span>
<span id="cb15-121"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb15-122"><a href=""></a>      ),</span>
<span id="cb15-123"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-124"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb15-125"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb15-126"><a href=""></a>      ),</span>
<span id="cb15-127"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-128"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb15-129"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-130"><a href=""></a>      ),</span>
<span id="cb15-131"><a href=""></a>    )</span>
<span id="cb15-132"><a href=""></a></span>
<span id="cb15-133"><a href=""></a>    <span class="va">self</span>.up2 <span class="op">=</span> nn.ConvTranspose2d(</span>
<span id="cb15-134"><a href=""></a>      in_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-135"><a href=""></a>      out_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-136"><a href=""></a>      kernel_size<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>),</span>
<span id="cb15-137"><a href=""></a>      stride<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb15-138"><a href=""></a>      padding<span class="op">=</span><span class="dv">1</span></span>
<span id="cb15-139"><a href=""></a>    )</span>
<span id="cb15-140"><a href=""></a>    <span class="va">self</span>.up_time_embed2 <span class="op">=</span> <span class="va">self</span>._make_fc(embed_dim, <span class="dv">96</span>)</span>
<span id="cb15-141"><a href=""></a>    <span class="va">self</span>.up_block2 <span class="op">=</span> nn.Sequential(</span>
<span id="cb15-142"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-143"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">96</span>,</span>
<span id="cb15-144"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-145"><a href=""></a>      ),</span>
<span id="cb15-146"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-147"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-148"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-149"><a href=""></a>      ),</span>
<span id="cb15-150"><a href=""></a>      <span class="va">self</span>._make_conv(</span>
<span id="cb15-151"><a href=""></a>        in_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb15-152"><a href=""></a>        out_channels<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb15-153"><a href=""></a>      ),</span>
<span id="cb15-154"><a href=""></a>    )</span>
<span id="cb15-155"><a href=""></a></span>
<span id="cb15-156"><a href=""></a>    <span class="va">self</span>.out_conv <span class="op">=</span> nn.Conv2d(</span>
<span id="cb15-157"><a href=""></a>      in_channels<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb15-158"><a href=""></a>      out_channels<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb15-159"><a href=""></a>      kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb15-160"><a href=""></a>      padding<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb15-161"><a href=""></a>    )</span>
<span id="cb15-162"><a href=""></a></span>
<span id="cb15-163"><a href=""></a>  <span class="kw">def</span> forward(<span class="va">self</span>, x, t):</span>
<span id="cb15-164"><a href=""></a>    t <span class="op">=</span> <span class="va">self</span>.time_embedding(t)</span>
<span id="cb15-165"><a href=""></a>    b <span class="op">=</span> <span class="bu">len</span>(x)</span>
<span id="cb15-166"><a href=""></a></span>
<span id="cb15-167"><a href=""></a>    down1 <span class="op">=</span> <span class="va">self</span>.block1(x <span class="op">+</span> <span class="va">self</span>.time_embed1(t).reshape(b, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb15-168"><a href=""></a>    down2 <span class="op">=</span> <span class="va">self</span>.block2(<span class="va">self</span>.down1(down1) <span class="op">+</span> <span class="va">self</span>.time_embed2(t).reshape(b, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb15-169"><a href=""></a>    down3 <span class="op">=</span> <span class="va">self</span>.block3(<span class="va">self</span>.down2(down2) <span class="op">+</span> <span class="va">self</span>.time_embed3(t).reshape(b, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb15-170"><a href=""></a></span>
<span id="cb15-171"><a href=""></a>    up1 <span class="op">=</span> <span class="va">self</span>.up_block1(</span>
<span id="cb15-172"><a href=""></a>      torch.cat([down2, <span class="va">self</span>.up1(down3)], dim<span class="op">=</span><span class="dv">1</span>) <span class="op">+</span> <span class="va">self</span>.up_time_embed1(t).reshape(b, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb15-173"><a href=""></a>    )</span>
<span id="cb15-174"><a href=""></a>    up2 <span class="op">=</span> <span class="va">self</span>.up_block2(</span>
<span id="cb15-175"><a href=""></a>      torch.cat([down1, <span class="va">self</span>.up2(up1)], dim<span class="op">=</span><span class="dv">1</span>) <span class="op">+</span> <span class="va">self</span>.up_time_embed2(t).reshape(b, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb15-176"><a href=""></a>    )</span>
<span id="cb15-177"><a href=""></a></span>
<span id="cb15-178"><a href=""></a>    <span class="cf">return</span> <span class="va">self</span>.out_conv(up2)</span>
<span id="cb15-179"><a href=""></a></span>
<span id="cb15-180"><a href=""></a><span class="bu">print</span>(UNet())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>

</section>
<section id="ddpm-9-output" class="slide level2 output-location-slide"><h2>DDPM</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e8eb7188-c7d8-4fef-80b9-f1dbe2e35c91" data-execution_count="12">
<div class="cell-output cell-output-stdout">
<pre><code>UNet(
  (time_embedding): Embedding(1000, 128)
  (time_embed1): Sequential(
    (0): Linear(in_features=128, out_features=1, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1, out_features=1, bias=True)
  )
  (block1): Sequential(
    (0): Sequential(
      (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
    (1): Sequential(
      (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
    (2): Sequential(
      (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
  )
  (down1): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (time_embed2): Sequential(
    (0): Linear(in_features=128, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=32, bias=True)
  )
  (block2): Sequential(
    (0): Sequential(
      (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
    (1): Sequential(
      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
    (2): Sequential(
      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
  )
  (down2): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (time_embed3): Sequential(
    (0): Linear(in_features=128, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (block3): Sequential(
    (0): Sequential(
      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
    (1): Sequential(
      (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
    (2): Sequential(
      (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
  )
  (up1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (up_time_embed1): Sequential(
    (0): Linear(in_features=128, out_features=192, bias=True)
    (1): ReLU()
    (2): Linear(in_features=192, out_features=192, bias=True)
  )
  (up_block1): Sequential(
    (0): Sequential(
      (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
    (1): Sequential(
      (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
    (2): Sequential(
      (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
  )
  (up2): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (up_time_embed2): Sequential(
    (0): Linear(in_features=128, out_features=96, bias=True)
    (1): ReLU()
    (2): Linear(in_features=96, out_features=96, bias=True)
  )
  (up_block2): Sequential(
    (0): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
    (1): Sequential(
      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
    (2): Sequential(
      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU()
      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
    )
  )
  (out_conv): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)</code></pre>
</div>
</div></section><section id="ddpm-10" class="slide level2">
<h2>DDPM</h2>
<p>We can now train the model.</p>
<div id="cd29841a" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="5ebaad75-3abe-46d0-d1eb-71524c3d5b4f" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href=""></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb17-2"><a href=""></a></span>
<span id="cb17-3"><a href=""></a><span class="co"># Hyperparameters</span></span>
<span id="cb17-4"><a href=""></a>learning_rate <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb17-5"><a href=""></a>epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb17-6"><a href=""></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb17-7"><a href=""></a>no_steps <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb17-8"><a href=""></a></span>
<span id="cb17-9"><a href=""></a>device <span class="op">=</span> torch.accelerator.current_accelerator().<span class="bu">type</span> <span class="cf">if</span> torch.accelerator.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb17-10"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss"> device"</span>)</span>
<span id="cb17-11"><a href=""></a></span>
<span id="cb17-12"><a href=""></a>model <span class="op">=</span> UNet(n_steps<span class="op">=</span>no_steps).to(device)</span>
<span id="cb17-13"><a href=""></a>ddpm <span class="op">=</span> DDPM(</span>
<span id="cb17-14"><a href=""></a>  model <span class="op">=</span> model,</span>
<span id="cb17-15"><a href=""></a>  image_dim <span class="op">=</span> (<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>),</span>
<span id="cb17-16"><a href=""></a>  device <span class="op">=</span> device,</span>
<span id="cb17-17"><a href=""></a>  no_steps <span class="op">=</span> no_steps</span>
<span id="cb17-18"><a href=""></a>)</span>
<span id="cb17-19"><a href=""></a></span>
<span id="cb17-20"><a href=""></a>dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb17-21"><a href=""></a>  data,</span>
<span id="cb17-22"><a href=""></a>  batch_size<span class="op">=</span>batch_size,</span>
<span id="cb17-23"><a href=""></a>  shuffle<span class="op">=</span><span class="va">True</span></span>
<span id="cb17-24"><a href=""></a>)</span>
<span id="cb17-25"><a href=""></a></span>
<span id="cb17-26"><a href=""></a>loss_fn <span class="op">=</span> nn.MSELoss().to(device)</span>
<span id="cb17-27"><a href=""></a>optimizer <span class="op">=</span> torch.optim.Adam(params<span class="op">=</span>model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb17-28"><a href=""></a></span>
<span id="cb17-29"><a href=""></a><span class="kw">def</span> train_epoch(dataloader, model, loss_fn, optimizer):</span>
<span id="cb17-30"><a href=""></a>  model.train() <span class="co"># Set model to training mode</span></span>
<span id="cb17-31"><a href=""></a></span>
<span id="cb17-32"><a href=""></a>  total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb17-33"><a href=""></a>  total_batches <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb17-34"><a href=""></a></span>
<span id="cb17-35"><a href=""></a>  <span class="cf">with</span> tqdm(dataloader, unit<span class="op">=</span><span class="st">"batch"</span>, <span class="bu">file</span><span class="op">=</span>sys.stdout) <span class="im">as</span> ep_tqdm:</span>
<span id="cb17-36"><a href=""></a>    ep_tqdm.set_description(<span class="st">"Train"</span>)</span>
<span id="cb17-37"><a href=""></a>    <span class="cf">for</span> X <span class="kw">in</span> ep_tqdm:</span>
<span id="cb17-38"><a href=""></a>      X <span class="op">=</span> X.to(device)</span>
<span id="cb17-39"><a href=""></a>      noise <span class="op">=</span> torch.randn(X.shape).to(device)</span>
<span id="cb17-40"><a href=""></a>      t <span class="op">=</span> torch.randint(<span class="dv">0</span>, no_steps, (<span class="bu">len</span>(X),)).to(device)</span>
<span id="cb17-41"><a href=""></a></span>
<span id="cb17-42"><a href=""></a>      <span class="co"># Forward pass</span></span>
<span id="cb17-43"><a href=""></a>      image_w_noise <span class="op">=</span> ddpm.add_noise(X, t, noise)</span>
<span id="cb17-44"><a href=""></a>      predicted_noise <span class="op">=</span> ddpm.predict_noise(image_w_noise, t)</span>
<span id="cb17-45"><a href=""></a>      loss <span class="op">=</span> loss_fn(predicted_noise, noise)</span>
<span id="cb17-46"><a href=""></a>        </span>
<span id="cb17-47"><a href=""></a>      <span class="co"># Backward pass</span></span>
<span id="cb17-48"><a href=""></a>      loss.backward()</span>
<span id="cb17-49"><a href=""></a>      optimizer.step()</span>
<span id="cb17-50"><a href=""></a></span>
<span id="cb17-51"><a href=""></a>      <span class="co"># Reset the computed gradients back to zero</span></span>
<span id="cb17-52"><a href=""></a>      optimizer.zero_grad()</span>
<span id="cb17-53"><a href=""></a></span>
<span id="cb17-54"><a href=""></a>      <span class="co"># Output stats</span></span>
<span id="cb17-55"><a href=""></a>      total_loss <span class="op">+=</span> loss</span>
<span id="cb17-56"><a href=""></a>      total_batches <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb17-57"><a href=""></a>      ep_tqdm.set_postfix(average_batch_loss<span class="op">=</span>(total_loss<span class="op">/</span>total_batches).item())</span>
<span id="cb17-58"><a href=""></a></span>
<span id="cb17-59"><a href=""></a><span class="co"># Organize the training loop</span></span>
<span id="cb17-60"><a href=""></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb17-61"><a href=""></a>  <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>t<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">-------------------------------"</span>)</span>
<span id="cb17-62"><a href=""></a>  train_epoch(dataloader, model, loss_fn, optimizer)</span>
<span id="cb17-63"><a href=""></a></span>
<span id="cb17-64"><a href=""></a>torch.save(model, <span class="st">'ddpm_unet.pth'</span>)</span>
<span id="cb17-65"><a href=""></a><span class="bu">print</span>(<span class="st">"Done!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>

</section>
<section id="ddpm-10-output" class="slide level2 output-location-slide"><h2>DDPM</h2><div class="cell output-location-slide" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="5ebaad75-3abe-46d0-d1eb-71524c3d5b4f" data-execution_count="13">
<div class="cell-output cell-output-stdout">
<pre><code>Using cuda device
Epoch 1
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.84batch/s, average_batch_loss=0.241]
Epoch 2
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.96batch/s, average_batch_loss=0.0743]
Epoch 3
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.99batch/s, average_batch_loss=0.0563]
Epoch 4
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.95batch/s, average_batch_loss=0.0478]
Epoch 5
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.93batch/s, average_batch_loss=0.0437]
Epoch 6
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.95batch/s, average_batch_loss=0.0402]
Epoch 7
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.93batch/s, average_batch_loss=0.038] 
Epoch 8
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.94batch/s, average_batch_loss=0.0357]
Epoch 9
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.93batch/s, average_batch_loss=0.0342]
Epoch 10
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.93batch/s, average_batch_loss=0.0325]
Epoch 11
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.99batch/s, average_batch_loss=0.0314]
Epoch 12
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.95batch/s, average_batch_loss=0.0305]
Epoch 13
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 24.00batch/s, average_batch_loss=0.0295]
Epoch 14
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.98batch/s, average_batch_loss=0.029] 
Epoch 15
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.92batch/s, average_batch_loss=0.0281]
Epoch 16
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.91batch/s, average_batch_loss=0.0279]
Epoch 17
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.95batch/s, average_batch_loss=0.0275]
Epoch 18
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.92batch/s, average_batch_loss=0.0271]
Epoch 19
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.91batch/s, average_batch_loss=0.0268]
Epoch 20
-------------------------------
Train: 100%|██████████| 547/547 [00:22&lt;00:00, 23.94batch/s, average_batch_loss=0.0259]
Done!</code></pre>
</div>
</div></section><section id="ddpm-11" class="slide level2">
<h2>DDPM</h2>
<p>Le’s generate some images.</p>
<div id="557e178b" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:86}}" data-outputid="e940c2a3-e115-4078-9e8d-1326de259aa4" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href=""></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb19-2"><a href=""></a></span>
<span id="cb19-3"><a href=""></a><span class="kw">def</span> show(img):</span>
<span id="cb19-4"><a href=""></a>  <span class="co">"""Function for displaying image"""</span></span>
<span id="cb19-5"><a href=""></a>  npimg <span class="op">=</span> img.detach().cpu().numpy()</span>
<span id="cb19-6"><a href=""></a>  plt.imshow(np.transpose(npimg, (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb19-7"><a href=""></a>  plt.axis(<span class="st">'off'</span>)</span>
<span id="cb19-8"><a href=""></a></span>
<span id="cb19-9"><a href=""></a>show(ddpm.sample(n_samples<span class="op">=</span><span class="dv">10</span>).transpose(<span class="dv">2</span>, <span class="dv">3</span>).reshape(<span class="dv">1</span>, <span class="dv">10</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">28</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>

</div>
<img data-src="09_autoencoders_and_ddpm_files/figure-revealjs/cell-15-output-1.png" class="r-stretch"></section>
<section id="practice-task" class="slide level2">
<h2>Practice task</h2>
<p>Try to augment the sampling procedure such that you would be able to specify the digit that you want to sample.</p>
<p>You can do this using Algorithm 2 from this <a href="https://arxiv.org/abs/2105.05233">paper</a>.</p>
<p>You can find the trained model for denoising <a href="https://github.com/jputrius/ml_intro/tree/main/models">here</a>.</p>

</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="09_autoencoders_and_ddpm_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="09_autoencoders_and_ddpm_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="09_autoencoders_and_ddpm_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="09_autoencoders_and_ddpm_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="09_autoencoders_and_ddpm_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="09_autoencoders_and_ddpm_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="09_autoencoders_and_ddpm_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="09_autoencoders_and_ddpm_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="09_autoencoders_and_ddpm_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="09_autoencoders_and_ddpm_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>